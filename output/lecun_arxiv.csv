title,authors,year,venue,citation_count,doi,arxiv_id,source,source_id,primary_category,categories,abstract,url,pdf_url
LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics,Randall Balestriero; Yann LeCun,2025,,0,,2511.08544,arxiv,2511.08544,cs.LG,cs.LG; cs.AI; cs.CV; stat.ML,"Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. We present a comprehensive theory of JEPAs and instantiate it in {\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\href{https://github.com/rbalestr-lab/lejepa}{GitHub repo}).",https://arxiv.org/abs/2511.08544v3,https://arxiv.org/pdf/2511.08544v3
Cambrian-S: Towards Spatial Supersensing in Video,Shusheng Yang; Jihan Yang; Pinzhi Huang; Ellis Brown; Zihao Yang; Yue Yu; Shengbang Tong; Zihan Zheng; Yifan Xu; Muhan Wang; Daohan Lu; Rob Fergus; Yann LeCun; Li Fei-Fei; Saining Xie,2025,,0,,2511.04670,arxiv,2511.04670,cs.CV,cs.CV,"We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.",https://arxiv.org/abs/2511.04670v1,https://arxiv.org/pdf/2511.04670v1
Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin,Enrique Queipo-de-Llano; Álvaro Arroyo; Federico Barbero; Xiaowen Dong; Michael Bronstein; Yann LeCun; Ravid Shwartz-Ziv,2025,,0,,2510.06477,arxiv,2510.06477,cs.LG,cs.LG; cs.AI,"Attention sinks and compression valleys have attracted significant attention as two puzzling phenomena in large language models, but have been studied in isolation. In this work, we present a surprising connection between attention sinks and compression valleys, tracing both to the formation of massive activations in the residual stream. We prove theoretically that massive activations necessarily produce representational compression and establish bounds on the resulting entropy reduction. Through experiments across several models (410M-120B parameters), we confirm that when the beginning-of-sequence token develops extreme activation norms in the middle layers, both compression valleys and attention sinks emerge simultaneously. Targeted ablation studies validate our theoretical predictions. This unified view motivates us to propose the Mix-Compress-Refine theory of information flow, as an attempt to explain how LLMs organize their computation in depth by controlling attention and representational compression via massive activations. Specifically, we posit that Transformer-based LLMs process tokens in three distinct phases: (1) broad mixing in the early layers, (2) compressed computation with limited mixing in the middle layers, and (3) selective refinement in the late layers. Our framework helps explain why embedding tasks perform best at intermediate layers, whereas generation tasks benefit from full-depth processing, clarifying differences in task-dependent representations.",https://arxiv.org/abs/2510.06477v1,https://arxiv.org/pdf/2510.06477v1
Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density,Randall Balestriero; Nicolas Ballas; Mike Rabbat; Yann LeCun,2025,,0,,2510.05949,arxiv,2510.05949,cs.LG,cs.LG; cs.AI; cs.CV; stat.ML,"Joint Embedding Predictive Architectures (JEPAs) learn representations able to solve numerous downstream tasks out-of-the-box. JEPAs combine two objectives: (i) a latent-space prediction term, i.e., the representation of a slightly perturbed sample must be predictable from the original sample's representation, and (ii) an anti-collapse term, i.e., not all samples should have the same representation. While (ii) is often considered as an obvious remedy to representation collapse, we uncover that JEPAs' anti-collapse term does much more--it provably estimates the data density. In short, any successfully trained JEPA can be used to get sample probabilities, e.g., for data curation, outlier detection, or simply for density estimation. Our theoretical finding is agnostic of the dataset and architecture used--in any case one can compute the learned probabilities of sample $x$ efficiently and in closed-form using the model's Jacobian matrix at $x$. Our findings are empirically validated across datasets (synthetic, controlled, and Imagenet) and across different Self Supervised Learning methods falling under the JEPA family (I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. We denote the method extracting the JEPA learned density as {\bf JEPA-SCORE}.",https://arxiv.org/abs/2510.05949v1,https://arxiv.org/pdf/2510.05949v1
LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures,Hai Huang; Yann LeCun; Randall Balestriero,2025,,0,,2509.14252,arxiv,2509.14252,cs.CL,cs.CL; cs.AI,"Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: https://github.com/rbalestr-lab/llm-jepa.",https://arxiv.org/abs/2509.14252v2,https://arxiv.org/pdf/2509.14252v2
Back to the Features: DINO as a Foundation for Video World Models,Federico Baldassarre; Marc Szafraniec; Basile Terver; Vasil Khalidov; Francisco Massa; Yann LeCun; Patrick Labatut; Maximilian Seitzer; Piotr Bojanowski,2025,,0,,2507.19468,arxiv,2507.19468,cs.CV,cs.CV,"We present DINO-world, a powerful generalist video world model trained to predict future frames in the latent space of DINOv2. By leveraging a pre-trained image encoder and training a future predictor on a large-scale uncurated video dataset, DINO-world learns the temporal dynamics of diverse scenes, from driving and indoor scenes to simulated environments. We show that DINO-world outperforms previous models on a variety of video prediction benchmarks, e.g. segmentation and depth forecasting, and demonstrates strong understanding of intuitive physics. Furthermore, we show that it is possible to fine-tune the predictor on observation-action trajectories. The resulting action-conditioned world model can be used for planning by simulating candidate trajectories in latent space.",https://arxiv.org/abs/2507.19468v1,https://arxiv.org/pdf/2507.19468v1
Whole-Body Conditioned Egocentric Video Prediction,Yutong Bai; Danny Tran; Amir Bar; Yann LeCun; Trevor Darrell; Jitendra Malik,2025,,0,,2506.21552,arxiv,2506.21552,cs.CV,cs.CV; cs.AI; cs.LG; cs.MM; cs.RO,"We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human.",https://arxiv.org/abs/2506.21552v1,https://arxiv.org/pdf/2506.21552v1
"V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning",Mido Assran; Adrien Bardes; David Fan; Quentin Garrido; Russell Howes; Mojtaba; Komeili; Matthew Muckley; Ammar Rizvi; Claire Roberts; Koustuv Sinha; Artem Zholus; Sergio Arnaud; Abha Gejji; Ada Martin; Francois Robert Hogan; Daniel Dugas; Piotr Bojanowski; Vasil Khalidov; Patrick Labatut; Francisco Massa; Marc Szafraniec; Kapil Krishnakumar; Yong Li; Xiaodong Ma; Sarath Chandar; Franziska Meier; Yann LeCun; Michael Rabbat; Nicolas Ballas,2025,,0,,2506.09985,arxiv,2506.09985,cs.AI,cs.AI; cs.CV; cs.LG; cs.RO,"A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.",https://arxiv.org/abs/2506.09985v1,https://arxiv.org/pdf/2506.09985v1
OSVI-WM: One-Shot Visual Imitation for Unseen Tasks using World-Model-Guided Trajectory Generation,Raktim Gautam Goswami; Prashanth Krishnamurthy; Yann LeCun; Farshad Khorrami,2025,,0,,2505.20425,arxiv,2505.20425,cs.RO,cs.RO,"Visual imitation learning enables robotic agents to acquire skills by observing expert demonstration videos. In the one-shot setting, the agent generates a policy after observing a single expert demonstration without additional fine-tuning. Existing approaches typically train and evaluate on the same set of tasks, varying only object configurations, and struggle to generalize to unseen tasks with different semantic or structural requirements. While some recent methods attempt to address this, they exhibit low success rates on hard test tasks that, despite being visually similar to some training tasks, differ in context and require distinct responses. Additionally, most existing methods lack an explicit model of environment dynamics, limiting their ability to reason about future states. To address these limitations, we propose a novel framework for one-shot visual imitation learning via world-model-guided trajectory generation. Given an expert demonstration video and the agent's initial observation, our method leverages a learned world model to predict a sequence of latent states and actions. This latent trajectory is then decoded into physical waypoints that guide the agent's execution. Our method is evaluated on two simulated benchmarks and three real-world robotic platforms, where it consistently outperforms prior approaches, with over 30% improvement in some cases.",https://arxiv.org/abs/2505.20425v1,https://arxiv.org/pdf/2505.20425v1
From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning,Chen Shani; Liron Soffer; Dan Jurafsky; Yann LeCun; Ravid Shwartz-Ziv,2025,,0,,2505.17117,arxiv,2505.17117,cs.CL,cs.CL; cs.AI; cs.IT,"Humans organize knowledge into compact categories that balance compression with semantic meaning preservation. Large Language Models (LLMs) demonstrate striking linguistic abilities, yet whether they achieve this same balance remains unclear. We apply the Information Bottleneck principle to quantitatively compare how LLMs and humans navigate this compression-meaning trade-off. Analyzing embeddings from 40+ LLMs against classic human categorization benchmarks, we uncover three key findings. First, LLMs broadly align with human categories but miss fine-grained semantic distinctions crucial for human understanding. Second, LLMs demonstrate aggressive statistical compression, achieving ``optimal'' information-theoretic efficiency, while humans prioritize contextual richness and adaptive flexibility. Third, encoder models surprisingly outperform decoder models in human alignment, suggesting that generation and understanding rely on distinct mechanisms in current architectures. In addition, training dynamics analysis reveals that conceptual structure develops in distinct phases: rapid initial formation followed by architectural reorganization, with semantic processing migrating from deeper to mid-network layers as models discover more efficient encoding. These divergent strategies, where LLMs optimize for compression and humans for adaptive utility, reveal fundamental differences between artificial and biological intelligence, guiding development toward more human-aligned AI.",https://arxiv.org/abs/2505.17117v5,https://arxiv.org/pdf/2505.17117v5
Scaling Language-Free Visual Representation Learning,David Fan; Shengbang Tong; Jiachen Zhu; Koustuv Sinha; Zhuang Liu; Xinlei Chen; Michael Rabbat; Nicolas Ballas; Yann LeCun; Amir Bar; Saining Xie,2025,,0,,2504.01017,arxiv,2504.01017,cs.CV,cs.CV,"Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: ""Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?"" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.",https://arxiv.org/abs/2504.01017v1,https://arxiv.org/pdf/2504.01017v1
Transformers without Normalization,Jiachen Zhu; Xinlei Chen; Kaiming He; Yann LeCun; Zhuang Liu,2025,,0,,2503.10622,arxiv,2503.10622,cs.LG,cs.LG; cs.AI; cs.CL; cs.CV,"Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) = \tanh(α$x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.",https://arxiv.org/abs/2503.10622v2,https://arxiv.org/pdf/2503.10622v2
Forgotten Polygons: Multimodal Large Language Models are Shape-Blind,William Rudman; Michal Golovanevsky; Amir Bar; Vedant Palit; Yann LeCun; Carsten Eickhoff; Ritambhara Singh,2025,,0,,2502.15969,arxiv,2502.15969,cs.CV,cs.CV; cs.AI; cs.CL,"Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of sides nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning. Code available at: https://github.com/rsinghlab/Shape-Blind.",https://arxiv.org/abs/2502.15969v4,https://arxiv.org/pdf/2502.15969v4
Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models,Vlad Sobal; Wancong Zhang; Kyunghyun Cho; Randall Balestriero; Tim G. J. Rudner; Yann LeCun,2025,,0,,2502.14819,arxiv,2502.14819,cs.LG,cs.LG,"A long-standing goal in AI is to develop agents capable of solving diverse tasks across a range of environments, including those never seen during training. Two dominant paradigms address this challenge: (i) reinforcement learning (RL), which learns policies via trial and error, and (ii) optimal control, which plans actions using a known or learned dynamics model. However, their comparative strengths in the offline setting - where agents must learn from reward-free trajectories - remain underexplored. In this work, we systematically evaluate RL and control-based methods on a suite of navigation tasks, using offline datasets of varying quality. On the RL side, we consider goal-conditioned and zero-shot methods. On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and employ it for planning. We investigate how factors such as data diversity, trajectory quality, and environment variability influence the performance of these approaches. Our results show that model-free RL benefits most from large amounts of high-quality data, whereas model-based planning generalizes better to unseen layouts and is more data-efficient, while achieving trajectory stitching performance comparable to leading model-free methods. Notably, planning with a latent dynamics model proves to be a strong approach for handling suboptimal offline data and adapting to diverse environments.",https://arxiv.org/abs/2502.14819v4,https://arxiv.org/pdf/2502.14819v4
Intuitive physics understanding emerges from self-supervised pretraining on natural videos,Quentin Garrido; Nicolas Ballas; Mahmoud Assran; Adrien Bardes; Laurent Najman; Michael Rabbat; Emmanuel Dupoux; Yann LeCun,2025,,0,,2502.11831,arxiv,2502.11831,cs.CV,cs.CV; cs.AI,"We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics.",https://arxiv.org/abs/2502.11831v1,https://arxiv.org/pdf/2502.11831v1
Layer by Layer: Uncovering Hidden Representations in Language Models,Oscar Skean; Md Rifat Arefin; Dan Zhao; Niket Patel; Jalal Naghiyev; Yann LeCun; Ravid Shwartz-Ziv,2025,,0,,2502.02013,arxiv,2502.02013,cs.LG,cs.LG; cs.AI; cs.CL,"From extracting features to generating text, the outputs of large language models (LLMs) typically rely on the final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks across various architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features, challenging the standard view on final-layer embeddings and opening new directions on using mid-layer representations for more robust and accurate representations.",https://arxiv.org/abs/2502.02013v2,https://arxiv.org/pdf/2502.02013v2
MetaMorph: Multimodal Understanding and Generation via Instruction Tuning,Shengbang Tong; David Fan; Jiachen Zhu; Yunyang Xiong; Xinlei Chen; Koustuv Sinha; Michael Rabbat; Yann LeCun; Saining Xie; Zhuang Liu,2024,,0,,2412.14164,arxiv,2412.14164,cs.CV,cs.CV,"In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a simple and effective extension to visual instruction tuning that enables a pretrained LLM to quickly morph into an unified autoregressive model capable of generating both text and visual tokens. VPiT teaches an LLM to predict discrete text tokens and continuous visual tokens from any input sequence of image and text data curated in an instruction-following format. Our empirical investigation reveals several intriguing properties of VPiT: (1) visual generation ability emerges as a natural byproduct of improved visual understanding, and can be unlocked efficiently with a small amount of generation data; (2) while we find understanding and generation to be mutually beneficial, understanding data contributes to both capabilities more effectively than generation data. Building upon these findings, we train our MetaMorph model and achieve competitive performance on both visual understanding and generation. In visual generation, MetaMorph can leverage the world knowledge and reasoning abilities gained from LLM pretraining, and overcome common failure modes exhibited by other generation models. Our results suggest that LLMs may have strong ""prior"" vision capabilities that can be efficiently adapted to both visual understanding and generation with a relatively simple instruction tuning process.",https://arxiv.org/abs/2412.14164v1,https://arxiv.org/pdf/2412.14164v1
Video Representation Learning with Joint-Embedding Predictive Architectures,Katrina Drozdov; Ravid Shwartz-Ziv; Yann LeCun,2024,,0,,2412.10925,arxiv,2412.10925,cs.CV,cs.CV; cs.AI,"Video representation learning is an increasingly important topic in machine learning research. We present Video JEPA with Variance-Covariance Regularization (VJ-VCR): a joint-embedding predictive architecture for self-supervised video representation learning that employs variance and covariance regularization to avoid representation collapse. We show that hidden representations from our VJ-VCR contain abstract, high-level information about the input data. Specifically, they outperform representations obtained from a generative baseline on downstream tasks that require understanding of the underlying dynamics of moving objects in the videos. Additionally, we explore different ways to incorporate latent variables into the VJ-VCR framework that capture information about uncertainty in the future in non-deterministic settings.",https://arxiv.org/abs/2412.10925v1,https://arxiv.org/pdf/2412.10925v1
Does Representation Matter? Exploring Intermediate Layers in Large Language Models,Oscar Skean; Md Rifat Arefin; Yann LeCun; Ravid Shwartz-Ziv,2024,,0,,2412.09563,arxiv,2412.09563,cs.LG,cs.LG; cs.CL,"Understanding what defines a good representation in large language models (LLMs) is fundamental to both theoretical understanding and practical applications. In this paper, we investigate the quality of intermediate representations in various LLM architectures, including Transformers and State Space Models (SSMs). We find that intermediate layers often yield more informative representations for downstream tasks than the final layers. To measure the representation quality, we adapt and apply a suite of metrics - such as prompt entropy, curvature, and augmentation-invariance - originally proposed in other contexts. Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer. Notably, we observe a bimodal pattern in the entropy of some intermediate layers and consider potential explanations tied to training data. Overall, our results illuminate the internal mechanics of LLMs and guide strategies for architectural optimization and training.",https://arxiv.org/abs/2412.09563v1,https://arxiv.org/pdf/2412.09563v1
Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation,Tal Zeevi; Ravid Shwartz-Ziv; Yann LeCun; Lawrence H. Staib; John A. Onofrey,2024,,0,,2412.07169,arxiv,2412.07169,cs.LG,cs.LG; cs.CV; stat.ML,"Accurate uncertainty estimation is crucial for deploying neural networks in risk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a widely used technique for approximating predictive uncertainty by performing stochastic forward passes with dropout during inference. However, using static dropout rates across all layers and inputs can lead to suboptimal uncertainty estimates, as it fails to adapt to the varying characteristics of individual inputs and network layers. Existing approaches optimize dropout rates during training using labeled data, resulting in fixed inference-time parameters that cannot adjust to new data distributions, compromising uncertainty estimates in Monte Carlo simulations.   In this paper, we propose Rate-In, an algorithm that dynamically adjusts dropout rates during inference by quantifying the information loss induced by dropout in each layer's feature maps. By treating dropout as controlled noise injection and leveraging information-theoretic principles, Rate-In adapts dropout rates per layer and per input instance without requiring ground truth labels. By quantifying the functional information loss in feature maps, we adaptively tune dropout rates to maintain perceptual quality across diverse medical imaging tasks and architectural configurations. Our extensive empirical study on synthetic data and real-world medical imaging tasks demonstrates that Rate-In improves calibration and sharpens uncertainty estimates compared to fixed or heuristic dropout rates without compromising predictive performance. Rate-In offers a practical, unsupervised, inference-time approach to optimizing dropout for more reliable predictive uncertainty estimation in critical applications.",https://arxiv.org/abs/2412.07169v4,https://arxiv.org/pdf/2412.07169v4
Navigation World Models,Amir Bar; Gaoyue Zhou; Danny Tran; Trevor Darrell; Yann LeCun,2024,,0,,2412.03572,arxiv,2412.03572,cs.CV,cs.CV; cs.AI; cs.LG; cs.RO,"Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.",https://arxiv.org/abs/2412.03572v2,https://arxiv.org/pdf/2412.03572v2
RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training,Raktim Gautam Goswami; Prashanth Krishnamurthy; Yann LeCun; Farshad Khorrami,2024,,0,,2411.17662,arxiv,2411.17662,cs.RO,cs.RO; cs.CV,"Vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks. Current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose. While images of robots inherently contain rich information about the robot's physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations. To address this, we introduce RoboPEPP, a method that fuses information about the robot's physical model into the encoder using a masking-based self-supervised embedding-predictive architecture. Specifically, we mask the robot's joints and pre-train an encoder-predictor model to infer the joints' embeddings from surrounding unmasked regions, enhancing the encoder's understanding of the robot's physical model. The pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation. Random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness. Our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time.",https://arxiv.org/abs/2411.17662v2,https://arxiv.org/pdf/2411.17662v2
Improving Pre-trained Self-Supervised Embeddings Through Effective Entropy Maximization,Deep Chakraborty; Yann LeCun; Tim G. J. Rudner; Erik Learned-Miller,2024,,0,,2411.15931,arxiv,2411.15931,cs.LG,cs.LG; cs.CV; cs.IT; stat.AP; stat.ML,"A number of different architectures and loss functions have been applied to the problem of self-supervised learning (SSL), with the goal of developing embeddings that provide the best possible pre-training for as-yet-unknown, lightly supervised downstream tasks. One of these SSL criteria is to maximize the entropy of a set of embeddings in some compact space. But the goal of maximizing the embedding entropy often depends -- whether explicitly or implicitly -- upon high dimensional entropy estimates, which typically perform poorly in more than a few dimensions. In this paper, we motivate an effective entropy maximization criterion (E2MC), defined in terms of easy-to-estimate, low-dimensional constraints. We demonstrate that using it to continue training an already-trained SSL model for only a handful of epochs leads to a consistent and, in some cases, significant improvement in downstream performance. We perform careful ablation studies to show that the improved performance is due to the proposed add-on criterion. We also show that continued pre-training with alternative criteria does not lead to notable improvements, and in some cases, even degrades performance.",https://arxiv.org/abs/2411.15931v2,https://arxiv.org/pdf/2411.15931v2
DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning,Gaoyue Zhou; Hengkai Pan; Yann LeCun; Lerrel Pinto,2024,,0,,2411.04983,arxiv,2411.04983,cs.RO,cs.RO; cs.AI,"The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, remains challenging to learn and are typically developed for task-specific solutions with online policy learning. To unlock world models' true potential, we argue that they should 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To this end, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic planning by treating goal features as prediction targets. We demonstrate that DINO-WM achieves zero-shot behavioral solutions at test time on six environments without expert demonstrations, reward modeling, or pre-learned inverse models, outperforming prior state-of-the-art work across diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.",https://arxiv.org/abs/2411.04983v2,https://arxiv.org/pdf/2411.04983v2
Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning,Md Rifat Arefin; Gopeshh Subbaraj; Nicolas Gontier; Yann LeCun; Irina Rish; Ravid Shwartz-Ziv; Christopher Pal,2024,,0,,2411.02344,arxiv,2411.02344,cs.LG,cs.LG; cs.CL,"Decoder-only Transformers often struggle with complex reasoning tasks, particularly arithmetic reasoning requiring multiple sequential operations. In this work, we identify representation collapse in the model's intermediate layers as a key factor limiting their reasoning capabilities. To address this, we propose Sequential Variance-Covariance Regularization (Seq-VCR), which enhances the entropy of intermediate representations and prevents collapse. Combined with dummy pause tokens as substitutes for chain-of-thought (CoT) tokens, our method significantly improves performance in arithmetic reasoning problems. In the challenging $5 \times 5$ integer multiplication task, our approach achieves $99.5\%$ exact match accuracy, outperforming models of the same size (which yield $0\%$ accuracy) and GPT-4 with five-shot CoT prompting ($44\%$). We also demonstrate superior results on arithmetic expression and longest increasing subsequence (LIS) datasets. Our findings highlight the importance of preventing intermediate layer representation collapse to enhance the reasoning capabilities of Transformers and show that Seq-VCR offers an effective solution without requiring explicit CoT supervision.",https://arxiv.org/abs/2411.02344v2,https://arxiv.org/pdf/2411.02344v2
Multi-modal AI for comprehensive breast cancer prognostication,Jan Witowski; Ken G. Zeng; Joseph Cappadona; Jailan Elayoubi; Khalil Choucair; Elena Diana Chiru; Nancy Chan; Young-Joon Kang; Frederick Howard; Irina Ostrovnaya; Carlos Fernandez-Granda; Freya Schnabel; Zoe Steinsnyder; Ugur Ozerdem; Kangning Liu; Waleed Abdulsattar; Yu Zong; Lina Daoud; Rafic Beydoun; Anas Saad; Nitya Thakore; Mohammad Sadic; Frank Yeung; Elisa Liu; Theodore Hill; Benjamin Swett; Danielle Rigau; Andrew Clayburn; Valerie Speirs; Marcus Vetter; Lina Sojak; Simone Soysal; Daniel Baumhoer; Jia-Wern Pan; Haslina Makmur; Soo-Hwang Teo; Linda Ma Pak; Victor Angel; Dovile Zilenaite-Petrulaitiene; Arvydas Laurinavicius; Natalie Klar; Brian D. Piening; Carlo Bifulco; Sun-Young Jun; Jae Pak Yi; Su Hyun Lim; Adam Brufsky; Francisco J. Esteva; Lajos Pusztai; Yann LeCun; Krzysztof J. Geras,2024,,0,,2410.21256,arxiv,2410.21256,cs.AI,cs.AI; cs.CV; eess.IV,"Treatment selection in breast cancer is guided by molecular subtypes and clinical characteristics. However, current tools including genomic assays lack the accuracy required for optimal clinical decision-making. We developed a novel artificial intelligence (AI)-based approach that integrates digital pathology images with clinical data, providing a more robust and effective method for predicting the risk of cancer recurrence in breast cancer patients. Specifically, we utilized a vision transformer pan-cancer foundation model trained with self-supervised learning to extract features from digitized H&E-stained slides. These features were integrated with clinical data to form a multi-modal AI test predicting cancer recurrence and death. The test was developed and evaluated using data from a total of 8,161 female breast cancer patients across 15 cohorts originating from seven countries. Of these, 3,502 patients from five cohorts were used exclusively for evaluation, while the remaining patients were used for training. Our test accurately predicted our primary endpoint, disease-free interval, in the five evaluation cohorts (C-index: 0.71 [0.68-0.75], HR: 3.63 [3.02-4.37, p<0.001]). In a direct comparison (n=858), the AI test was more accurate than Oncotype DX, the standard-of-care 21-gene assay, achieving a C-index of 0.67 [0.61-0.74] versus 0.61 [0.49-0.73], respectively. Additionally, the AI test added independent prognostic information to Oncotype DX in a multivariate analysis (HR: 3.11 [1.91-5.09, p<0.001)]). The test demonstrated robust accuracy across major molecular breast cancer subtypes, including TNBC (C-index: 0.71 [0.62-0.81], HR: 3.81 [2.35-6.17, p=0.02]), where no diagnostic tools are currently recommended by clinical guidelines. These results suggest that our AI test improves upon the accuracy of existing prognostic tests, while being applicable to a wider range of patients.",https://arxiv.org/abs/2410.21256v2,https://arxiv.org/pdf/2410.21256v2
PooDLe: Pooled and dense self-supervised learning from naturalistic videos,Alex N. Wang; Christopher Hoang; Yuwen Xiong; Yann LeCun; Mengye Ren,2024,,0,,2408.11208,arxiv,2408.11208,cs.CV,cs.CV; cs.LG,"Self-supervised learning has driven significant progress in learning from single-subject, iconic images. However, there are still unanswered questions about the use of minimally-curated, naturalistic video data, which contain dense scenes with many independent objects, imbalanced class distributions, and varying object sizes. In this paper, we propose PooDLe, a self-supervised learning method that combines an invariance-based objective on pooled representations with a dense SSL objective that enforces equivariance to optical flow warping. Our results show that a unified objective applied at multiple feature scales is essential for learning effective image representations from naturalistic videos. We validate our method with experiments on the BDD100K driving video dataset and the Walking Tours first-person video dataset, demonstrating its ability to capture spatial understanding from a dense objective and semantic understanding via a pooled representation objective.",https://arxiv.org/abs/2408.11208v3,https://arxiv.org/pdf/2408.11208v3
$\mathbb{X}$-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs,Vlad Sobal; Mark Ibrahim; Randall Balestriero; Vivien Cabannes; Diane Bouchacourt; Pietro Astolfi; Kyunghyun Cho; Yann LeCun,2024,,0,,2407.18134,arxiv,2407.18134,cs.CV,cs.CV; cs.LG,"Learning good representations involves capturing the diverse ways in which data samples relate. Contrastive loss - an objective matching related samples - underlies methods from self-supervised to multimodal learning. Contrastive losses, however, can be viewed more broadly as modifying a similarity graph to indicate how samples should relate in the embedding space. This view reveals a shortcoming in contrastive learning: the similarity graph is binary, as only one sample is the related positive sample. Crucially, similarities \textit{across} samples are ignored. Based on this observation, we revise the standard contrastive loss to explicitly encode how a sample relates to others. We experiment with this new objective, called $\mathbb{X}$-Sample Contrastive, to train vision models based on similarities in class or text caption descriptions. Our study spans three scales: ImageNet-1k with 1 million, CC3M with 3 million, and CC12M with 12 million samples. The representations learned via our objective outperform both contrastive self-supervised and vision-language models trained on the same data across a range of tasks. When training on CC12M, we outperform CLIP by $0.6\%$ on both ImageNet and ImageNet Real. Our objective appears to work particularly well in lower-data regimes, with gains over CLIP of $16.8\%$ on ImageNet and $18.1\%$ on ImageNet Real when training with CC3M. Finally, our objective seems to encourage the model to learn representations that separate objects from their attributes and backgrounds, with gains of $3.3$-$5.6$\% over CLIP on ImageNet9. We hope the proposed solution takes a small step towards developing richer learning objectives for understanding sample relations in foundation models.",https://arxiv.org/abs/2407.18134v2,https://arxiv.org/pdf/2407.18134v2
"LiveBench: A Challenging, Contamination-Limited LLM Benchmark",Colin White; Samuel Dooley; Manley Roberts; Arka Pal; Ben Feuer; Siddhartha Jain; Ravid Shwartz-Ziv; Neel Jain; Khalid Saifullah; Sreemanti Dey; Shubh-Agrawal; Sandeep Singh Sandha; Siddartha Naidu; Chinmay Hegde; Yann LeCun; Tom Goldstein; Willie Neiswanger; Micah Goldblum,2024,,0,,2406.19314,arxiv,2406.19314,cs.CL,cs.CL; cs.AI; cs.LG,"Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.",https://arxiv.org/abs/2406.19314v2,https://arxiv.org/pdf/2406.19314v2
"Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",Shengbang Tong; Ellis Brown; Penghao Wu; Sanghyun Woo; Manoj Middepogu; Sai Charitha Akula; Jihan Yang; Shusheng Yang; Adithya Iyer; Xichen Pan; Ziteng Wang; Rob Fergus; Yann LeCun; Saining Xie,2024,,0,,2406.16860,arxiv,2406.16860,cs.CV,cs.CV,"We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, address the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.",https://arxiv.org/abs/2406.16860v2,https://arxiv.org/pdf/2406.16860v2
Just How Flexible are Neural Networks in Practice?,Ravid Shwartz-Ziv; Micah Goldblum; Arpit Bansal; C. Bayan Bruss; Yann LeCun; Andrew Gordon Wilson,2024,,0,,2406.11463,arxiv,2406.11463,cs.LG,cs.LG; stat.ML,"It is widely believed that a neural network can fit a training set containing at least as many samples as it has parameters, underpinning notions of overparameterized and underparameterized models. In practice, however, we only find solutions accessible via our training procedure, including the optimizer and regularizers, limiting flexibility. Moreover, the exact parameterization of the function class, built into an architecture, shapes its loss surface and impacts the minima we find. In this work, we examine the ability of neural networks to fit data in practice. Our findings indicate that: (1) standard optimizers find minima where the model can only fit training sets with significantly fewer samples than it has parameters; (2) convolutional networks are more parameter-efficient than MLPs and ViTs, even on randomly labeled data; (3) while stochastic training is thought to have a regularizing effect, SGD actually finds minima that fit more training data than full-batch gradient descent; (4) the difference in capacity to fit correctly labeled and incorrectly labeled samples can be predictive of generalization; (5) ReLU activation functions result in finding minima that fit more data despite being designed to avoid vanishing and exploding gradients in deep architectures.",https://arxiv.org/abs/2406.11463v1,https://arxiv.org/pdf/2406.11463v1
Towards an Improved Understanding and Utilization of Maximum Manifold Capacity Representations,Rylan Schaeffer; Victor Lecomte; Dhruv Bhandarkar Pai; Andres Carranza; Berivan Isik; Alyssa Unell; Mikail Khona; Thomas Yerxa; Yann LeCun; SueYeon Chung; Andrey Gromov; Ravid Shwartz-Ziv; Sanmi Koyejo,2024,,0,,2406.09366,arxiv,2406.09366,cs.LG,cs.LG; cs.CV; q-bio.NC,"Maximum Manifold Capacity Representations (MMCR) is a recent multi-view self-supervised learning (MVSSL) method that matches or surpasses other leading MVSSL methods. MMCR is intriguing because it does not fit neatly into any of the commonplace MVSSL lineages, instead originating from a statistical mechanical perspective on the linear separability of data manifolds. In this paper, we seek to improve our understanding and our utilization of MMCR. To better understand MMCR, we leverage tools from high dimensional probability to demonstrate that MMCR incentivizes alignment and uniformity of learned embeddings. We then leverage tools from information theory to show that such embeddings maximize a well-known lower bound on mutual information between views, thereby connecting the geometric perspective of MMCR to the information-theoretic perspective commonly discussed in MVSSL. To better utilize MMCR, we mathematically predict and experimentally confirm non-monotonic changes in the pretraining loss akin to double descent but with respect to atypical hyperparameters. We also discover compute scaling laws that enable predicting the pretraining loss as a function of gradients steps, batch size, embedding dimension and number of views. We then show that MMCR, originally applied to image data, is performant on multimodal image-text data. By more deeply understanding the theoretical and empirical behavior of MMCR, our work reveals insights on improving MVSSL methods.",https://arxiv.org/abs/2406.09366v1,https://arxiv.org/pdf/2406.09366v1
Hierarchical World Models as Visual Whole-Body Humanoid Controllers,Nicklas Hansen; Jyothir S; Vlad Sobal; Yann LeCun; Xiaolong Wang; Hao Su,2024,,0,,2405.18418,arxiv,2405.18418,cs.LG,cs.LG; cs.CV; cs.RO,"Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology. Learning from visual observations further exacerbates this difficulty. In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives. Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards. Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans.",https://arxiv.org/abs/2405.18418v3,https://arxiv.org/pdf/2405.18418v3
Towards a Framework for Openness in Foundation Models: Proceedings from the Columbia Convening on Openness in Artificial Intelligence,Adrien Basdevant; Camille François; Victor Storchan; Kevin Bankston; Ayah Bdeir; Brian Behlendorf; Merouane Debbah; Sayash Kapoor; Yann LeCun; Mark Surman; Helen King-Turvey; Nathan Lambert; Stefano Maffulli; Nik Marda; Govind Shivkumar; Justine Tunney,2024,,0,,2405.15802,arxiv,2405.15802,cs.SE,cs.SE; cs.AI,"Over the past year, there has been a robust debate about the benefits and risks of open sourcing foundation models. However, this discussion has often taken place at a high level of generality or with a narrow focus on specific technical attributes. In part, this is because defining open source for foundation models has proven tricky, given its significant differences from traditional software development. In order to inform more practical and nuanced decisions about opening AI systems, including foundation models, this paper presents a framework for grappling with openness across the AI stack. It summarizes previous work on this topic, analyzes the various potential reasons to pursue openness, and outlines how openness varies in different parts of the AI stack, both at the model and at the system level. In doing so, its authors hope to provide a common descriptive framework to deepen a nuanced and rigorous understanding of openness in AI and enable further work around definitions of openness and safety in AI.",https://arxiv.org/abs/2405.15802v1,https://arxiv.org/pdf/2405.15802v1
Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning,Yuexiang Zhai; Hao Bai; Zipeng Lin; Jiayi Pan; Shengbang Tong; Yifei Zhou; Alane Suhr; Saining Xie; Yann LeCun; Yi Ma; Sergey Levine,2024,,0,,2405.10292,arxiv,2405.10292,cs.AI,cs.AI; cs.CL; cs.CV; cs.LG,"Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.",https://arxiv.org/abs/2405.10292v3,https://arxiv.org/pdf/2405.10292v3
The Entropy Enigma: Success and Failure of Entropy Minimization,Ori Press; Ravid Shwartz-Ziv; Yann LeCun; Matthias Bethge,2024,,0,,2405.05012,arxiv,2405.05012,cs.CV,cs.CV,"Entropy minimization (EM) is frequently used to increase the accuracy of classification models when they're faced with new data at test time. EM is a self-supervised learning method that optimizes classifiers to assign even higher probabilities to their top predicted classes. In this paper, we analyze why EM works when adapting a model for a few steps and why it eventually fails after adapting for many steps. We show that, at first, EM causes the model to embed test images close to training images, thereby increasing model accuracy. After many steps of optimization, EM makes the model embed test images far away from the embeddings of training images, which results in a degradation of accuracy. Building upon our insights, we present a method for solving a practical problem: estimating a model's accuracy on a given arbitrary dataset without having access to its labels. Our method estimates accuracy by looking at how the embeddings of input images change as the model is optimized to minimize entropy. Experiments on 23 challenging datasets show that our method sets the SoTA with a mean absolute error of $5.75\%$, an improvement of $29.62\%$ over the previous SoTA on this task. Our code is available at https://github.com/oripress/EntropyEnigma",https://arxiv.org/abs/2405.05012v2,https://arxiv.org/pdf/2405.05012v2
Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning,Théo Moutakanni; Piotr Bojanowski; Guillaume Chassagnon; Céline Hudelot; Armand Joulin; Yann LeCun; Matthew Muckley; Maxime Oquab; Marie-Pierre Revel; Maria Vakalopoulou,2024,,0,,2405.01469,arxiv,2405.01469,cs.CV,cs.CV; cs.AI,"AI Foundation models are gaining traction in various applications, including medical fields like radiology. However, medical foundation models are often tested on limited tasks, leaving their generalisability and biases unexplored. We present RayDINO, a large visual encoder trained by self-supervision on 873k chest X-rays. We compare RayDINO to previous state-of-the-art models across nine radiology tasks, from classification and dense segmentation to text generation, and provide an in depth analysis of population, age and sex biases of our model. Our findings suggest that self-supervision allows patient-centric AI proving useful in clinical workflows and interpreting X-rays holistically. With RayDINO and small task-specific adapters, we reach state-of-the-art results and improve generalization to unseen populations while mitigating bias, illustrating the true promise of foundation models: versatility and robustness.",https://arxiv.org/abs/2405.01469v1,https://arxiv.org/pdf/2405.01469v1
EgoPet: Egomotion and Interaction Data from an Animal's Perspective,Amir Bar; Arya Bakhtiar; Danny Tran; Antonio Loquercio; Jathushan Rajasegaran; Yann LeCun; Amir Globerson; Trevor Darrell,2024,,0,,2404.09991,arxiv,2404.09991,cs.RO,cs.RO; cs.CV,"Animals perceive the world to plan their actions and interact with other agents to accomplish complex tasks, demonstrating capabilities that are still unmatched by AI systems. To advance our understanding and reduce the gap between the capabilities of animals and AI systems, we introduce a dataset of pet egomotion imagery with diverse examples of simultaneous egomotion and multi-agent interaction. Current video datasets separately contain egomotion and interaction examples, but rarely both at the same time. In addition, EgoPet offers a radically distinct perspective from existing egocentric datasets of humans or vehicles. We define two in-domain benchmark tasks that capture animal behavior, and a third benchmark to assess the utility of EgoPet as a pretraining resource to robotic quadruped locomotion, showing that models trained from EgoPet outperform those trained from prior datasets.",https://arxiv.org/abs/2404.09991v1,https://arxiv.org/pdf/2404.09991v1
Learning and Leveraging World Models in Visual Representation Learning,Quentin Garrido; Mahmoud Assran; Nicolas Ballas; Adrien Bardes; Laurent Najman; Yann LeCun,2024,,0,,2403.00504,arxiv,2403.00504,cs.CV,cs.CV; cs.AI; cs.LG,"Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.",https://arxiv.org/abs/2403.00504v1,https://arxiv.org/pdf/2403.00504v1
Learning by Reconstruction Produces Uninformative Features For Perception,Randall Balestriero; Yann LeCun,2024,,0,,2402.11337,arxiv,2402.11337,cs.CV,cs.CV; cs.AI; stat.ML,"Input space reconstruction is an attractive representation learning paradigm. Despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90\% of the pixel variance can be solved with 45\% test accuracy. Using the bottom subspace instead, accounting for only 20\% of the pixel variance, reaches 55\% test accuracy. The features for perception being learned last explains the need for long training time, e.g., with Masked Autoencoders. Learning by denoising is a popular strategy to alleviate that misalignment. We prove that while some noise strategies such as masking are indeed beneficial, others such as additive Gaussian noise are not. Yet, even in the case of masking, we find that the benefits vary as a function of the mask's shape, ratio, and the considered dataset. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide first clues on how to detect if a noise strategy is never beneficial regardless of the perception task.",https://arxiv.org/abs/2402.11337v1,https://arxiv.org/pdf/2402.11337v1
Revisiting Feature Prediction for Learning Visual Representations from Video,Adrien Bardes; Quentin Garrido; Jean Ponce; Xinlei Chen; Michael Rabbat; Yann LeCun; Mahmoud Assran; Nicolas Ballas,2024,,0,,2404.08471,arxiv,2404.08471,cs.CV,cs.CV; cs.AI; cs.LG,"This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model's parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.",https://arxiv.org/abs/2404.08471v1,https://arxiv.org/pdf/2404.08471v1
G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering,Xiaoxin He; Yijun Tian; Yifei Sun; Nitesh V. Chawla; Thomas Laurent; Yann LeCun; Xavier Bresson; Bryan Hooi,2024,,0,,2402.07630,arxiv,2402.07630,cs.LG,cs.LG,"Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\footnote{Our codes and datasets are available at: \url{https://github.com/XiaoxinHe/G-Retriever}}",https://arxiv.org/abs/2402.07630v3,https://arxiv.org/pdf/2402.07630v3
Fast and Exact Enumeration of Deep Networks Partitions Regions,Randall Balestriero; Yann LeCun,2024,,0,,2401.11188,arxiv,2401.11188,cs.LG,cs.LG; cs.AI,"One fruitful formulation of Deep Networks (DNs) enabling their theoretical study and providing practical guidelines to practitioners relies on Piecewise Affine Splines. In that realm, a DN's input-mapping is expressed as per-region affine mapping where those regions are implicitly determined by the model's architecture and form a partition of their input space. That partition -- which is involved in all the results spanned from this line of research -- has so far only been computed on $2/3$-dimensional slices of the DN's input space or estimated by random sampling. In this paper, we provide the first parallel algorithm that does exact enumeration of the DN's partition regions. The proposed algorithm enables one to finally assess the closeness of the commonly employed approximations methods, e.g. based on random sampling of the DN input space. One of our key finding is that if one is only interested in regions with ``large'' volume, then uniform sampling of the space is highly efficient, but that if one is also interested in discovering the ``small'' regions of the partition, then uniform sampling is exponentially costly with the DN's input space dimension. On the other hand, our proposed method has complexity scaling linearly with input dimension and the number of regions.",https://arxiv.org/abs/2401.11188v1,https://arxiv.org/pdf/2401.11188v1
Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,Shengbang Tong; Zhuang Liu; Yuexiang Zhai; Yi Ma; Yann LeCun; Saining Xie,2024,,0,,2401.06209,arxiv,2401.06209,cs.CV,cs.CV,"Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",https://arxiv.org/abs/2401.06209v2,https://arxiv.org/pdf/2401.06209v2
Gradient-based Planning with World Models,Jyothir S; Siddhartha Jalagam; Yann LeCun; Vlad Sobal,2023,,0,,2312.17227,arxiv,2312.17227,cs.LG,cs.LG; cs.AI,"The enduring challenge in the field of artificial intelligence has been the control of systems to achieve desired behaviours. While for systems governed by straightforward dynamics equations, methods like Linear Quadratic Regulation (LQR) have historically proven highly effective, most real-world tasks, which require a general problem-solver, demand world models with dynamics that cannot be easily described by simple equations. Consequently, these models must be learned from data using neural networks. Most model predictive control (MPC) algorithms designed for visual world models have traditionally explored gradient-free population-based optimisation methods, such as Cross Entropy and Model Predictive Path Integral (MPPI) for planning. However, we present an exploration of a gradient-based alternative that fully leverages the differentiability of the world model. In our study, we conduct a comparative analysis between our method and other MPC-based alternatives, as well as policy-based algorithms. In a sample-efficient setting, our method achieves on par or superior performance compared to the alternative approaches in most tasks. Additionally, we introduce a hybrid model that combines policy networks and gradient-based MPC, which outperforms pure policy based methods thereby holding promise for Gradient-based planning with world models in complex real-world tasks.",https://arxiv.org/abs/2312.17227v1,https://arxiv.org/pdf/2312.17227v1
GAIA: a benchmark for General AI Assistants,Grégoire Mialon; Clémentine Fourrier; Craig Swift; Thomas Wolf; Yann LeCun; Thomas Scialom,2023,,0,,2311.12983,arxiv,2311.12983,cs.CL,cs.CL; cs.AI,"We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92\% vs. 15\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark.",https://arxiv.org/abs/2311.12983v1,https://arxiv.org/pdf/2311.12983v1
URLOST: Unsupervised Representation Learning without Stationarity or Topology,Zeyu Yun; Juexiao Zhang; Yann LeCun; Yubei Chen,2023,,0,,2310.04496,arxiv,2310.04496,cs.CV,cs.CV; cs.LG,"Unsupervised representation learning has seen tremendous progress. However, it is constrained by its reliance on domain specific stationarity and topology, a limitation not found in biological intelligence systems. For instance, unlike computer vision, human vision can process visual signals sampled from highly irregular and non-stationary sensors. We introduce a novel framework that learns from high-dimensional data without prior knowledge of stationarity and topology. Our model, abbreviated as URLOST, combines a learnable self-organizing layer, spectral clustering, and a masked autoencoder (MAE). We evaluate its effectiveness on three diverse data modalities including simulated biological vision data, neural recordings from the primary visual cortex, and gene expressions. Compared to state-of-the-art unsupervised learning methods like SimCLR and MAE, our model excels at learning meaningful representations across diverse modalities without knowing their stationarity or topology. It also outperforms other methods that are not dependent on these factors, setting a new benchmark in the field. We position this work as a step toward unsupervised learning methods capable of generalizing across diverse high-dimensional data modalities.",https://arxiv.org/abs/2310.04496v2,https://arxiv.org/pdf/2310.04496v2
Stochastic positional embeddings improve masked image modeling,Amir Bar; Florian Bordes; Assaf Shocher; Mahmoud Assran; Pascal Vincent; Nicolas Ballas; Trevor Darrell; Amir Globerson; Yann LeCun,2023,,0,,2308.00566,arxiv,2308.00566,cs.CV,cs.CV; cs.AI; cs.LG,"Masked Image Modeling (MIM) is a promising self-supervised learning approach that enables learning from unlabeled images. Despite its recent success, learning good representations through MIM remains challenging because it requires predicting the right semantic content in accurate locations. For example, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose to incorporate location uncertainty into MIM by using stochastic positional embeddings (StoP). Specifically, we condition the model on stochastic masked token positions drawn from a Gaussian distribution. StoP reduces overfitting to location features and guides the model toward learning features that are more robust to location uncertainties. Quantitatively, StoP improves downstream MIM performance on a variety of downstream tasks, including $+1.7\%$ on ImageNet linear probing using ViT-B, and $+2.5\%$ for ViT-H using $1\%$ of the data.",https://arxiv.org/abs/2308.00566v2,https://arxiv.org/pdf/2308.00566v2
MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features,Adrien Bardes; Jean Ponce; Yann LeCun,2023,,0,,2307.12698,arxiv,2307.12698,cs.CV,cs.CV; cs.AI; cs.LG,"Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.",https://arxiv.org/abs/2307.12698v1,https://arxiv.org/pdf/2307.12698v1
Self-Supervised Learning with Lie Symmetries for Partial Differential Equations,Grégoire Mialon; Quentin Garrido; Hannah Lawrence; Danyal Rehman; Yann LeCun; Bobak T. Kiani,2023,,0,,2307.05432,arxiv,2307.05432,cs.LG,cs.LG; math.NA,"Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation models for PDEs. Code: https://github.com/facebookresearch/SSLForPDEs.",https://arxiv.org/abs/2307.05432v2,https://arxiv.org/pdf/2307.05432v2
Variance-Covariance Regularization Improves Representation Learning,Jiachen Zhu; Katrina Evtimova; Yubei Chen; Ravid Shwartz-Ziv; Yann LeCun,2023,,0,,2306.13292,arxiv,2306.13292,cs.LG,cs.LG; cs.AI; cs.CV,"Transfer learning plays a key role in advancing machine learning models, yet conventional supervised pretraining often undermines feature transferability by prioritizing features that minimize the pretraining loss. In this work, we adapt a self-supervised learning regularization technique from the VICReg method to supervised learning contexts, introducing Variance-Covariance Regularization (VCReg). This adaptation encourages the network to learn high-variance, low-covariance representations, promoting learning more diverse features. We outline best practices for an efficient implementation of our framework, including applying it to the intermediate representations. Through extensive empirical evaluation, we demonstrate that our method significantly enhances transfer learning for images and videos, achieving state-of-the-art performance across numerous tasks and datasets. VCReg also improves performance in scenarios like long-tail learning and hierarchical classification. Additionally, we show its effectiveness may stem from its success in addressing challenges like gradient starvation and neural collapse. In summary, VCReg offers a universally applicable regularization framework that significantly advances transfer learning and highlights the connection between gradient starvation, neural collapse, and feature transferability.",https://arxiv.org/abs/2306.13292v2,https://arxiv.org/pdf/2306.13292v2
Introduction to Latent Variable Energy-Based Models: A Path Towards Autonomous Machine Intelligence,Anna Dawid; Yann LeCun,2023,,0,10.1088/1742-5468/ad292b,2306.02572,arxiv,2306.02572,cs.LG,cs.LG; cond-mat.dis-nn; stat.ML,"Current automated systems have crucial limitations that need to be addressed before artificial intelligence can reach human-like levels and bring new technological revolutions. Among others, our societies still lack Level 5 self-driving cars, domestic robots, and virtual assistants that learn reliable world models, reason, and plan complex action sequences. In these notes, we summarize the main ideas behind the architecture of autonomous intelligence of the future proposed by Yann LeCun. In particular, we introduce energy-based and latent variable models and combine their advantages in the building block of LeCun's proposal, that is, in the hierarchical joint embedding predictive architecture (H-JEPA).",https://arxiv.org/abs/2306.02572v1,https://arxiv.org/pdf/2306.02572v1
Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning,Xiaoxin He; Xavier Bresson; Thomas Laurent; Adam Perold; Yann LeCun; Bryan Hooi,2023,,0,,2305.19523,arxiv,2305.19523,cs.LG,cs.LG,"Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of explanations as features: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an LLM-to-LM interpreter to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data. Our codes and datasets are available at: https://github.com/XiaoxinHe/TAPE.",https://arxiv.org/abs/2305.19523v5,https://arxiv.org/pdf/2305.19523v5
Reverse Engineering Self-Supervised Learning,Ido Ben-Shaul; Ravid Shwartz-Ziv; Tomer Galanti; Shai Dekel; Yann LeCun,2023,,0,,2305.15614,arxiv,2305.15614,cs.LG,cs.LG; cs.AI,"Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provide valuable insights into SSL's representation learning mechanisms and their impact on performance across different sets of classes.",https://arxiv.org/abs/2305.15614v2,https://arxiv.org/pdf/2305.15614v2
A Cookbook of Self-Supervised Learning,Randall Balestriero; Mark Ibrahim; Vlad Sobal; Ari Morcos; Shashank Shekhar; Tom Goldstein; Florian Bordes; Adrien Bardes; Gregoire Mialon; Yuandong Tian; Avi Schwarzschild; Andrew Gordon Wilson; Jonas Geiping; Quentin Garrido; Pierre Fernandez; Amir Bar; Hamed Pirsiavash; Yann LeCun; Micah Goldblum,2023,,0,,2304.12210,arxiv,2304.12210,cs.LG,cs.LG; cs.CV,"Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.",https://arxiv.org/abs/2304.12210v2,https://arxiv.org/pdf/2304.12210v2
To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review,Ravid Shwartz-Ziv; Yann LeCun,2023,,0,,2304.09355,arxiv,2304.09355,cs.LG,cs.LG; cs.IT,"Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory, and notably the information bottleneck principle, has been pivotal in shaping deep neural networks. This principle focuses on optimizing the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the \textit{self-supervised information-theoretic learning problem}. We weave together existing research into a cohesive narrative, delve into contemporary self-supervised methodologies, and spotlight potential research avenues and inherent challenges. Additionally, we discuss the empirical evaluation of information-theoretic quantities and their estimation methods. Overall, this paper furnishes an exhaustive review of the intersection of information theory, self-supervised learning, and deep neural networks.",https://arxiv.org/abs/2304.09355v5,https://arxiv.org/pdf/2304.09355v5
EMP-SSL: Towards Self-Supervised Learning in One Training Epoch,Shengbang Tong; Yubei Chen; Yi Ma; Yann Lecun,2023,,0,,2304.03977,arxiv,2304.03977,cs.CV,cs.CV; cs.AI,"Recently, self-supervised learning (SSL) has achieved tremendous success in learning image representation. Despite the empirical success, most self-supervised learning methods are rather ""inefficient"" learners, typically taking hundreds of training epochs to fully converge. In this work, we show that the key towards efficient self-supervised learning is to increase the number of crops from each image instance. Leveraging one of the state-of-the-art SSL method, we introduce a simplistic form of self-supervised learning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL) that does not rely on many heuristic techniques for SSL such as weight sharing between the branches, feature-wise normalization, output quantization, and stop gradient, etc, and reduces the training epochs by two orders of magnitude. We show that the proposed method is able to converge to 85.1% on CIFAR-10, 58.5% on CIFAR-100, 38.1% on Tiny ImageNet and 58.5% on ImageNet-100 in just one epoch. Furthermore, the proposed method achieves 91.5% on CIFAR-10, 70.1% on CIFAR-100, 51.5% on Tiny ImageNet and 78.9% on ImageNet-100 with linear probing in less than ten training epochs. In addition, we show that EMP-SSL shows significantly better transferability to out-of-domain datasets compared to baseline SSL methods. We will release the code in https://github.com/tsb0601/EMP-SSL.",https://arxiv.org/abs/2304.03977v1,https://arxiv.org/pdf/2304.03977v1
Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need,Vivien Cabannes; Leon Bottou; Yann Lecun; Randall Balestriero,2023,,0,,2303.15256,arxiv,2303.15256,cs.LG,cs.LG; cs.AI; cs.HC,"Self-Supervised Learning (SSL) has emerged as the solution of choice to learn transferable representations from unlabeled data. However, SSL requires to build samples that are known to be semantically akin, i.e. positive views. Requiring such knowledge is the main limitation of SSL and is often tackled by ad-hoc strategies e.g. applying known data-augmentations to the same input. In this work, we formalize and generalize this principle through Positive Active Learning (PAL) where an oracle queries semantic relationships between samples. PAL achieves three main objectives. First, it unveils a theoretically grounded learning framework beyond SSL, based on similarity graphs, that can be extended to tackle supervised and semi-supervised learning depending on the employed oracle. Second, it provides a consistent algorithm to embed a priori knowledge, e.g. some observed labels, into any SSL losses without any change in the training pipeline. Third, it provides a proper active learning framework yielding low-cost solutions to annotate datasets, arguably bringing the gap between theory and practice of active learning that is based on simple-to-answer-by-non-experts queries of semantic relationships between inputs.",https://arxiv.org/abs/2303.15256v2,https://arxiv.org/pdf/2303.15256v2
An Information-Theoretic Perspective on Variance-Invariance-Covariance Regularization,Ravid Shwartz-Ziv; Randall Balestriero; Kenji Kawaguchi; Tim G. J. Rudner; Yann LeCun,2023,,0,,2303.00633,arxiv,2303.00633,cs.IT,cs.IT; cs.AI,"Variance-Invariance-Covariance Regularization (VICReg) is a self-supervised learning (SSL) method that has shown promising results on a variety of tasks. However, the fundamental mechanisms underlying VICReg remain unexplored. In this paper, we present an information-theoretic perspective on the VICReg objective. We begin by deriving information-theoretic quantities for deterministic networks as an alternative to unrealistic stochastic network assumptions. We then relate the optimization of the VICReg objective to mutual information optimization, highlighting underlying assumptions and facilitating a constructive comparison with other SSL algorithms and derive a generalization bound for VICReg, revealing its inherent advantages for downstream tasks. Building on these results, we introduce a family of SSL methods derived from information-theoretic principles that outperform existing SSL techniques.",https://arxiv.org/abs/2303.00633v4,https://arxiv.org/pdf/2303.00633v4
Augmented Language Models: a Survey,Grégoire Mialon; Roberto Dessì; Maria Lomeli; Christoforos Nalmpantis; Ram Pasunuru; Roberta Raileanu; Baptiste Rozière; Timo Schick; Jane Dwivedi-Yu; Asli Celikyilmaz; Edouard Grave; Yann LeCun; Thomas Scialom,2023,,0,,2302.07842,arxiv,2302.07842,cs.CL,cs.CL,"This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.",https://arxiv.org/abs/2302.07842v1,https://arxiv.org/pdf/2302.07842v1
Self-supervised learning of Split Invariant Equivariant representations,Quentin Garrido; Laurent Najman; Yann Lecun,2023,,0,,2302.10283,arxiv,2302.10283,cs.CV,cs.CV; cs.AI; cs.LG,"Recent progress has been made towards learning invariant or equivariant representations with self-supervised learning. While invariant methods are evaluated on large scale datasets, equivariant ones are evaluated in smaller, more controlled, settings. We aim at bridging the gap between the two in order to learn more diverse representations that are suitable for a wide range of tasks. We start by introducing a dataset called 3DIEBench, consisting of renderings from 3D models over  55 classes and more than 2.5 million images where we have full control on the transformations applied to the objects. We further introduce a predictor architecture based on hypernetworks to learn equivariant representations with no possible collapse to invariance. We introduce SIE (Split Invariant-Equivariant) which combines the hypernetwork-based predictor with representations split in two parts, one invariant, the other equivariant, to learn richer representations. We demonstrate significant performance gains over existing methods on equivariance related tasks from both a qualitative and quantitative point of view. We further analyze our introduced predictor and show how it steers the learned latent space. We hope that both our introduced dataset and approach will enable learning richer representations without supervision in more complex scenarios. Code and data are available at https://github.com/facebookresearch/SIE.",https://arxiv.org/abs/2302.10283v2,https://arxiv.org/pdf/2302.10283v2
"The SSL Interplay: Augmentations, Inductive Bias, and Generalization",Vivien Cabannes; Bobak T. Kiani; Randall Balestriero; Yann LeCun; Alberto Bietti,2023,,0,,2302.02774,arxiv,2302.02774,stat.ML,stat.ML; cs.AI; cs.LG; math.ST,"Self-supervised learning (SSL) has emerged as a powerful framework to learn representations from raw data without supervision. Yet in practice, engineers face issues such as instability in tuning optimizers and collapse of representations during training. Such challenges motivate the need for a theory to shed light on the complex interplay between the choice of data augmentation, network architecture, and training algorithm. We study such an interplay with a precise analysis of generalization performance on both pretraining and downstream tasks in a theory friendly setup, and highlight several insights for SSL practitioners that arise from our theory.",https://arxiv.org/abs/2302.02774v2,https://arxiv.org/pdf/2302.02774v2
Blockwise Self-Supervised Learning at Scale,Shoaib Ahmed Siddiqui; David Krueger; Yann LeCun; Stéphane Deny,2023,,0,,2302.01647,arxiv,2302.01647,cs.CV,cs.CV; cs.AI; cs.LG,"Current state-of-the-art deep networks are all powered by backpropagation. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.",https://arxiv.org/abs/2302.01647v2,https://arxiv.org/pdf/2302.01647v2
Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,Mahmoud Assran; Quentin Duval; Ishan Misra; Piotr Bojanowski; Pascal Vincent; Michael Rabbat; Yann LeCun; Nicolas Ballas,2023,,0,,2301.08243,arxiv,2301.08243,cs.CV,cs.CV; cs.AI; cs.LG; eess.IV,"This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.",https://arxiv.org/abs/2301.08243v3,https://arxiv.org/pdf/2301.08243v3
A Generalization of ViT/MLP-Mixer to Graphs,Xiaoxin He; Bryan Hooi; Thomas Laurent; Adam Perold; Yann LeCun; Xavier Bresson,2022,,0,,2212.13350,arxiv,2212.13350,cs.CV,cs.CV,"Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning. Standard GNNs define a local message-passing mechanism which propagates information over the whole graph domain by stacking multiple layers. This paradigm suffers from two major limitations, over-squashing and poor long-range dependencies, that can be solved using global attention but significantly increases the computational cost to quadratic complexity. In this work, we propose an alternative approach to overcome these structural limitations by leveraging the ViT/MLP-Mixer architectures introduced in computer vision. We introduce a new class of GNNs, called Graph ViT/MLP-Mixer, that holds three key properties. First, they capture long-range dependency and mitigate the issue of over-squashing as demonstrated on Long Range Graph Benchmark and TreeNeighbourMatch datasets. Second, they offer better speed and memory efficiency with a complexity linear to the number of nodes and edges, surpassing the related Graph Transformer and expressive GNN models. Third, they show high expressivity in terms of graph isomorphism as they can distinguish at least 3-WL non-isomorphic graphs. We test our architecture on 4 simulated datasets and 7 real-world benchmarks, and show highly competitive results on all of them. The source code is available for reproducibility at: \url{https://github.com/XiaoxinHe/Graph-ViT-MLPMixer}.",https://arxiv.org/abs/2212.13350v2,https://arxiv.org/pdf/2212.13350v2
Joint Embedding Predictive Architectures Focus on Slow Features,Vlad Sobal; Jyothir S; Siddhartha Jalagam; Nicolas Carion; Kyunghyun Cho; Yann LeCun,2022,,0,,2211.10831,arxiv,2211.10831,cs.LG,cs.LG,"Many common methods for learning a world model for pixel-based environments use generative architectures trained with pixel-level reconstruction objectives. Recently proposed Joint Embedding Predictive Architectures (JEPA) offer a reconstruction-free alternative. In this work, we analyze performance of JEPA trained with VICReg and SimCLR objectives in the fully offline setting without access to rewards, and compare the results to the performance of the generative architecture. We test the methods in a simple environment with a moving dot with various background distractors, and probe learned representations for the dot's location. We find that JEPA methods perform on par or better than reconstruction when distractor noise changes every time step, but fail when the noise is fixed. Furthermore, we provide a theoretical explanation for the poor performance of JEPA-based methods with fixed noise, highlighting an important limitation.",https://arxiv.org/abs/2211.10831v1,https://arxiv.org/pdf/2211.10831v1
POLICE: Provably Optimal Linear Constraint Enforcement for Deep Neural Networks,Randall Balestriero; Yann LeCun,2022,,0,,2211.01340,arxiv,2211.01340,cs.LG,cs.LG; cs.CV; stat.ML,"Deep Neural Networks (DNNs) outshine alternative function approximators in many settings thanks to their modularity in composing any desired differentiable operator. The formed parametrized functional is then tuned to solve a task at hand from simple gradient descent. This modularity comes at the cost of making strict enforcement of constraints on DNNs, e.g. from a priori knowledge of the task, or from desired physical properties, an open challenge. In this paper we propose the first provable affine constraint enforcement method for DNNs that only requires minimal changes into a given DNN's forward-pass, that is computationally friendly, and that leaves the optimization of the DNN's parameter to be unconstrained, i.e. standard gradient-based method can be employed. Our method does not require any sampling and provably ensures that the DNN fulfills the affine constraint on a given input space's region at any point during training, and testing. We coin this method POLICE, standing for Provably Optimal LInear Constraint Enforcement. Github: https://github.com/RandallBalestriero/POLICE",https://arxiv.org/abs/2211.01340v3,https://arxiv.org/pdf/2211.01340v3
Unsupervised Learning of Structured Representations via Closed-Loop Transcription,Shengbang Tong; Xili Dai; Yubei Chen; Mingyang Li; Zengyi Li; Brent Yi; Yann LeCun; Yi Ma,2022,,0,,2210.16782,arxiv,2210.16782,cs.CV,cs.CV,"This paper proposes an unsupervised method for learning a unified representation that serves both discriminative and generative purposes. While most existing unsupervised learning approaches focus on a representation for only one of these two goals, we show that a unified representation can enjoy the mutual benefits of having both. Such a representation is attainable by generalizing the recently proposed \textit{closed-loop transcription} framework, known as CTRL, to the unsupervised setting. This entails solving a constrained maximin game over a rate reduction objective that expands features of all samples while compressing features of augmentations of each sample. Through this process, we see discriminative low-dimensional structures emerge in the resulting representations. Under comparable experimental conditions and network complexities, we demonstrate that these structured representations enable classification performance close to state-of-the-art unsupervised discriminative representations, and conditionally generated image quality significantly higher than that of state-of-the-art unsupervised generative models. Source code can be found at https://github.com/Delay-Xili/uCTRL.",https://arxiv.org/abs/2210.16782v1,https://arxiv.org/pdf/2210.16782v1
Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution,Anthony Zador; Sean Escola; Blake Richards; Bence Ölveczky; Yoshua Bengio; Kwabena Boahen; Matthew Botvinick; Dmitri Chklovskii; Anne Churchland; Claudia Clopath; James DiCarlo; Surya Ganguli; Jeff Hawkins; Konrad Koerding; Alexei Koulakov; Yann LeCun; Timothy Lillicrap; Adam Marblestone; Bruno Olshausen; Alexandre Pouget; Cristina Savin; Terrence Sejnowski; Eero Simoncelli; Sara Solla; David Sussillo; Andreas S. Tolias; Doris Tsao,2022,,0,,2210.08340,arxiv,2210.08340,cs.AI,cs.AI; q-bio.NC,"Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities, inherited from over 500 million years of evolution, that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI.",https://arxiv.org/abs/2210.08340v3,https://arxiv.org/pdf/2210.08340v3
VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment,Shraman Pramanick; Li Jing; Sayan Nag; Jiachen Zhu; Hardik Shah; Yann LeCun; Rama Chellappa,2022,,0,,2210.04135,arxiv,2210.04135,cs.CV,cs.CV; cs.LG; cs.MM,"Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision-Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-modal fusion deep into the uni-modal backbones during pre-training and removes fusion-specific transformer layers, further reducing memory requirements. Extensive experiments on a wide range of vision- and vision-language downstream tasks demonstrate the effectiveness of VoLTA on fine-grained applications without compromising the coarse-grained downstream performance, often outperforming methods using significantly more caption and box annotations.",https://arxiv.org/abs/2210.04135v3,https://arxiv.org/pdf/2210.04135v3
RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank,Quentin Garrido; Randall Balestriero; Laurent Najman; Yann Lecun,2022,,0,,2210.02885,arxiv,2210.02885,cs.LG,cs.LG; cs.AI; cs.CV,"Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations but only few principled guidelines that would help practitioners to successfully deploy them. The main reason for that pitfall comes from JE-SSL's core principle of not employing any input reconstruction therefore lacking visual cues of unsuccessful training. Adding non informative loss values to that, it becomes difficult to deploy SSL on a new dataset for which no labels can help to judge the quality of the learned representation. In this study, we develop a simple unsupervised criterion that is indicative of the quality of the learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method -- coined RankMe -- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels. A further benefit of RankMe is that it does not have any training or hyper-parameters to tune. Through thorough empirical experiments involving hundreds of training episodes, we demonstrate how RankMe can be used for hyperparameter selection with nearly no reduction in final performance compared to the current selection method that involve a dataset's labels. We hope that RankMe will facilitate the deployment of JE-SSL towards domains that do not have the opportunity to rely on labels for representations' quality assessment.",https://arxiv.org/abs/2210.02885v3,https://arxiv.org/pdf/2210.02885v3
VICRegL: Self-Supervised Learning of Local Visual Features,Adrien Bardes; Jean Ponce; Yann LeCun,2022,,0,,2210.01571,arxiv,2210.01571,cs.CV,cs.CV; cs.AI; cs.LG,"Most recent self-supervised methods for learning image representations focus on either producing a global feature with invariance properties, or producing a set of local features. The former works best for classification tasks while the latter is best for detection and segmentation tasks. This paper explores the fundamental trade-off between learning local and global features. A new method called VICRegL is proposed that learns good global and local features simultaneously, yielding excellent performance on detection and segmentation tasks while maintaining good performance on classification tasks. Concretely, two identical branches of a standard convolutional net architecture are fed two differently distorted versions of the same image. The VICReg criterion is applied to pairs of global feature vectors. Simultaneously, the VICReg criterion is applied to pairs of local feature vectors occurring before the last pooling layer. Two local feature vectors are attracted to each other if their l2-distance is below a threshold or if their relative locations are consistent with a known geometric transformation between the two input images. We demonstrate strong performance on linear classification and segmentation transfer tasks. Code and pretrained models are publicly available at: https://github.com/facebookresearch/VICRegL",https://arxiv.org/abs/2210.01571v1,https://arxiv.org/pdf/2210.01571v1
Minimalistic Unsupervised Learning with the Sparse Manifold Transform,Yubei Chen; Zeyu Yun; Yi Ma; Bruno Olshausen; Yann LeCun,2022,,0,,2209.15261,arxiv,2209.15261,cs.LG,cs.LG; cs.CV; stat.ML,"We describe a minimalistic and interpretable method for unsupervised learning, without resorting to data augmentation, hyperparameter tuning, or other engineering designs, that achieves performance close to the SOTA SSL methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic sparse manifold transform, one can achieve 99.3% KNN top-1 accuracy on MNIST, 81.1% KNN top-1 accuracy on CIFAR-10 and 53.2% on CIFAR-100. With a simple gray-scale augmentation, the model gets 83.2% KNN top-1 accuracy on CIFAR-10 and 57% on CIFAR-100. These results significantly close the gap between simplistic ""white-box"" methods and the SOTA methods. Additionally, we provide visualization to explain how an unsupervised representation transform is formed. The proposed method is closely connected to latent-embedding self-supervised methods and can be treated as the simplest form of VICReg. Though there remains a small performance gap between our simple constructive model and SOTA methods, the evidence points to this as a promising direction for achieving a principled and white-box approach to unsupervised learning.",https://arxiv.org/abs/2209.15261v2,https://arxiv.org/pdf/2209.15261v2
Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations,Grégoire Mialon; Randall Balestriero; Yann LeCun,2022,,0,,2209.14905,arxiv,2209.14905,cs.LG,cs.LG,"Self-Supervised Learning (SSL) methods such as VICReg, Barlow Twins or W-MSE avoid collapse of their joint embedding architectures by constraining or regularizing the covariance matrix of their projector's output. This study highlights important properties of such strategy, which we coin Variance-Covariance regularization (VCReg). More precisely, we show that {\em VCReg combined to a MLP projector enforces pairwise independence between the features of the learned representation}. This result emerges by bridging VCReg applied on the projector's output to kernel independence criteria applied on the projector's input. We empirically validate our findings where (i) we put in evidence which projector's characteristics favor pairwise independence, (ii) we demonstrate pairwise independence to be beneficial for out-of-domain generalization, (iii) we demonstrate that the scope of VCReg goes beyond SSL by using it to solve Independent Component Analysis. This provides the first theoretical motivation and explanation of MLP projectors in SSL.",https://arxiv.org/abs/2209.14905v2,https://arxiv.org/pdf/2209.14905v2
Joint Embedding Self-Supervised Learning in the Kernel Regime,Bobak T. Kiani; Randall Balestriero; Yubei Chen; Seth Lloyd; Yann LeCun,2022,,0,,2209.14884,arxiv,2209.14884,cs.LG,cs.LG; cs.AI; stat.ML,"The fundamental goal of self-supervised learning (SSL) is to produce useful representations of data without access to any labels for classifying the data. Modern methods in SSL, which form representations based on known or constructed relationships between samples, have been particularly effective at this task. Here, we aim to extend this framework to incorporate algorithms based on kernel methods where embeddings are constructed by linear maps acting on the feature space of a kernel. In this kernel regime, we derive methods to find the optimal form of the output representations for contrastive and non-contrastive loss functions. This procedure produces a new representation space with an inner product denoted as the induced kernel which generally correlates points which are related by an augmentation in kernel space and de-correlates points otherwise. We analyze our kernel model on small datasets to identify common features of self-supervised learning algorithms and gain theoretical insights into their performance on downstream tasks.",https://arxiv.org/abs/2209.14884v1,https://arxiv.org/pdf/2209.14884v1
Light-weight probing of unsupervised representations for Reinforcement Learning,Wancong Zhang; Anthony GX-Chen; Vlad Sobal; Yann LeCun; Nicolas Carion,2022,,0,,2208.12345,arxiv,2208.12345,cs.LG,cs.LG; cs.AI,"Unsupervised visual representation learning offers the opportunity to leverage large corpora of unlabeled trajectories to form useful visual representations, which can benefit the training of reinforcement learning (RL) algorithms. However, evaluating the fitness of such representations requires training RL algorithms which is computationally intensive and has high variance outcomes. Inspired by the vision community, we study whether linear probing can be a proxy evaluation task for the quality of unsupervised RL representation. Specifically, we probe for the observed reward in a given state and the action of an expert in a given state, both of which are generally applicable to many RL domains. Through rigorous experimentation, we show that the probing tasks are strongly rank correlated with the downstream RL performance on the Atari100k Benchmark, while having lower variance and up to 600x lower computational cost. This provides a more efficient method for exploring the space of pretraining algorithms and identifying promising pretraining recipes without the need to run RL evaluations for every setting. Leveraging this framework, we further improve existing self-supervised learning (SSL) recipes for RL, highlighting the importance of the forward model, the size of the visual backbone, and the precise formulation of the unsupervised objective.",https://arxiv.org/abs/2208.12345v2,https://arxiv.org/pdf/2208.12345v2
What Do We Maximize in Self-Supervised Learning?,Ravid Shwartz-Ziv; Randall Balestriero; Yann LeCun,2022,,0,,2207.10081,arxiv,2207.10081,cs.LG,cs.LG; cs.AI,"In this paper, we examine self-supervised learning methods, particularly VICReg, to provide an information-theoretical understanding of their construction. As a first step, we demonstrate how information-theoretic quantities can be obtained for a deterministic network, offering a possible alternative to prior work that relies on stochastic models. This enables us to demonstrate how VICReg can be (re)discovered from first principles and its assumptions about data distribution. Furthermore, we empirically demonstrate the validity of our assumptions, confirming our novel understanding of VICReg. Finally, we believe that the derivation and insights we obtain can be generalized to many other SSL methods, opening new avenues for theoretical and practical understanding of SSL and transfer learning.",https://arxiv.org/abs/2207.10081v1,https://arxiv.org/pdf/2207.10081v1
TiCo: Transformation Invariance and Covariance Contrast for Self-Supervised Visual Representation Learning,Jiachen Zhu; Rafael M. Moraes; Serkan Karakulak; Vlad Sobol; Alfredo Canziani; Yann LeCun,2022,,0,,2206.10698,arxiv,2206.10698,cs.CV,cs.CV; cs.AI; cs.LG,"We present Transformation Invariance and Covariance Contrast (TiCo) for self-supervised visual representation learning. Similar to other recent self-supervised learning methods, our method is based on maximizing the agreement among embeddings of different distorted versions of the same image, which pushes the encoder to produce transformation invariant representations. To avoid the trivial solution where the encoder generates constant vectors, we regularize the covariance matrix of the embeddings from different images by penalizing low rank solutions. By jointly minimizing the transformation invariance loss and covariance contrast loss, we get an encoder that is able to produce useful representations for downstream tasks. We analyze our method and show that it can be viewed as a variant of MoCo with an implicit memory bank of unlimited size at no extra memory cost. This makes our method perform better than alternative methods when using small batch sizes. TiCo can also be seen as a modification of Barlow Twins. By connecting the contrastive and redundancy-reduction methods together, TiCo gives us new insights into how joint embedding methods work.",https://arxiv.org/abs/2206.10698v2,https://arxiv.org/pdf/2206.10698v2
Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning,Yubei Chen; Adrien Bardes; Zengyi Li; Yann LeCun,2022,,0,,2206.08954,arxiv,2206.08954,cs.CV,cs.CV; cs.LG,"Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches primarily learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as BagSSL. Even with 32x32 patch representation, BagSSL achieves 62% top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the SSL representation is relatively invariant at the global scale, we show that locality is preserved when we zoom into local patch-level representation. Further, we show that patch representation aggregation can improve various SOTA baseline methods by a large margin. The patch representation is considerably easier to understand, and this work makes a step to demystify self-supervised representation learning.",https://arxiv.org/abs/2206.08954v2,https://arxiv.org/pdf/2206.08954v2
Masked Siamese ConvNets,Li Jing; Jiachen Zhu; Yann LeCun,2022,,0,,2206.07700,arxiv,2206.07700,cs.CV,cs.CV; cs.AI; cs.LG,"Self-supervised learning has shown superior performances over supervised methods on various vision benchmarks. The siamese network, which encourages embeddings to be invariant to distortions, is one of the most successful self-supervised visual representation learning approaches. Among all the augmentation methods, masking is the most general and straightforward method that has the potential to be applied to all kinds of input and requires the least amount of domain knowledge. However, masked siamese networks require particular inductive bias and practically only work well with Vision Transformers. This work empirically studies the problems behind masked siamese networks with ConvNets. We propose several empirical designs to overcome these problems gradually. Our method performs competitively on low-shot image classification and outperforms previous methods on object detection benchmarks. We discuss several remaining issues and hope this work can provide useful data points for future general-purpose self-supervised learning.",https://arxiv.org/abs/2206.07700v1,https://arxiv.org/pdf/2206.07700v1
Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone,Zi-Yi Dou; Aishwarya Kamath; Zhe Gan; Pengchuan Zhang; Jianfeng Wang; Linjie Li; Zicheng Liu; Ce Liu; Yann LeCun; Nanyun Peng; Jianfeng Gao; Lijuan Wang,2022,,0,,2206.07643,arxiv,2206.07643,cs.CV,cs.CV; cs.CL; cs.LG,"Vision-language (VL) pre-training has recently received considerable attention. However, most existing end-to-end pre-training approaches either only aim to tackle VL tasks such as image-text retrieval, visual question answering (VQA) and image captioning that test high-level understanding of images, or only target region-level understanding for tasks such as phrase grounding and object detection. We present FIBER (Fusion-In-the-Backbone-based transformER), a new VL model architecture that can seamlessly handle both these types of tasks. Instead of having dedicated transformer layers for fusion after the uni-modal backbones, FIBER pushes multimodal fusion deep into the model by inserting cross-attention into the image and text backbones, bringing gains in terms of memory and performance. In addition, unlike previous work that is either only pre-trained on image-text data or on fine-grained data with box-level annotations, we present a two-stage pre-training strategy that uses both these kinds of data efficiently: (i) coarse-grained pre-training based on image-text data; followed by (ii) fine-grained pre-training based on image-text-box data. We conduct comprehensive experiments on a wide range of VL tasks, ranging from VQA, image captioning, and retrieval, to phrase grounding, referring expression comprehension, and object detection. Using deep multimodal fusion coupled with the two-stage pre-training, FIBER provides consistent performance improvements over strong baselines across all tasks, often outperforming methods using magnitudes more data. Code is available at https://github.com/microsoft/FIBER.",https://arxiv.org/abs/2206.07643v2,https://arxiv.org/pdf/2206.07643v2
On the duality between contrastive and non-contrastive self-supervised learning,Quentin Garrido; Yubei Chen; Adrien Bardes; Laurent Najman; Yann Lecun,2022,,0,10.48550/arXiv.2206.02574,2206.02574,arxiv,2206.02574,cs.LG,cs.LG; cs.AI; cs.CV,"Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning.",https://arxiv.org/abs/2206.02574v3,https://arxiv.org/pdf/2206.02574v3
Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods,Randall Balestriero; Yann LeCun,2022,,0,,2205.11508,arxiv,2205.11508,cs.LG,cs.LG; cs.AI; cs.CV; math.SP; stat.ML,"Self-Supervised Learning (SSL) surmises that inputs and pairwise positive relationships are enough to learn meaningful representations. Although SSL has recently reached a milestone: outperforming supervised methods in many modalities\dots the theoretical foundations are limited, method-specific, and fail to provide principled design guidelines to practitioners. In this paper, we propose a unifying framework under the helm of spectral manifold learning to address those limitations. Through the course of this study, we will rigorously demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous spectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al.   This unification will then allow us to obtain (i) the closed-form optimal representation for each method, (ii) the closed-form optimal network parameters in the linear regime for each method, (iii) the impact of the pairwise relations used during training on each of those quantities and on downstream task performances, and most importantly, (iv) the first theoretical bridge between contrastive and non-contrastive methods towards global and local spectral embedding methods respectively, hinting at the benefits and limitations of each. For example, (i) if the pairwise relation is aligned with the downstream task, any SSL method can be employed successfully and will recover the supervised method, but in the low data regime, VICReg's invariance hyper-parameter should be high; (ii) if the pairwise relation is misaligned with the downstream task, VICReg with small invariance hyper-parameter should be preferred over SimCLR or BarlowTwins.",https://arxiv.org/abs/2205.11508v3,https://arxiv.org/pdf/2205.11508v3
Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors,Ravid Shwartz-Ziv; Micah Goldblum; Hossein Souri; Sanyam Kapoor; Chen Zhu; Yann LeCun; Andrew Gordon Wilson,2022,,0,,2205.10279,arxiv,2205.10279,cs.LG,cs.LG; cs.CV,"Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.",https://arxiv.org/abs/2205.10279v1,https://arxiv.org/pdf/2205.10279v1
Separating the World and Ego Models for Self-Driving,Vlad Sobal; Alfredo Canziani; Nicolas Carion; Kyunghyun Cho; Yann LeCun,2022,,0,,2204.07184,arxiv,2204.07184,cs.RO,cs.RO,"Training self-driving systems to be robust to the long-tail of driving scenarios is a critical problem. Model-based approaches leverage simulation to emulate a wide range of scenarios without putting users at risk in the real world. One promising path to faithful simulation is to train a forward model of the world to predict the future states of both the environment and the ego-vehicle given past states and a sequence of actions. In this paper, we argue that it is beneficial to model the state of the ego-vehicle, which often has simple, predictable and deterministic behavior, separately from the rest of the environment, which is much more complex and highly multimodal. We propose to model the ego-vehicle using a simple and differentiable kinematic model, while training a stochastic convolutional forward model on raster representations of the state to predict the behavior of the rest of the environment. We explore several configurations of such decoupled models, and evaluate their performance both with Model Predictive Control (MPC) and direct policy learning. We test our methods on the task of highway driving and demonstrate lower crash rates and better stability. The code is available at https://github.com/vladisai/pytorch-PPUU/tree/ICLR2022.",https://arxiv.org/abs/2204.07184v1,https://arxiv.org/pdf/2204.07184v1
The Effects of Regularization and Data Augmentation are Class Dependent,Randall Balestriero; Leon Bottou; Yann LeCun,2022,,0,,2204.03632,arxiv,2204.03632,cs.LG,cs.LG; cs.CV; stat.ML,"Regularization is a fundamental technique to prevent over-fitting and to improve generalization performances by constraining a model's complexity. Current Deep Networks heavily rely on regularizers such as Data-Augmentation (DA) or weight-decay, and employ structural risk minimization, i.e. cross-validation, to select the optimal regularization hyper-parameters. In this study, we demonstrate that techniques such as DA or weight decay produce a model with a reduced complexity that is unfair across classes. The optimal amount of DA or weight decay found from cross-validation leads to disastrous model performances on some classes e.g. on Imagenet with a resnet50, the ""barn spider"" classification test accuracy falls from $68\%$ to $46\%$ only by introducing random crop DA during training. Even more surprising, such performance drop also appears when introducing uninformative regularization techniques such as weight decay. Those results demonstrate that our search for ever increasing generalization performance -- averaged over all classes and samples -- has left us with models and regularizers that silently sacrifice performances on some classes. This scenario can become dangerous when deploying a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on INaturalist sees its performances fall from $70\%$ to $30\%$ on class \#8889 when introducing random crop DA during the Imagenet pre-training phase. Those results demonstrate that designing novel regularizers without class-dependent bias remains an open research question.",https://arxiv.org/abs/2204.03632v2,https://arxiv.org/pdf/2204.03632v2
projUNN: efficient method for training deep networks with unitary matrices,Bobak Kiani; Randall Balestriero; Yann LeCun; Seth Lloyd,2022,,0,,2203.05483,arxiv,2203.05483,cs.LG,cs.LG; cs.AI; quant-ph,"In learning with recurrent or very deep feed-forward networks, employing unitary matrices in each layer can be very effective at maintaining long-range stability. However, restricting network parameters to be unitary typically comes at the cost of expensive parameterizations or increased training runtime. We propose instead an efficient method based on rank-$k$ updates -- or their rank-$k$ approximation -- that maintains performance at a nearly optimal training runtime. We introduce two variants of this method, named Direct (projUNN-D) and Tangent (projUNN-T) projected Unitary Neural Networks, that can parameterize full $N$-dimensional unitary or orthogonal matrices with a training runtime scaling as $O(kN^2)$. Our method either projects low-rank gradients onto the closest unitary matrix (projUNN-T) or transports unitary matrices in the direction of the low-rank gradient (projUNN-D). Even in the fastest setting ($k=1$), projUNN is able to train a model's unitary parameters to reach comparable performances against baseline implementations. In recurrent neural network settings, projUNN closely matches or exceeds benchmarked results from prior unitary neural networks. Finally, we preliminarily explore projUNN in training orthogonal convolutional neural networks, which are currently unable to outperform state of the art models but can potentially enhance stability and robustness at large depth.",https://arxiv.org/abs/2203.05483v3,https://arxiv.org/pdf/2203.05483v3
A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments,Randall Balestriero; Ishan Misra; Yann LeCun,2022,,0,,2202.08325,arxiv,2202.08325,cs.LG,cs.LG; cs.CV,"Data-Augmentation (DA) is known to improve performance across tasks and datasets. We propose a method to theoretically analyze the effect of DA and study questions such as: how many augmented samples are needed to correctly estimate the information encoded by that DA? How does the augmentation policy impact the final parameters of a model? We derive several quantities in close-form, such as the expectation and variance of an image, loss, and model's output under a given DA distribution. Those derivations open new avenues to quantify the benefits and limitations of DA. For example, we show that common DAs require tens of thousands of samples for the loss at hand to be correctly estimated and for the model training to converge. We show that for a training loss to be stable under DA sampling, the model's saliency map (gradient of the loss with respect to the model's input) must align with the smallest eigenvector of the sample variance under the considered DA augmentation, hinting at a possible explanation on why models tend to shift their focus from edges to textures.",https://arxiv.org/abs/2202.08325v1,https://arxiv.org/pdf/2202.08325v1
Neural Manifold Clustering and Embedding,Zengyi Li; Yubei Chen; Yann LeCun; Friedrich T. Sommer,2022,,0,,2201.10000,arxiv,2201.10000,cs.LG,cs.LG; cs.CV,"Given a union of non-linear manifolds, non-linear subspace clustering or manifold clustering aims to cluster data points based on manifold structures and also learn to parameterize each manifold as a linear subspace in a feature space. Deep neural networks have the potential to achieve this goal under highly non-linear settings given their large capacity and flexibility. We argue that achieving manifold clustering with neural networks requires two essential ingredients: a domain-specific constraint that ensures the identification of the manifolds, and a learning algorithm for embedding each manifold to a linear subspace in the feature space. This work shows that many constraints can be implemented by data augmentation. For subspace feature learning, Maximum Coding Rate Reduction (MCR$^2$) objective can be used. Putting them together yields {\em Neural Manifold Clustering and Embedding} (NMCE), a novel method for general purpose manifold clustering, which significantly outperforms autoencoder-based deep subspace clustering. Further, on more challenging natural image datasets, NMCE can also outperform other algorithms specifically designed for clustering. Qualitatively, we demonstrate that NMCE learns a meaningful and interpretable feature space. As the formulation of NMCE is closely related to several important Self-supervised learning (SSL) methods, we believe this work can help us build a deeper understanding on SSL representation learning.",https://arxiv.org/abs/2201.10000v1,https://arxiv.org/pdf/2201.10000v1
Sparse Coding with Multi-Layer Decoders using Variance Regularization,Katrina Evtimova; Yann LeCun,2021,,0,,2112.09214,arxiv,2112.09214,cs.CV,cs.CV; cs.LG,"Sparse representations of images are useful in many computer vision applications. Sparse coding with an $l_1$ penalty and a learned linear dictionary requires regularization of the dictionary to prevent a collapse in the $l_1$ norms of the codes. Typically, this regularization entails bounding the Euclidean norms of the dictionary's elements. In this work, we propose a novel sparse coding protocol which prevents a collapse in the codes without the need to regularize the decoder. Our method regularizes the codes directly so that each latent code component has variance greater than a fixed threshold over a set of sparse representations for a given set of inputs. Furthermore, we explore ways to effectively train sparse coding systems with multi-layer decoders since they can model more complex relationships than linear dictionaries. In our experiments with MNIST and natural image patches, we show that decoders learned with our approach have interpretable features both in the linear and multi-layer case. Moreover, we show that sparse autoencoders with multi-layer decoders trained using our variance regularization method produce higher quality reconstructions with sparser representations when compared to autoencoders with linear dictionaries. Additionally, sparse representations obtained with our variance regularization approach are useful in the downstream tasks of denoising and classification in the low-data regime.",https://arxiv.org/abs/2112.09214v2,https://arxiv.org/pdf/2112.09214v2
Learning in High Dimension Always Amounts to Extrapolation,Randall Balestriero; Jerome Pesenti; Yann LeCun,2021,,0,,2110.09485,arxiv,2110.09485,cs.LG,cs.LG; cs.CV,"The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample $x$ whenever this sample falls inside or on the boundary of the given dataset's convex hull. Extrapolation occurs when $x$ falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional ($>$100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation definition as an indicator of generalization performances.",https://arxiv.org/abs/2110.09485v2,https://arxiv.org/pdf/2110.09485v2
Understanding Dimensional Collapse in Contrastive Self-supervised Learning,Li Jing; Pascal Vincent; Yann LeCun; Yuandong Tian,2021,,0,,2110.09348,arxiv,2110.09348,cs.CV,cs.CV; cs.AI; cs.LG,"Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",https://arxiv.org/abs/2110.09348v3,https://arxiv.org/pdf/2110.09348v3
Decoupled Contrastive Learning,Chun-Hsiao Yeh; Cheng-Yao Hong; Yen-Chi Hsu; Tyng-Luh Liu; Yubei Chen; Yann LeCun,2021,,0,,2110.06848,arxiv,2110.06848,cs.LG,cs.LG; cs.CV,"Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented ""views"" of the same image as positive to be pulled closer, and all other images as negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, including large sample batches, extensive training epochs, etc. We are thus motivated to tackle these issues and establish a simple, efficient, yet competitive baseline of contrastive learning. Specifically, we identify, from theoretical and empirical studies, a noticeable negative-positive-coupling (NPC) effect in the widely used InfoNCE loss, leading to unsuitable learning efficiency concerning the batch size. By removing the NPC effect, we propose decoupled contrastive learning (DCL) loss, which removes the positive term from the denominator and significantly improves the learning efficiency. DCL achieves competitive performance with less sensitivity to sub-optimal hyperparameters, requiring neither large batches in SimCLR, momentum encoding in MoCo, or large epochs. We demonstrate with various benchmarks while manifesting robustness as much less sensitive to suboptimal hyperparameters. Notably, SimCLR with DCL achieves 68.2% ImageNet-1K top-1 accuracy using batch size 256 within 200 epochs pre-training, outperforming its SimCLR baseline by 6.4%. Further, DCL can be combined with the SOTA contrastive learning method, NNCLR, to achieve 72.3% ImageNet-1K top-1 accuracy with 512 batch size in 400 epochs, which represents a new SOTA in contrastive learning. We believe DCL provides a valuable baseline for future contrastive SSL studies.",https://arxiv.org/abs/2110.06848v3,https://arxiv.org/pdf/2110.06848v3
Compact and Optimal Deep Learning with Recurrent Parameter Generators,Jiayun Wang; Yubei Chen; Stella X. Yu; Brian Cheung; Yann LeCun,2021,,0,,2107.07110,arxiv,2107.07110,cs.CV,cs.CV; cs.LG,"Deep learning has achieved tremendous success by training increasingly large models, which are then compressed for practical deployment. We propose a drastically different approach to compact and optimal deep learning: We decouple the Degrees of freedom (DoF) and the actual number of parameters of a model, optimize a small DoF with predefined random linear constraints for a large model of arbitrary architecture, in one-stage end-to-end learning. Specifically, we create a recurrent parameter generator (RPG), which repeatedly fetches parameters from a ring and unpacks them onto a large model with random permutation and sign flipping to promote parameter decorrelation. We show that gradient descent can automatically find the best model under constraints with faster convergence. Our extensive experimentation reveals a log-linear relationship between model DoF and accuracy. Our RPG demonstrates remarkable DoF reduction and can be further pruned and quantized for additional run-time performance gain. For example, in terms of top-1 accuracy on ImageNet, RPG achieves $96\%$ of ResNet18's performance with only $18\%$ DoF (the equivalent of one convolutional layer) and $52\%$ of ResNet34's performance with only $0.25\%$ DoF! Our work shows a significant potential of constrained neural optimization in compact and optimal deep learning.",https://arxiv.org/abs/2107.07110v3,https://arxiv.org/pdf/2107.07110v3
VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning,Adrien Bardes; Jean Ponce; Yann LeCun,2021,,0,,2105.04906,arxiv,2105.04906,cs.CV,cs.CV; cs.AI; cs.LG,"Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.",https://arxiv.org/abs/2105.04906v3,https://arxiv.org/pdf/2105.04906v3
MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding,Aishwarya Kamath; Mannat Singh; Yann LeCun; Gabriel Synnaeve; Ishan Misra; Nicolas Carion,2021,,0,,2104.12763,arxiv,2104.12763,cs.CV,cs.CV; cs.CL; cs.LG,"Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.",https://arxiv.org/abs/2104.12763v2,https://arxiv.org/pdf/2104.12763v2
Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors,Zeyu Yun; Yubei Chen; Bruno A Olshausen; Yann LeCun,2021,,0,,2103.15949,arxiv,2103.15949,cs.CL,cs.CL; cs.LG,"Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these ""black boxes"" as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at https://github.com/zeyuyun1/TransformerVis",https://arxiv.org/abs/2103.15949v2,https://arxiv.org/pdf/2103.15949v2
Barlow Twins: Self-Supervised Learning via Redundancy Reduction,Jure Zbontar; Li Jing; Ishan Misra; Yann LeCun; Stéphane Deny,2021,,0,,2103.03230,arxiv,2103.03230,cs.CV,cs.CV; cs.AI; cs.LG; q-bio.NC,"Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.",https://arxiv.org/abs/2103.03230v3,https://arxiv.org/pdf/2103.03230v3
Implicit Rank-Minimizing Autoencoder,Li Jing; Jure Zbontar; Yann LeCun,2020,,0,,2010.00679,arxiv,2010.00679,cs.LG,cs.LG; cs.CV; stat.ML,"An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate the validity of the method on several image generation and representation learning tasks.",https://arxiv.org/abs/2010.00679v2,https://arxiv.org/pdf/2010.00679v2
Inspirational Adversarial Image Generation,Baptiste Rozière; Morgane Riviere; Olivier Teytaud; Jérémy Rapin; Yann LeCun; Camille Couprie,2019,,0,,1906.11661,arxiv,1906.11661,cs.CV,cs.CV; cs.LG; stat.ML,"The task of image generation started to receive some attention from artists and designers to inspire them in new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control on them. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user choice by performing several optimization steps to recover optimal parameters from the model's latent space. We tested several exploration methods starting with classic gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so that they can even be used without numerical criterion, without inspirational image, but with only with human preference. Thus, by iterating on one's preferences we could make robust Facial Composite or Fashion Generation algorithms. High resolution of the produced design generations are obtained using progressive growing of GANs. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.",https://arxiv.org/abs/1906.11661v2,https://arxiv.org/pdf/1906.11661v2
Unsupervised Image Matching and Object Discovery as Optimization,Huy V. Vo; Francis Bach; Minsu Cho; Kai Han; Yann LeCun; Patrick Perez; Jean Ponce,2019,,0,,1904.03148,arxiv,1904.03148,cs.CV,cs.CV,"Learning with complete or partial supervision is powerful but relies on ever-growing human annotation efforts. As a way to mitigate this serious problem, as well as to serve specific applications, unsupervised learning has emerged as an important field of research. In computer vision, unsupervised learning comes in various guises. We focus here on the unsupervised discovery and matching of object categories among images in a collection, following the work of Cho et al. 2015. We show that the original approach can be reformulated and solved as a proper optimization problem. Experiments on several benchmarks establish the merit of our approach.",https://arxiv.org/abs/1904.03148v1,https://arxiv.org/pdf/1904.03148v1
Learning about an exponential amount of conditional distributions,Mohamed Ishmael Belghazi; Maxime Oquab; Yann LeCun; David Lopez-Paz,2019,,0,,1902.08401,arxiv,1902.08401,cs.LG,cs.LG; stat.ML,"We introduce the Neural Conditioner (NC), a self-supervised machine able to learn about all the conditional distributions of a random vector $X$. The NC is a function $NC(x \cdot a, a, r)$ that leverages adversarial training to match each conditional distribution $P(X_r|X_a=x_a)$. After training, the NC generalizes to sample from conditional distributions never seen, including the joint distribution. The NC is also able to auto-encode examples, providing data representations useful for downstream classification tasks. In sum, the NC integrates different self-supervised tasks (each being the estimation of a conditional distribution) and levels of supervision (partially observed data) seamlessly into a single learning experience.",https://arxiv.org/abs/1902.08401v1,https://arxiv.org/pdf/1902.08401v1
Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic,Mikael Henaff; Alfredo Canziani; Yann LeCun,2019,,0,,1901.02705,arxiv,1901.02705,cs.LG,cs.LG; cs.AI; stat.ML,"Learning a policy using only observational data is challenging because the distribution of states it induces at execution time may differ from the distribution observed during training. We propose to train a policy by unrolling a learned model of the environment dynamics over multiple time steps while explicitly penalizing two costs: the original cost the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on. We measure this second cost by using the uncertainty of the dynamics model about its own predictions, using recent ideas from uncertainty estimation for deep networks. We evaluate our approach using a large-scale observational dataset of driving behavior recorded from traffic cameras, and show that we are able to learn effective driving policies from purely observational data, with no environment interaction.",https://arxiv.org/abs/1901.02705v1,https://arxiv.org/pdf/1901.02705v1
A Spectral Regularizer for Unsupervised Disentanglement,Aditya Ramesh; Youngduck Choi; Yann LeCun,2018,,0,,1812.01161,arxiv,1812.01161,stat.ML,stat.ML; cs.AI; cs.LG,"A generative model with a disentangled representation allows for independent control over different aspects of the output. Learning disentangled representations has been a recent topic of great interest, but it remains poorly understood. We show that even for GANs that do not possess disentangled representations, one can find curved trajectories in latent space over which local disentanglement occurs. These trajectories are found by iteratively following the leading right-singular vectors of the Jacobian of the generator with respect to its input. Based on this insight, we describe an efficient regularizer that aligns these vectors with the coordinate axes, and show that it can be used to induce disentangled representations in GANs, in a completely unsupervised manner.",https://arxiv.org/abs/1812.01161v2,https://arxiv.org/pdf/1812.01161v2
Adversarially-Trained Normalized Noisy-Feature Auto-Encoder for Text Generation,Xiang Zhang; Yann LeCun,2018,,0,,1811.04201,arxiv,1811.04201,cs.CL,cs.CL; cs.LG,"This article proposes Adversarially-Trained Normalized Noisy-Feature Auto-Encoder (ATNNFAE) for byte-level text generation. An ATNNFAE consists of an auto-encoder where the internal code is normalized on the unit sphere and corrupted by additive noise. Simultaneously, a replica of the decoder (sharing the same parameters as the AE decoder) is used as the generator and fed with random latent vectors. An adversarial discriminator is trained to distinguish training samples reconstructed from the AE from samples produced through the random-input generator, making the entire generator-discriminator path differentiable for discrete data like text. The combined effect of noise injection in the code and shared weights between the decoder and the generator can prevent the mode collapsing phenomenon commonly observed in GANs. Since perplexity cannot be applied to non-sequential text generation, we propose a new evaluation method using the total variance distance between frequencies of hash-coded byte-level n-grams (NGTVD). NGTVD is a single benchmark that can characterize both the quality and the diversity of the generated texts. Experiments are offered in 6 large-scale datasets in Arabic, Chinese and English, with comparisons against n-gram baselines and recurrent neural networks (RNNs). Ablation study on both the noise level and the discriminator is performed. We find that RNNs have trouble competing with the n-gram baselines, and the ATNNFAE results are generally competitive.",https://arxiv.org/abs/1811.04201v1,https://arxiv.org/pdf/1811.04201v1
GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations,Zhilin Yang; Jake Zhao; Bhuwan Dhingra; Kaiming He; William W. Cohen; Ruslan Salakhutdinov; Yann LeCun,2018,,0,,1806.05662,arxiv,1806.05662,cs.LG,cs.LG; cs.CL; cs.CV; stat.ML,"Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden unit), or embedding-free units such as image pixels.",https://arxiv.org/abs/1806.05662v3,https://arxiv.org/pdf/1806.05662v3
Backpropagation for Implicit Spectral Densities,Aditya Ramesh; Yann LeCun,2018,,0,,1806.00499,arxiv,1806.00499,cs.LG,cs.LG; cs.AI; stat.ML,"Most successful machine intelligence systems rely on gradient-based learning, which is made possible by backpropagation. Some systems are designed to aid us in interpreting data when explicit goals cannot be provided. These unsupervised systems are commonly trained by backpropagating through a likelihood function. We introduce a tool that allows us to do this even when the likelihood is not explicitly set, by instead using the implicit likelihood of the model. Explicitly defining the likelihood often entails making heavy-handed assumptions that impede our ability to solve challenging tasks. On the other hand, the implicit likelihood of the model is accessible without the need for such assumptions. Our tool, which we call spectral backpropagation, allows us to optimize it in much greater generality than what has been attempted before. GANs can also be viewed as a technique for optimizing implicit likelihoods. We study them using spectral backpropagation in order to demonstrate robustness for high-dimensional problems, and identify two novel properties of the generator G: (1) there exist aberrant, nonsensical outputs to which G assigns very high likelihood, and (2) the eigenvectors of the metric induced by G over latent space correspond to quasi-disentangled explanatory factors.",https://arxiv.org/abs/1806.00499v1,https://arxiv.org/pdf/1806.00499v1
Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks,Behnam Neyshabur; Zhiyuan Li; Srinadh Bhojanapalli; Yann LeCun; Nathan Srebro,2018,,0,,1805.12076,arxiv,1805.12076,cs.LG,cs.LG; stat.ML,"Despite existing work on ensuring generalization of neural networks in terms of scale sensitive complexity measures, such as norms, margin and sharpness, these complexity measures do not offer an explanation of why neural networks generalize better with over-parametrization. In this work we suggest a novel complexity measure based on unit-wise capacities resulting in a tighter generalization bound for two layer ReLU networks. Our capacity bound correlates with the behavior of test error with increasing network sizes, and could potentially explain the improvement in generalization with over-parametrization. We further present a matching lower bound for the Rademacher complexity that improves over previous capacity lower bounds for neural networks.",https://arxiv.org/abs/1805.12076v1,https://arxiv.org/pdf/1805.12076v1
DeSIGN: Design Inspiration from Generative Networks,Othman Sbai; Mohamed Elhoseiny; Antoine Bordes; Yann LeCun; Camille Couprie,2018,,0,,1804.00921,arxiv,1804.00921,cs.LG,cs.LG; stat.ML,"Can an algorithm create original and compelling fashion designs to serve as an inspirational assistant? To help answer this question, we design and investigate different image generation models associated with different loss functions to boost creativity in fashion generation. The dimensions of our explorations include: (i) different Generative Adversarial Networks architectures that start from noise vectors to generate fashion items, (ii) novel loss functions that encourage novelty, inspired from Sharma-Mittal divergence, a generalized mutual information measure for the widely used relative entropies such as Kullback-Leibler, and (iii) a generation process following the key elements of fashion design (disentangling shape and texture components). A key challenge of this study is the evaluation of generated designs and the retrieval of best ones, hence we put together an evaluation protocol associating automatic metrics and human experimental studies that we hope will help ease future research. We show that our proposed creativity criterion yield better overall appreciation than the one employed in Creative Adversarial Networks. In the end, about 61% of our images are thought to be created by human designers rather than by a computer while also being considered original per our human subject experiments, and our proposed loss scores the highest compared to existing losses in both novelty and likability.",https://arxiv.org/abs/1804.00921v2,https://arxiv.org/pdf/1804.00921v2
Predicting Future Instance Segmentation by Forecasting Convolutional Features,Pauline Luc; Camille Couprie; Yann LeCun; Jakob Verbeek,2018,,0,,1803.11496,arxiv,1803.11496,cs.CV,cs.CV,"Anticipating future events is an important prerequisite towards intelligent behavior. Video forecasting has been studied as a proxy task towards this goal. Recent work has shown that to predict semantic segmentation of future frames, forecasting at the semantic level is more effective than forecasting RGB frames and then segmenting these. In this paper we consider the more challenging problem of future instance segmentation, which additionally segments out individual objects. To deal with a varying number of output labels per image, we develop a predictive model in the space of fixed-sized convolutional features of the Mask R-CNN instance segmentation model. We apply the ""detection head'"" of Mask R-CNN on the predicted features to produce the instance segmentation of future frames. Experiments show that this approach significantly improves over strong baselines based on optical flow and repurposed instance segmentation architectures.",https://arxiv.org/abs/1803.11496v2,https://arxiv.org/pdf/1803.11496v2
Byte-Level Recursive Convolutional Auto-Encoder for Text,Xiang Zhang; Yann LeCun,2018,,0,,1802.01817,arxiv,1802.01817,cs.CL,cs.CL,"This article proposes to auto-encode text at byte-level using convolutional networks with a recursive architecture. The motivation is to explore whether it is possible to have scalable and homogeneous text generation at byte-level in a non-sequential fashion through the simple task of auto-encoding. We show that non-sequential text generation from a fixed-length representation is not only possible, but also achieved much better auto-encoding results than recurrent networks. The proposed model is a multi-stage deep convolutional encoder-decoder framework using residual connections, containing up to 160 parameterized layers. Each encoder or decoder contains a shared group of modules that consists of either pooling or upsampling layers, making the network recursive in terms of abstraction levels in representation. Results for 6 large-scale paragraph datasets are reported, in 3 languages including Arabic, Chinese and English. Analyses are conducted to study several properties of the proposed model.",https://arxiv.org/abs/1802.01817v1,https://arxiv.org/pdf/1802.01817v1
A Closer Look at Spatiotemporal Convolutions for Action Recognition,Du Tran; Heng Wang; Lorenzo Torresani; Jamie Ray; Yann LeCun; Manohar Paluri,2017,,0,,1711.11248,arxiv,1711.11248,cs.CV,cs.CV,"In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly advantages in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block ""R(2+1)D"" which gives rise to CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101 and HMDB51.",https://arxiv.org/abs/1711.11248v3,https://arxiv.org/pdf/1711.11248v3
Prediction Under Uncertainty with Error-Encoding Networks,Mikael Henaff; Junbo Zhao; Yann LeCun,2017,,0,,1711.04994,arxiv,1711.04994,cs.AI,cs.AI,"In this work we introduce a new framework for performing temporal predictions in the presence of uncertainty. It is based on a simple idea of disentangling components of the future state which are predictable from those which are inherently unpredictable, and encoding the unpredictable components into a low-dimensional latent variable which is fed into a forward model. Our method uses a supervised training objective which is fast and easy to train. We evaluate it in the context of video prediction on multiple datasets and show that it is able to consistently generate diverse predictions without the need for alternating minimization over a latent space or adversarial training.",https://arxiv.org/abs/1711.04994v3,https://arxiv.org/pdf/1711.04994v3
A hierarchical loss and its problems when classifying non-hierarchically,Cinna Wu; Mark Tygert; Yann LeCun,2017,,0,,1709.01062,arxiv,1709.01062,cs.LG,cs.LG; cs.CV; stat.ML,"Failing to distinguish between a sheepdog and a skyscraper should be worse and penalized more than failing to distinguish between a sheepdog and a poodle; after all, sheepdogs and poodles are both breeds of dogs. However, existing metrics of failure (so-called ""loss"" or ""win"") used in textual or visual classification/recognition via neural networks seldom leverage a-priori information, such as a sheepdog being more similar to a poodle than to a skyscraper. We define a metric that, inter alia, can penalize failure to distinguish between a sheepdog and a skyscraper more than failure to distinguish between a sheepdog and a poodle. Unlike previously employed possibilities, this metric is based on an ultrametric tree associated with any given tree organization into a semantically meaningful hierarchy of a classifier's classes. An ultrametric tree is a tree with a so-called ultrametric distance metric such that all leaves are at the same distance from the root. Unfortunately, extensive numerical experiments indicate that the standard practice of training neural networks via stochastic gradient descent with random starting points often drives down the hierarchical loss nearly as much when minimizing the standard cross-entropy loss as when trying to minimize the hierarchical loss directly. Thus, this hierarchical loss is unreliable as an objective for plain, randomly started stochastic gradient descent to minimize; the main value of the hierarchical loss may be merely as a meaningful metric of success of a classifier.",https://arxiv.org/abs/1709.01062v2,https://arxiv.org/pdf/1709.01062v2
"Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?",Xiang Zhang; Yann LeCun,2017,,0,,1708.02657,arxiv,1708.02657,cs.CL,cs.CL; cs.LG,"This article offers an empirical study on the different ways of encoding Chinese, Japanese, Korean (CJK) and English languages for text classification. Different encoding levels are studied, including UTF-8 bytes, characters, words, romanized characters and romanized words. For all encoding levels, whenever applicable, we provide comparisons with linear models, fastText and convolutional networks. For convolutional networks, we compare between encoding mechanisms using character glyph images, one-hot (or one-of-n) encoding, and embedding. In total there are 473 models, using 14 large-scale text classification datasets in 4 languages including Chinese, English, Japanese and Korean. Some conclusions from these results include that byte-level one-hot encoding based on UTF-8 consistently produces competitive results for convolutional networks, that word-level n-grams linear models are competitive even without perfect word segmentation, and that fastText provides the best result using character-level n-gram encoding but can overfit when the features are overly rich.",https://arxiv.org/abs/1708.02657v2,https://arxiv.org/pdf/1708.02657v2
Adversarially Regularized Autoencoders,Jake Zhao; Yoon Kim; Kelly Zhang; Alexander M. Rush; Yann LeCun,2017,,0,,1706.04223,arxiv,1706.04223,cs.LG,cs.LG; cs.CL; cs.NE,"Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently-proposed Wasserstein autoencoder (WAE) which formalizes the adversarial autoencoder (AAE) as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. This adversarially regularized autoencoder (ARAE) allows us to generate natural textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic/human evaluation compared to existing methods.",https://arxiv.org/abs/1706.04223v3,https://arxiv.org/pdf/1706.04223v3
Model-Based Planning with Discrete and Continuous Actions,Mikael Henaff; William F. Whitney; Yann LeCun,2017,,0,,1705.07177,arxiv,1705.07177,cs.AI,cs.AI,"Action planning using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over model-free RL methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete. In this work, we show that it is in fact possible to effectively perform planning via backprop in discrete action spaces, using a simple paramaterization of the actions vectors on the simplex combined with input noise when training the forward model. Our experiments show that this approach can match or outperform model-free RL and discrete planning methods on gridworld navigation tasks in terms of performance and/or planning time while using limited environment interactions, and can additionally be used to perform model-based control in a challenging new task where the action space combines discrete and continuous actions. We furthermore propose a policy distillation approach which yields a fast policy network which can be used at inference time, removing the need for an iterative planning procedure.",https://arxiv.org/abs/1705.07177v2,https://arxiv.org/pdf/1705.07177v2
Predicting Deeper into the Future of Semantic Segmentation,Pauline Luc; Natalia Neverova; Camille Couprie; Jakob Verbeek; Yann LeCun,2017,,0,,1703.07684,arxiv,1703.07684,cs.CV,cs.CV; cs.LG,"The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.",https://arxiv.org/abs/1703.07684v3,https://arxiv.org/pdf/1703.07684v3
Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs,Li Jing; Yichen Shen; Tena Dubček; John Peurifoy; Scott Skirlo; Yann LeCun; Max Tegmark; Marin Soljačić,2016,,0,,1612.05231,arxiv,1612.05231,cs.LG,cs.LG; cs.NE; stat.ML,"Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely $\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.",https://arxiv.org/abs/1612.05231v3,https://arxiv.org/pdf/1612.05231v3
Tracking the World State with Recurrent Entity Networks,Mikael Henaff; Jason Weston; Arthur Szlam; Antoine Bordes; Yann LeCun,2016,,0,,1612.03969,arxiv,1612.03969,cs.CL,cs.CL,"We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.",https://arxiv.org/abs/1612.03969v3,https://arxiv.org/pdf/1612.03969v3
Geometric deep learning: going beyond Euclidean data,Michael M. Bronstein; Joan Bruna; Yann LeCun; Arthur Szlam; Pierre Vandergheynst,2016,,0,10.1109/MSP.2017.2693418,1611.08097,arxiv,1611.08097,cs.CV,cs.CV,"Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.",https://arxiv.org/abs/1611.08097v2,https://arxiv.org/pdf/1611.08097v2
Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond,Levent Sagun; Leon Bottou; Yann LeCun,2016,,0,,1611.07476,arxiv,1611.07476,cs.LG,cs.LG,"We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges that depend on the input data.",https://arxiv.org/abs/1611.07476v2,https://arxiv.org/pdf/1611.07476v2
Disentangling factors of variation in deep representations using adversarial training,Michael Mathieu; Junbo Zhao; Pablo Sprechmann; Aditya Ramesh; Yann LeCun,2016,,0,,1611.03383,arxiv,1611.03383,cs.LG,cs.LG; stat.ML,"We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentanglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities.",https://arxiv.org/abs/1611.03383v1,https://arxiv.org/pdf/1611.03383v1
Entropy-SGD: Biasing Gradient Descent Into Wide Valleys,Pratik Chaudhari; Anna Choromanska; Stefano Soatto; Yann LeCun; Carlo Baldassi; Christian Borgs; Jennifer Chayes; Levent Sagun; Riccardo Zecchina,2016,,0,,1611.01838,arxiv,1611.01838,cs.LG,cs.LG; stat.ML,"This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.",https://arxiv.org/abs/1611.01838v5,https://arxiv.org/pdf/1611.01838v5
Energy-based Generative Adversarial Network,Junbo Zhao; Michael Mathieu; Yann LeCun,2016,,0,,1609.03126,arxiv,1609.03126,cs.LG,cs.LG; stat.ML,"We introduce the ""Energy-based Generative Adversarial Network"" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",https://arxiv.org/abs/1609.03126v4,https://arxiv.org/pdf/1609.03126v4
Fast Incremental Learning for Off-Road Robot Navigation,Artem Provodin; Liila Torabi; Beat Flepp; Yann LeCun; Michael Sergio; L. D. Jackel; Urs Muller; Jure Zbontar,2016,,0,,1606.08057,arxiv,1606.08057,cs.RO,cs.RO,"A promising approach to autonomous driving is machine learning. In such systems, training datasets are created that capture the sensory input to a vehicle as well as the desired response. A disadvantage of using a learned navigation system is that the learning process itself may require a huge number of training examples and a large amount of computing. To avoid the need to collect a large training set of driving examples, we describe a system that takes advantage of the huge number of training examples provided by ImageNet, but is able to adapt quickly using a small training set for the specific driving environment.",https://arxiv.org/abs/1606.08057v1,https://arxiv.org/pdf/1606.08057v1
Very Deep Convolutional Networks for Text Classification,Alexis Conneau; Holger Schwenk; Loïc Barrault; Yann Lecun,2016,,0,,1606.01781,arxiv,1606.01781,cs.CL,cs.CL; cs.LG; cs.NE,"The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.",https://arxiv.org/abs/1606.01781v2,https://arxiv.org/pdf/1606.01781v2
What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures?,Kevin Jarrett; Koray Kvukcuoglu; Karol Gregor; Yann LeCun,2016,,0,,1606.01535,arxiv,1606.01535,cs.CV,cs.CV,"(This paper was written in November 2011 and never published. It is posted on arXiv.org in its original form in June 2016). Many recent object recognition systems have proposed using a two phase training procedure to learn sparse convolutional feature hierarchies: unsupervised pre-training followed by supervised fine-tuning. Recent results suggest that these methods provide little improvement over purely supervised systems when the appropriate nonlinearities are included. This paper presents an empirical exploration of the space of learning procedures for sparse convolutional networks to assess which method produces the best performance. In our study, we introduce an augmentation of the Predictive Sparse Decomposition method that includes a discriminative term (DPSD). We also introduce a new single phase supervised learning procedure that places an L1 penalty on the output state of each layer of the network. This forces the network to produce sparse codes without the expensive pre-training phase. Using DPSD with a new, complex predictor that incorporates lateral inhibition, combined with multi-scale feature pooling, and supervised refinement, the system achieves a 70.6\% recognition rate on Caltech-101. With the addition of convolutional training, a 77\% recognition was obtained on the CIfAR-10 dataset.",https://arxiv.org/abs/1606.01535v1,https://arxiv.org/pdf/1606.01535v1
Phase 3: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Bioacoustic Applicaitons,Peter J. Dugan; Christopher W. Clark; Yann André LeCun; Sofie M. Van Parijs,2016,,0,,1605.00983,arxiv,1605.00983,cs.DC,cs.DC,"Goals of this research phase is to investigate advanced detection and classification pardims useful for data-mining passive large passive acoustic archives. Technical objectives are to develop and refine a High Performance Computing, Acoustic Data Accelerator (HPC-ADA) along with MATLAB based software based on time series acoustic signal Detection cLassification using Machine learning Algorithms, called DeLMA. Data scientists and biologists integrate to use the HPC-ADA and DeLMA technologies to explore data using newly developed techniques aimed at inspection of data extracted at large spatial and temporal scales.",https://arxiv.org/abs/1605.00983v2,https://arxiv.org/pdf/1605.00983v2
Phase 4: DCL System Using Deep Learning Approaches for Land-Based or Ship-Based Real-Time Recognition and Localization of Marine Mammals - Distributed Processing and Big Data Applications,Peter J. Dugan; Christopher W. Clark; Yann André LeCun; Sofie M. Van Parijs,2016,,0,,1605.00982,arxiv,1605.00982,cs.DC,cs.DC,"While the animal bioacoustics community at large is collecting huge amounts of acoustic data at an unprecedented pace, processing these data is problematic. Currently in bioacoustics, there is no effective way to achieve high performance computing using commericial off the shelf (COTS) or government off the shelf (GOTS) tools. Although several advances have been made in the open source and commercial software community, these offerings either support specific applications that do not integrate well with data formats in bioacoustics or they are too general. Furthermore, complex algorithms that use deep learning strategies require special considerations, such as very large libraiers of exemplars (whale sounds) readily available for algorithm training and testing. Detection-classification for passive acoustics is a data-mining strategy and our goals are aligned with best practices that appeal to the general data mining and machine learning communities where the problem of processing large data is common. Therefore, the objective of this work is to advance the state-of-the art for data-mining large passive acoustic datasets as they pertain to bioacoustics. With this basic deficiency recognized at the forefront, portions of the grant were dedicated to fostering deep-learning by way of international competitions (kaggle.com) meant to attract deep-learning solutions. The focus of this early work was targeted to make significant progress in addressing big data systems and advanced algorithms over the duration of the grant from 2012 to 2015. This early work provided simulataneous advances in systems-algorithms research while supporting various collaborations and projects.",https://arxiv.org/abs/1605.00982v2,https://arxiv.org/pdf/1605.00982v2
Phase 2: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Machine Learning Detection Algorithms,Peter J. Dugan; Christopher W. Clark; Yann André LeCun; Sofie M. Van Parijs,2016,,0,,1605.00972,arxiv,1605.00972,cs.CV,cs.CV,"Overarching goals for this work aim to advance the state of the art for detection, classification and localization (DCL) in the field of bioacoustics. This goal is primarily achieved by building a generic framework for detection-classification (DC) using a fast, efficient and scalable architecture, demonstrating the capabilities of this system using on a variety of low-frequency mid-frequency cetacean sounds. Two primary goals are to develop transferable technologies for detection and classification in, one: the area of advanced algorithms, such as deep learning and other methods; and two: advanced systems, capable of real-time and archival processing. For each key area, we will focus on producing publications from this work and providing tools and software to the community where/when possible. Currently massive amounts of acoustic data are being collected by various institutions, corporations and national defense agencies. The long-term goal is to provide technical capability to analyze the data using automatic algorithms for (DC) based on machine intelligence. The goal of the automation is to provide effective and efficient mechanisms by which to process large acoustic datasets for understanding the bioacoustic behaviors of marine mammals. This capability will provide insights into the potential ecological impacts and influences of anthropogenic ocean sounds. This work focuses on building technologies using a maturity model based on DARPA 6.1 and 6.2 processes, for basic and applied research, respectively.",https://arxiv.org/abs/1605.00972v2,https://arxiv.org/pdf/1605.00972v2
Phase 1: DCL System Research Using Advanced Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - HPC System Implementation,Peter J. Dugan; Christopher W. Clark; Yann André LeCun; Sofie M. Van Parijs,2016,,0,,1605.00971,arxiv,1605.00971,cs.DC,cs.DC,"We aim to investigate advancing the state of the art of detection, classification and localization (DCL) in the field of bioacoustics. The two primary goals are to develop transferable technologies for detection and classification in: (1) the area of advanced algorithms, such as deep learning and other methods; and (2) advanced systems, capable of real-time and archival and processing. This project will focus on long-term, continuous datasets to provide automatic recognition, minimizing human time to annotate the signals. Effort will begin by focusing on several years of multi-channel acoustic data collected in the Stellwagen Bank National Marine Sanctuary (SBNMS) between 2006 and 2010. Our efforts will incorporate existing technologies in the bioacoustics signal processing community, advanced high performance computing (HPC) systems, and new approaches aimed at automatically detecting-classifying and measuring features for species-specific marine mammal sounds within passive acoustic data.",https://arxiv.org/abs/1605.00971v2,https://arxiv.org/pdf/1605.00971v2
Recurrent Orthogonal Networks and Long-Memory Tasks,Mikael Henaff; Arthur Szlam; Yann LeCun,2016,,0,,1602.06662,arxiv,1602.06662,cs.NE,cs.NE; cs.AI; cs.LG; stat.ML,"Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.",https://arxiv.org/abs/1602.06662v2,https://arxiv.org/pdf/1602.06662v2
Universal halting times in optimization and machine learning,Levent Sagun; Thomas Trogdon; Yann LeCun,2015,,0,10.1090/qam/1483,1511.06444,arxiv,1511.06444,cs.LG,cs.LG; math.NA; math.PR,"The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that, after centering and scaling, remains unchanged even when the distribution on the landscape is changed. We observe two qualitative classes: A Gumbel-like distribution that appears in Google searches, human decision times, the QR eigenvalue algorithm and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.",https://arxiv.org/abs/1511.06444v3,https://arxiv.org/pdf/1511.06444v3
Super-Resolution with Deep Convolutional Sufficient Statistics,Joan Bruna; Pablo Sprechmann; Yann LeCun,2015,,0,,1511.05666,arxiv,1511.05666,cs.CV,cs.CV,"Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep convolutional neural networks. The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed approach in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension.",https://arxiv.org/abs/1511.05666v4,https://arxiv.org/pdf/1511.05666v4
Deep multi-scale video prediction beyond mean square error,Michael Mathieu; Camille Couprie; Yann LeCun,2015,,0,,1511.05440,arxiv,1511.05440,cs.LG,cs.LG; cs.CV; stat.ML,"Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset",https://arxiv.org/abs/1511.05440v6,https://arxiv.org/pdf/1511.05440v6
Binary embeddings with structured hashed projections,Anna Choromanska; Krzysztof Choromanski; Mariusz Bojarski; Tony Jebara; Sanjiv Kumar; Yann LeCun,2015,,0,,1511.05212,arxiv,1511.05212,cs.LG,cs.LG,"We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudo-random projection is described by a matrix, where not all entries are independent random variables but instead a fixed ""budget of randomness"" is distributed across the matrix. Such matrices can be efficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. In particular, they generalize previous extensions of the Johnson-Lindenstrauss lemma and prove the plausibility of the approach that was so far only heuristically confirmed for some special structured matrices. Consequently, we show that many structured matrices can be used as an efficient information compression mechanism. Our findings build a better understanding of certain deep architectures, which contain randomly weighted and untrained layers, and yet achieve high performance on different learning tasks. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier.",https://arxiv.org/abs/1511.05212v5,https://arxiv.org/pdf/1511.05212v5
Universum Prescription: Regularization using Unlabeled Data,Xiang Zhang; Yann LeCun,2015,,0,,1511.03719,arxiv,1511.03719,cs.LG,cs.LG,"This paper shows that simply prescribing ""none of the above"" labels to unlabeled data has a beneficial regularization effect to supervised learning. We call it universum prescription by the fact that the prescribed labels cannot be one of the supervised labels. In spite of its simplicity, universum prescription obtained competitive results in training deep convolutional networks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitative justification of these approaches using Rademacher complexity is presented. The effect of a regularization parameter -- probability of sampling from unlabeled data -- is also studied empirically.",https://arxiv.org/abs/1511.03719v7,https://arxiv.org/pdf/1511.03719v7
Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches,Jure Žbontar; Yann LeCun,2015,,0,,1510.05970,arxiv,1510.05970,cs.CV,cs.CV; cs.LG; cs.NE,"We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.",https://arxiv.org/abs/1510.05970v2,https://arxiv.org/pdf/1510.05970v2
Very Deep Multilingual Convolutional Neural Networks for LVCSR,Tom Sercu; Christian Puhrsch; Brian Kingsbury; Yann LeCun,2015,,0,,1509.08967,arxiv,1509.08967,cs.CL,cs.CL; cs.NE,"Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6% relative) over the best published CNN result so far.",https://arxiv.org/abs/1509.08967v2,https://arxiv.org/pdf/1509.08967v2
High Performance Computer Acoustic Data Accelerator: A New System for Exploring Marine Mammal Acoustics for Big Data Applications,Peter Dugan; John Zollweg; Marian Popescu; Denise Risch; Herve Glotin; Yann LeCun; and Christopher Clark,2015,,0,,1509.03591,arxiv,1509.03591,cs.DC,cs.DC,"This paper presents a new software model designed for distributed sonic signal detection runtime using machine learning algorithms called DeLMA. A new algorithm--Acoustic Data-mining Accelerator (ADA)--is also presented. ADA is a robust yet scalable solution for efficiently processing big sound archives using distributing computing technologies. Together, DeLMA and the ADA algorithm provide a powerful tool currently being used by the Bioacoustics Research Program (BRP) at the Cornell Lab of Ornithology, Cornell University. This paper provides a high level technical overview of the system, and discusses various aspects of the design. Basic runtime performance and project summary are presented. The DeLMA-ADA baseline performance comparing desktop serial configuration to a 64 core distributed HPC system shows as much as a 44 times faster increase in runtime execution. Performance tests using 48 cores on the HPC shows a 9x to 12x efficiency over a 4 core desktop solution. Project summary results for 19 east coast deployments show that the DeLMA-ADA solution has processed over three million channel hours of sound to date.",https://arxiv.org/abs/1509.03591v1,https://arxiv.org/pdf/1509.03591v1
Character-level Convolutional Networks for Text Classification,Xiang Zhang; Junbo Zhao; Yann LeCun,2015,,0,,1509.01626,arxiv,1509.01626,cs.LG,cs.LG; cs.CL,"This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",https://arxiv.org/abs/1509.01626v3,https://arxiv.org/pdf/1509.01626v3
Deep Convolutional Networks on Graph-Structured Data,Mikael Henaff; Joan Bruna; Yann LeCun,2015,,0,,1506.05163,arxiv,1506.05163,cs.LG,cs.LG; cs.CV; cs.NE,"Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities.   In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.",https://arxiv.org/abs/1506.05163v1,https://arxiv.org/pdf/1506.05163v1
Learning to Linearize Under Uncertainty,Ross Goroshin; Michael Mathieu; Yann LeCun,2015,,0,,1506.03011,arxiv,1506.03011,cs.CV,cs.CV,"Training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture.",https://arxiv.org/abs/1506.03011v2,https://arxiv.org/pdf/1506.03011v2
Stacked What-Where Auto-encoders,Junbo Zhao; Michael Mathieu; Ross Goroshin; Yann LeCun,2015,,0,,1506.02351,arxiv,1506.02351,stat.ML,stat.ML; cs.LG; cs.NE,"We present a novel architecture, the ""stacked what-where auto-encoders"" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the ""what"" which are fed to the next layer, and its complementary variable ""where"" that are fed to the corresponding layer in the generative decoder.",https://arxiv.org/abs/1506.02351v8,https://arxiv.org/pdf/1506.02351v8
Unsupervised Feature Learning from Temporal Data,Ross Goroshin; Joan Bruna; Jonathan Tompson; David Eigen; Yann LeCun,2015,,0,,1504.02518,arxiv,1504.02518,cs.CV,cs.CV; cs.LG,"Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.",https://arxiv.org/abs/1504.02518v2,https://arxiv.org/pdf/1504.02518v2
A mathematical motivation for complex-valued convolutional networks,Joan Bruna; Soumith Chintala; Yann LeCun; Serkan Piantino; Arthur Szlam; Mark Tygert,2015,,0,,1503.03438,arxiv,1503.03438,cs.LG,cs.LG; cs.NE; stat.ML,"A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors followed by (2) taking the absolute value of every entry of the resulting vectors followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as ""data-driven multiscale windowed power spectra,"" ""data-driven multiscale windowed absolute spectra,"" ""data-driven multiwavelet absolute values,"" or (in their most general configuration) ""data-driven nonlinear multiwavelet packets."" Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max. pooling, etc., do not obviously exhibit the same exact correspondence with data-driven wavelets (whereas for complex-valued convnets, the correspondence is much more than just a vague analogy). Courtesy of the exact correspondence, the remarkably rich and rigorous body of mathematical analysis for wavelets applies directly to (complex-valued) convnets.",https://arxiv.org/abs/1503.03438v3,https://arxiv.org/pdf/1503.03438v3
Text Understanding from Scratch,Xiang Zhang; Yann LeCun,2015,,0,,1502.01710,arxiv,1502.01710,cs.LG,cs.LG; cs.CL,"This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.",https://arxiv.org/abs/1502.01710v5,https://arxiv.org/pdf/1502.01710v5
Fast Convolutional Nets With fbfft: A GPU Performance Evaluation,Nicolas Vasilache; Jeff Johnson; Michael Mathieu; Soumith Chintala; Serkan Piantino; Yann LeCun,2014,,0,,1412.7580,arxiv,1412.7580,cs.LG,cs.LG; cs.DC; cs.NE,"We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided.",https://arxiv.org/abs/1412.7580v3,https://arxiv.org/pdf/1412.7580v3
Audio Source Separation with Discriminative Scattering Networks,Pablo Sprechmann; Joan Bruna; Yann LeCun,2014,,0,,1412.7022,arxiv,1412.7022,cs.SD,cs.SD; cs.LG,"In this report we describe an ongoing line of research for solving single-channel source separation problems. Many monaural signal decomposition techniques proposed in the literature operate on a feature space consisting of a time-frequency representation of the input data. A challenge faced by these approaches is to effectively exploit the temporal dependencies of the signals at scales larger than the duration of a time-frame. In this work we propose to tackle this problem by modeling the signals using a time-frequency representation with multiple temporal resolutions. The proposed representation consists of a pyramid of wavelet scattering operators, which generalizes Constant Q Transforms (CQT) with extra layers of convolution and complex modulus. We first show that learning standard models with this multi-resolution setting improves source separation results over fixed-resolution methods. As study case, we use Non-Negative Matrix Factorizations (NMF) that has been widely considered in many audio application. Then, we investigate the inclusion of the proposed multi-resolution setting into a discriminative training regime. We discuss several alternatives using different deep neural network architectures.",https://arxiv.org/abs/1412.7022v3,https://arxiv.org/pdf/1412.7022v3
Deep learning with Elastic Averaging SGD,Sixin Zhang; Anna Choromanska; Yann LeCun,2014,,0,,1412.6651,arxiv,1412.6651,cs.LG,cs.LG; stat.ML,"We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.",https://arxiv.org/abs/1412.6651v8,https://arxiv.org/pdf/1412.6651v8
Explorations on high dimensional landscapes,Levent Sagun; V. Ugur Guney; Gerard Ben Arous; Yann LeCun,2014,,0,,1412.6615,arxiv,1412.6615,stat.ML,stat.ML; cs.LG,Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. We provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.,https://arxiv.org/abs/1412.6615v4,https://arxiv.org/pdf/1412.6615v4
Unsupervised Learning of Spatiotemporally Coherent Metrics,Ross Goroshin; Joan Bruna; Jonathan Tompson; David Eigen; Yann LeCun,2014,,0,,1412.6056,arxiv,1412.6056,cs.CV,cs.CV,"Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.",https://arxiv.org/abs/1412.6056v6,https://arxiv.org/pdf/1412.6056v6
The Loss Surfaces of Multilayer Networks,Anna Choromanska; Mikael Henaff; Michael Mathieu; Gérard Ben Arous; Yann LeCun,2014,,0,,1412.0233,arxiv,1412.0233,cs.LG,cs.LG,"We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.",https://arxiv.org/abs/1412.0233v3,https://arxiv.org/pdf/1412.0233v3
Efficient Object Localization Using Convolutional Networks,Jonathan Tompson; Ross Goroshin; Arjun Jain; Yann LeCun; Christopher Bregler,2014,,0,,1411.4280,arxiv,1411.4280,cs.CV,cs.CV,"Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.",https://arxiv.org/abs/1411.4280v3,https://arxiv.org/pdf/1411.4280v3
Differentially- and non-differentially-private random decision trees,Mariusz Bojarski; Anna Choromanska; Krzysztof Choromanski; Yann LeCun,2014,,0,,1410.6973,arxiv,1410.6973,cs.LG,cs.LG,"We consider supervised learning with random decision trees, where the tree construction is completely random. The method is popularly used and works well in practice despite the simplicity of the setting, but its statistical mechanism is not yet well-understood. In this paper we provide strong theoretical guarantees regarding learning with random decision trees. We analyze and compare three different variants of the algorithm that have minimal memory requirements: majority voting, threshold averaging and probabilistic averaging. The random structure of the tree enables us to adapt these methods to a differentially-private setting thus we also propose differentially-private versions of all three schemes. We give upper-bounds on the generalization error and mathematically explain how the accuracy depends on the number of random decision trees. Furthermore, we prove that only logarithmic (in the size of the dataset) number of independently selected random decision trees suffice to correctly classify most of the data, even when differential-privacy guarantees must be maintained. We empirically show that majority voting and threshold averaging give the best accuracy, also for conservative users requiring high privacy guarantees. Furthermore, we demonstrate that a simple majority voting rule is an especially good candidate for the differentially-private classifier since it is much less sensitive to the choice of forest parameters than other methods.",https://arxiv.org/abs/1410.6973v2,https://arxiv.org/pdf/1410.6973v2
MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation,Arjun Jain; Jonathan Tompson; Yann LeCun; Christoph Bregler,2014,,0,,1409.7963,arxiv,1409.7963,cs.CV,cs.CV; cs.LG; cs.NE,"In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.",https://arxiv.org/abs/1409.7963v1,https://arxiv.org/pdf/1409.7963v1
Computing the Stereo Matching Cost with a Convolutional Neural Network,Jure Žbontar; Yann LeCun,2014,,0,10.1109/CVPR.2015.7298767,1409.4326,arxiv,1409.4326,cs.CV,cs.CV; cs.LG; cs.NE,"We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset.",https://arxiv.org/abs/1409.4326v2,https://arxiv.org/pdf/1409.4326v2
Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation,Jonathan Tompson; Arjun Jain; Yann LeCun; Christoph Bregler,2014,,0,,1406.2984,arxiv,1406.2984,cs.CV,cs.CV,This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.,https://arxiv.org/abs/1406.2984v2,https://arxiv.org/pdf/1406.2984v2
Fast Approximation of Rotations and Hessians matrices,Michael Mathieu; Yann LeCun,2014,,0,,1404.7195,arxiv,1404.7195,cs.LG,cs.LG,"A new method to represent and approximate rotation matrices is introduced. The method represents approximations of a rotation matrix $Q$ with linearithmic complexity, i.e. with $\frac{1}{2}n\lg(n)$ rotations over pairs of coordinates, arranged in an FFT-like fashion. The approximation is ""learned"" using gradient descent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ is a diagonal matrix. It can be used to approximate covariance matrix of Gaussian models in order to speed up inference, or to estimate and track the inverse Hessian of an objective function by relating changes in parameters to changes in gradient along the trajectory followed by the optimization procedure. Experiments were conducted to approximate synthetic matrices, covariance matrices of real data, and Hessian matrices of objective functions involved in machine learning problems.",https://arxiv.org/abs/1404.7195v1,https://arxiv.org/pdf/1404.7195v1
Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation,Remi Denton; Wojciech Zaremba; Joan Bruna; Yann LeCun; Rob Fergus,2014,,0,,1404.0736,arxiv,1404.0736,cs.CV,cs.CV; cs.LG,"We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1% of the original model.",https://arxiv.org/abs/1404.0736v2,https://arxiv.org/pdf/1404.0736v2
"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",Pierre Sermanet; David Eigen; Xiang Zhang; Michael Mathieu; Rob Fergus; Yann LeCun,2013,,0,,1312.6229,arxiv,1312.6229,cs.CV,cs.CV,"We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",https://arxiv.org/abs/1312.6229v4,https://arxiv.org/pdf/1312.6229v4
Spectral Networks and Locally Connected Networks on Graphs,Joan Bruna; Wojciech Zaremba; Arthur Szlam; Yann LeCun,2013,,0,,1312.6203,arxiv,1312.6203,cs.LG,cs.LG; cs.CV; cs.NE,"Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",https://arxiv.org/abs/1312.6203v3,https://arxiv.org/pdf/1312.6203v3
Fast Training of Convolutional Networks through FFTs,Michael Mathieu; Mikael Henaff; Yann LeCun,2013,,0,,1312.5851,arxiv,1312.5851,cs.CV,cs.CV; cs.LG; cs.NE,"Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.",https://arxiv.org/abs/1312.5851v5,https://arxiv.org/pdf/1312.5851v5
Understanding Deep Architectures using a Recursive Convolutional Network,David Eigen; Jason Rolfe; Rob Fergus; Yann LeCun,2013,,0,,1312.1847,arxiv,1312.1847,cs.LG,cs.LG,"A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We find that while increasing the numbers of layers and parameters each have clear benefit, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and finds most of its benefit through the introduction of more weights. Our results (i) empirically confirm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.",https://arxiv.org/abs/1312.1847v2,https://arxiv.org/pdf/1312.1847v2
Signal Recovery from Pooling Representations,Joan Bruna; Arthur Szlam; Yann LeCun,2013,,0,,1311.4025,arxiv,1311.4025,stat.ML,stat.ML,"In this work we compute lower Lipschitz bounds of $\ell_p$ pooling operators for $p=1, 2, \infty$ as well as $\ell_p$ pooling operators preceded by half-rectification layers. These give sufficient conditions for the design of invertible neural network layers. Numerical experiments on MNIST and image patches confirm that pooling layers can be inverted with phase recovery algorithms. Moreover, the regularity of the inverse pooling, controlled by the lower Lipschitz constant, is empirically verified with a nearest neighbor regression.",https://arxiv.org/abs/1311.4025v3,https://arxiv.org/pdf/1311.4025v3
Discriminative Recurrent Sparse Auto-Encoders,Jason Tyler Rolfe; Yann LeCun,2013,,0,,1301.3775,arxiv,1301.3775,cs.LG,cs.LG; cs.CV,"We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters.   From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.",https://arxiv.org/abs/1301.3775v4,https://arxiv.org/pdf/1301.3775v4
"Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients",Tom Schaul; Yann LeCun,2013,,0,,1301.3764,arxiv,1301.3764,cs.LG,cs.LG; cs.AI; stat.ML,"Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.",https://arxiv.org/abs/1301.3764v2,https://arxiv.org/pdf/1301.3764v2
Saturating Auto-Encoders,Rostislav Goroshin; Yann LeCun,2013,,0,,1301.3577,arxiv,1301.3577,cs.LG,cs.LG,"We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.",https://arxiv.org/abs/1301.3577v3,https://arxiv.org/pdf/1301.3577v3
Indoor Semantic Segmentation using depth information,Camille Couprie; Clément Farabet; Laurent Najman; Yann LeCun,2013,,0,,1301.3572,arxiv,1301.3572,cs.CV,cs.CV,"This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.",https://arxiv.org/abs/1301.3572v2,https://arxiv.org/pdf/1301.3572v2
Learning Stable Group Invariant Representations with Convolutional Networks,Joan Bruna; Arthur Szlam; Yann LeCun,2013,,0,,1301.3537,arxiv,1301.3537,cs.AI,cs.AI; math.NA,"Transformation groups, such as translations or rotations, effectively express part of the variability observed in many recognition problems. The group structure enables the construction of invariant signal representations with appealing mathematical properties, where convolutions, together with pooling operators, bring stability to additive and geometric perturbations of the input. Whereas physical transformation groups are ubiquitous in image and audio applications, they do not account for all the variability of complex signal classes.   We show that the invariance properties built by deep convolutional networks can be cast as a form of stable group invariance. The network wiring architecture determines the invariance group, while the trainable filter coefficients characterize the group action. We give explanatory examples which illustrate how the network architecture controls the resulting invariance group. We also explore the principle by which additional convolutional layers induce a group factorization enabling more abstract, powerful invariant representations.",https://arxiv.org/abs/1301.3537v1,https://arxiv.org/pdf/1301.3537v1
Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities,Tommi Vatanen; Tapani Raiko; Harri Valpola; Yann LeCun,2013,,0,,1301.3476,arxiv,1301.3476,cs.LG,cs.LG; cs.CV; stat.ML,"Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.",https://arxiv.org/abs/1301.3476v3,https://arxiv.org/pdf/1301.3476v3
Causal graph-based video segmentation,Camille Couprie; Clément Farabet; Yann LeCun,2013,,0,,1301.1671,arxiv,1301.1671,cs.CV,cs.CV,"Numerous approaches in image processing and computer vision are making use of super-pixels as a pre-processing step. Among the different methods producing such over-segmentation of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time. The algorithm may be trivially extended to video segmentation by considering a video as a 3D volume, however, this can not be the case for causal segmentation, when subsequent frames are unknown. We propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real time applications.",https://arxiv.org/abs/1301.1671v1,https://arxiv.org/pdf/1301.1671v1
Pedestrian Detection with Unsupervised Multi-Stage Feature Learning,Pierre Sermanet; Koray Kavukcuoglu; Soumith Chintala; Yann LeCun,2012,,0,,1212.0142,arxiv,1212.0142,cs.CV,cs.CV; cs.LG,"Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.",https://arxiv.org/abs/1212.0142v2,https://arxiv.org/pdf/1212.0142v2
No More Pesky Learning Rates,Tom Schaul; Sixin Zhang; Yann LeCun,2012,,0,,1206.1106,arxiv,1206.1106,stat.ML,stat.ML; cs.LG,"The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.",https://arxiv.org/abs/1206.1106v2,https://arxiv.org/pdf/1206.1106v2
Convolutional Neural Networks Applied to House Numbers Digit Classification,Pierre Sermanet; Soumith Chintala; Yann LeCun,2012,,0,,1204.3968,arxiv,1204.3968,cs.CV,cs.CV; cs.LG; cs.NE,"We classify digits of real-world house numbers using convolutional neural networks (ConvNets). ConvNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 94.85% accuracy on the SVHN dataset (45.2% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.",https://arxiv.org/abs/1204.3968v1,https://arxiv.org/pdf/1204.3968v1
Fast approximations to structured sparse coding and applications to object classification,Arthur Szlam; Karol Gregor; Yann LeCun,2012,,0,,1202.6384,arxiv,1202.6384,cs.CV,cs.CV,"We describe a method for fast approximation of sparse coding. The input space is subdivided by a binary decision tree, and we simultaneously learn a dictionary and assignment of allowed dictionary elements for each leaf of the tree. We store a lookup table with the assignments and the pseudoinverses for each node, allowing for very fast inference. We give an algorithm for learning the tree, the dictionary and the dictionary element assignment, and In the process of describing this algorithm, we discuss the more general problem of learning the groups in group structured sparse modelling. We show that our method creates good sparse representations by using it in the object recognition framework of \cite{lazebnik06,yang-cvpr-09}. Implementing our own fast version of the SIFT descriptor the whole system runs at 20 frames per second on $321 \times 481$ sized images on a laptop with a quad-core cpu, while sacrificing very little accuracy on the Caltech 101 and 15 scenes benchmarks.",https://arxiv.org/abs/1202.6384v1,https://arxiv.org/pdf/1202.6384v1
"Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers",Clément Farabet; Camille Couprie; Laurent Najman; Yann LeCun,2012,,0,,1202.2160,arxiv,1202.2160,cs.CV,cs.CV; cs.LG,"Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image.   The scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average ""purity"" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free.   The system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 \times 240 image labeling in less than 1 second.",https://arxiv.org/abs/1202.2160v2,https://arxiv.org/pdf/1202.2160v2
Learning Representations by Maximizing Compression,Karol Gregor; Yann LeCun,2011,,0,,1108.1169,arxiv,1108.1169,cs.CV,cs.CV,"We give an algorithm that learns a representation of data through compression. The algorithm 1) predicts bits sequentially from those previously seen and 2) has a structure and a number of computations similar to an autoencoder. The likelihood under the model can be calculated exactly, and arithmetic coding can be used directly for compression. When training on digits the algorithm learns filters similar to those of restricted boltzman machines and denoising autoencoders. Independent samples can be drawn from the model by a single sweep through the pixels. The algorithm has a good compression performance when compared to other methods that work under random ordering of pixels.",https://arxiv.org/abs/1108.1169v1,https://arxiv.org/pdf/1108.1169v1
Efficient Learning of Sparse Invariant Representations,Karol Gregor; Yann LeCun,2011,,0,,1105.5307,arxiv,1105.5307,cs.CV,cs.CV; cs.NE,"We propose a simple and efficient algorithm for learning sparse invariant representations from unlabeled data with fast inference. When trained on short movies sequences, the learned features are selective to a range of orientations and spatial frequencies, but robust to a wide range of positions, similar to complex cells in the primary visual cortex. We give a hierarchical version of the algorithm, and give guarantees of fast convergence under certain conditions.",https://arxiv.org/abs/1105.5307v1,https://arxiv.org/pdf/1105.5307v1
Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition,Koray Kavukcuoglu; Marc'Aurelio Ranzato; Yann LeCun,2010,,0,,1010.3467,arxiv,1010.3467,cs.CV,cs.CV; cs.LG,"Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.",https://arxiv.org/abs/1010.3467v1,https://arxiv.org/pdf/1010.3467v1
Convolutional Matching Pursuit and Dictionary Training,Arthur Szlam; Koray Kavukcuoglu; Yann LeCun,2010,,0,,1010.0422,arxiv,1010.0422,cs.CV,cs.CV,Matching pursuit and K-SVD is demonstrated in the translation invariant setting,https://arxiv.org/abs/1010.0422v1,https://arxiv.org/pdf/1010.0422v1
Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields,Karo Gregor; Yann LeCun,2010,,0,,1006.0448,arxiv,1006.0448,cs.NE,cs.NE,"We introduce a new neural architecture and an unsupervised algorithm for learning invariant representations from temporal sequence of images. The system uses two groups of complex cells whose outputs are combined multiplicatively: one that represents the content of the image, constrained to be constant over several consecutive frames, and one that represents the precise location of features, which is allowed to vary over time but constrained to be sparse. The architecture uses an encoder to extract features, and a decoder to reconstruct the input from the features. The method was applied to patches extracted from consecutive movie frames and produces orientation and frequency selective units analogous to the complex cells in V1. An extension of the method is proposed to train a network composed of units with local receptive field spread over a large image of arbitrary size. A layer of complex cells, subject to sparsity constraints, pool feature units over overlapping local neighborhoods, which causes the feature units to organize themselves into pinwheel patterns of orientation-selective receptive fields, similar to those observed in the mammalian visual cortex. A feed-forward encoder efficiently computes the feature representation of full images.",https://arxiv.org/abs/1006.0448v1,https://arxiv.org/pdf/1006.0448v1
