{
  "0": {
    "input": {
      "id": 0,
      "name": "Yann LeCun",
      "scholar_id": "WLN3QrAAAAAJ",
      "semantic_scholar_id": null,
      "arxiv_name": "Yann LeCun"
    },
    "errors": {},
    "parsed_at": "2025-11-28T20:47:13.343041",
    "semantic_scholar": {
      "name": "Yann LeCun",
      "source": "semantic_scholar",
      "source_id": "1688882",
      "affiliation": "Facebook",
      "orcid": null,
      "homepage": null,
      "interests": [],
      "metrics": {
        "citations": 259193,
        "citations_recent": 0,
        "h_index": 137,
        "h_index_recent": 0,
        "i10_index": 0,
        "publication_count": 405
      },
      "citations_per_year": {},
      "publications_per_year": {
        "2024": 2,
        "2023": 22,
        "2022": 32,
        "2021": 15,
        "2020": 3,
        "2019": 8,
        "2018": 13,
        "2017": 15,
        "2016": 19,
        "2015": 22,
        "2014": 18,
        "2013": 24,
        "2012": 22,
        "2011": 12,
        "2010": 20,
        "2009": 15,
        "2008": 11,
        "2007": 17,
        "2006": 10,
        "2005": 6,
        "2004": 2,
        "2003": 1,
        "2002": 2,
        "2001": 3,
        "2000": 2,
        "1999": 4,
        "1998": 12,
        "1997": 5,
        "1996": 2,
        "1995": 4,
        "1994": 10,
        "1993": 8,
        "1992": 10,
        "1991": 7,
        "1990": 8,
        "1989": 7,
        "1988": 4,
        "1987": 3,
        "1986": 1,
        "1985": 1,
        "1977": 1
      },
      "first_publication_year": 1977,
      "last_publication_year": 2024,
      "years_active": 47,
      "publications": [
        {
          "title": "FAST INCREMENTAL LEARNING FOR AUTONOMOUS GROUND NAVIGATION",
          "year": 2024,
          "citations": 0,
          "abstract": "\n ABSTRACT \n A promising approach to autonomous driving is machine learning. In machine \n learning systems, training datasets are created that capture the sensory input \n to a vehicle as well as the desired response. One disadvantage of using a \n learned navigation system is that the learning process itself may require both a \n huge number of training examples and a large amount of computing. To avoid the \n need to collect a large training set of driving examples, we describe a system \n that takes advantage of the immense number of training examples provided by \n ImageNet, but at the same time is able to adapt quickly using a small training \n set for the driving environment. \n",
          "venue": "SAE technical paper series",
          "doi": "10.4271/2024-01-3556",
          "url": "https://www.semanticscholar.org/paper/4c4d882c3e54612946d289fd159da8de98fb3200",
          "authors": [
            "Artem Provodin",
            "L. Torabi",
            "Urs Muller",
            "B. Flepp",
            "Michael Sergio",
            "Jure Zbontar",
            "Yann LeCun",
            "L. Jackel"
          ]
        },
        {
          "title": "Learning and Leveraging World Models in Visual Representation Learning",
          "year": 2024,
          "citations": 48,
          "abstract": "Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2403.00504",
          "url": "https://www.semanticscholar.org/paper/4d1eadaa9a04e86aef2278ae13eb9fce644c9fc5",
          "authors": [
            "Q. Garrido",
            "Mahmoud Assran",
            "Nicolas Ballas",
            "Adrien Bardes",
            "Laurent Najman",
            "Yann LeCun"
          ]
        },
        {
          "title": "The SSL Interplay: Augmentations, Inductive Bias, and Generalization",
          "year": 2023,
          "citations": 41,
          "abstract": "Self-supervised learning (SSL) has emerged as a powerful framework to learn representations from raw data without supervision. Yet in practice, engineers face issues such as instability in tuning optimizers and collapse of representations during training. Such challenges motivate the need for a theory to shed light on the complex interplay between the choice of data augmentation, network architecture, and training algorithm. We study such an interplay with a precise analysis of generalization performance on both pretraining and downstream tasks in a theory friendly setup, and highlight several insights for SSL practitioners that arise from our theory.",
          "venue": "International Conference on Machine Learning",
          "doi": "10.48550/arXiv.2302.02774",
          "url": "https://www.semanticscholar.org/paper/034081ba8bd12b9466414fce3e885451a92b075a",
          "authors": [
            "Vivien A. Cabannes",
            "B. Kiani",
            "Randall Balestriero",
            "Yann LeCun",
            "A. Bietti"
          ]
        },
        {
          "title": "Minimalistic Unsupervised Representation Learning with the Sparse Manifold Transform",
          "year": 2023,
          "citations": 5,
          "abstract": null,
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0380271e23c5a3348f9f8ad1906b692b22f8b75e",
          "authors": [
            "Yubei Chen",
            "Zeyu Yun",
            "Yi Ma",
            "B. Olshausen",
            "Yann LeCun"
          ]
        },
        {
          "title": "Self-supervised learning of Split Invariant Equivariant representations",
          "year": 2023,
          "citations": 40,
          "abstract": "Recent progress has been made towards learning invariant or equivariant representations with self-supervised learning. While invariant methods are evaluated on large scale datasets, equivariant ones are evaluated in smaller, more controlled, settings. We aim at bridging the gap between the two in order to learn more diverse representations that are suitable for a wide range of tasks. We start by introducing a dataset called 3DIEBench, consisting of renderings from 3D models over 55 classes and more than 2.5 million images where we have full control on the transformations applied to the objects. We further introduce a predictor architecture based on hypernetworks to learn equivariant representations with no possible collapse to invariance. We introduce SIE (Split Invariant-Equivariant) which combines the hypernetwork-based predictor with representations split in two parts, one invariant, the other equivariant, to learn richer representations. We demonstrate significant performance gains over existing methods on equivariance related tasks from both a qualitative and quantitative point of view. We further analyze our introduced predictor and show how it steers the learned latent space. We hope that both our introduced dataset and approach will enable learning richer representations without supervision in more complex scenarios. Code and data are available at https://github.com/facebookresearch/SIE.",
          "venue": "International Conference on Machine Learning",
          "doi": "10.48550/arXiv.2302.10283",
          "url": "https://www.semanticscholar.org/paper/10923e416d15ab36161f4ab9ad40aa15bb91f541",
          "authors": [
            "Q. Garrido",
            "Laurent Najman",
            "Yann LeCun"
          ]
        },
        {
          "title": "Fast and Exact Enumeration of Deep Networks Partitions Regions",
          "year": 2023,
          "citations": 5,
          "abstract": "One fruitful formulation of Deep Networks (DNs) enabling their theoretical study and providing practical guidelines to practitioners relies on Piecewise Affine Splines. In that realm, a DN’s input-mapping is expressed as per-region affine map-ping where those regions are implicitly determined by the model’s architecture and form a partition of their input space. That partition –which is involved in all the results spanned from this line of research– has so far only been computed on 2/3-dimensional slices of the DN’s input space or estimated by random sampling. In this paper, we provide the first parallel algorithm that does exact enumeration of the DN’s partition regions. The proposed algorithm enables one to finally assess the closeness of the commonly employed approximations methods, e.g. based on random sampling of the DN input space. One of our key finding is that if one is only interested in regions with \"large\" volume, then uniform sampling of the space is highly efficient, but that if one is also interested in discovering the \"small\" regions of the partition, then uniform sampling is exponentially costly with the DN’s input space dimension. On the other hand, our proposed method has complexity scaling linearly with input dimension and the number of regions.",
          "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
          "doi": "10.1109/icassp49357.2023.10095698",
          "url": "https://www.semanticscholar.org/paper/132606b0fc37bd2a9f50d3199f29b8b2defbcb93",
          "authors": [
            "Randall Balestriero",
            "Yann LeCun"
          ]
        },
        {
          "title": "Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need",
          "year": 2023,
          "citations": 14,
          "abstract": "Self-Supervised Learning (SSL) has emerged as the solution of choice to learn transferable representations from unlabeled data. However, SSL requires to build samples that are known to be semantically akin, i.e. positive views. Requiring such knowledge is the main limitation of SSL and is often tackled by ad-hoc strategies e.g. applying known data-augmentations to the same input. In this work, we formalize and generalize this principle through Positive Active Learning (PAL) where an oracle queries semantic relationships between samples. PAL achieves three main objectives. First, it unveils a theoretically grounded learning framework beyond SSL, based on similarity graphs, that can be extended to tackle supervised and semi-supervised learning depending on the employed oracle. Second, it provides a consistent algorithm to embed a priori knowledge, e.g. some observed labels, into any SSL losses without any change in the training pipeline. Third, it provides a proper active learning framework yielding low-cost solutions to annotate datasets, arguably bringing the gap between theory and practice of active learning that is based on simple-to-answer-by-non-experts queries of semantic relationships between inputs.",
          "venue": "IEEE International Conference on Computer Vision",
          "doi": "10.1109/ICCV51070.2023.01491",
          "url": "https://www.semanticscholar.org/paper/138fa5f64fe54376022998fe553b6156a93ff19e",
          "authors": [
            "Vivien A. Cabannes",
            "L. Bottou",
            "Yann LeCun",
            "Randall Balestriero"
          ]
        },
        {
          "title": "An Information-Theoretic Perspective on Variance-Invariance-Covariance Regularization",
          "year": 2023,
          "citations": 26,
          "abstract": "Variance-Invariance-Covariance Regularization (VICReg) is a self-supervised learning (SSL) method that has shown promising results on a variety of tasks. However, the fundamental mechanisms underlying VICReg remain unexplored. In this paper, we present an information-theoretic perspective on the VICReg objective. We begin by deriving information-theoretic quantities for deterministic networks as an alternative to unrealistic stochastic network assumptions. We then relate the optimization of the VICReg objective to mutual information optimization, highlighting underlying assumptions and facilitating a constructive comparison with other SSL algorithms and derive a generalization bound for VICReg, revealing its inherent advantages for downstream tasks. Building on these results, we introduce a family of SSL methods derived from information-theoretic principles that outperform existing SSL techniques.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2303.00633",
          "url": "https://www.semanticscholar.org/paper/1ea4f4dcedbbe6d10aad30c3cb02fe2b0572b090",
          "authors": [
            "Ravid Shwartz-Ziv",
            "Randall Balestriero",
            "Kenji Kawaguchi",
            "Tim G. J. Rudner",
            "Yann LeCun"
          ]
        },
        {
          "title": "Augmented Language Models: a Survey",
          "year": 2023,
          "citations": 467,
          "abstract": "This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.",
          "venue": "Trans. Mach. Learn. Res.",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2029349c55c1dba3493c5b3bd25152f18ba21ae2",
          "authors": [
            "G. Mialon",
            "Roberto Dessì",
            "M. Lomeli",
            "Christoforos Nalmpantis",
            "Ramakanth Pasunuru",
            "R. Raileanu",
            "Baptiste Rozière",
            "Timo Schick",
            "Jane Dwivedi-Yu",
            "Asli Celikyilmaz",
            "Edouard Grave",
            "Yann LeCun",
            "Thomas Scialom"
          ]
        },
        {
          "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
          "year": 2023,
          "citations": 120,
          "abstract": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of explanations as features: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an LLM-to-LM interpreter to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data. Our codes and datasets are available at: https://github.com/XiaoxinHe/TAPE.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2d2b05f0969568ac3fd3c2cca5df04c4136c5416",
          "authors": [
            "Xiaoxin He",
            "X. Bresson",
            "T. Laurent",
            "Adam Perold",
            "Yann LeCun",
            "Bryan Hooi"
          ]
        },
        {
          "title": "MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features",
          "year": 2023,
          "citations": 36,
          "abstract": "Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2307.12698",
          "url": "https://www.semanticscholar.org/paper/3c1e43b7d3f5fd42a06c65e3aafe6d8f4a606d5c",
          "authors": [
            "Adrien Bardes",
            "J. Ponce",
            "Yann LeCun"
          ]
        },
        {
          "title": "EMP-SSL: Towards Self-Supervised Learning in One Training Epoch",
          "year": 2023,
          "citations": 29,
          "abstract": "Recently, self-supervised learning (SSL) has achieved tremendous success in learning image representation. Despite the empirical success, most self-supervised learning methods are rather\"inefficient\"learners, typically taking hundreds of training epochs to fully converge. In this work, we show that the key towards efficient self-supervised learning is to increase the number of crops from each image instance. Leveraging one of the state-of-the-art SSL method, we introduce a simplistic form of self-supervised learning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL) that does not rely on many heuristic techniques for SSL such as weight sharing between the branches, feature-wise normalization, output quantization, and stop gradient, etc, and reduces the training epochs by two orders of magnitude. We show that the proposed method is able to converge to 85.1% on CIFAR-10, 58.5% on CIFAR-100, 38.1% on Tiny ImageNet and 58.5% on ImageNet-100 in just one epoch. Furthermore, the proposed method achieves 91.5% on CIFAR-10, 70.1% on CIFAR-100, 51.5% on Tiny ImageNet and 78.9% on ImageNet-100 with linear probing in less than ten training epochs. In addition, we show that EMP-SSL shows significantly better transferability to out-of-domain datasets compared to baseline SSL methods. We will release the code in https://github.com/tsb0601/EMP-SSL.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2304.03977",
          "url": "https://www.semanticscholar.org/paper/5367ca1c122a0806549e484fb488a977b4334777",
          "authors": [
            "Shengbang Tong",
            "Yubei Chen",
            "Y. Ma",
            "Yann LeCun"
          ]
        },
        {
          "title": "Self-Supervised Learning with Lie Symmetries for Partial Differential Equations",
          "year": 2023,
          "citations": 28,
          "abstract": "Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation models for PDEs. Code: https://github.com/facebookresearch/SSLForPDEs.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2307.05432",
          "url": "https://www.semanticscholar.org/paper/5dc01119b2e4911b0c1ae61233e36efe78588aed",
          "authors": [
            "G. Mialon",
            "Q. Garrido",
            "Hannah Lawrence",
            "Danyal Rehman",
            "Yann LeCun",
            "B. Kiani"
          ]
        },
        {
          "title": "Adapting Grounded Visual Question Answering Models to Low Resource Languages",
          "year": 2023,
          "citations": 4,
          "abstract": "While huge progress has been made on a variety of vision and language tasks in recent years, most major advances have been restricted to the English language due to the scarcity of relevant training and evaluation datasets in other languages. A popular approach to address this gap, has been to utilize machine-translated multi-modal datasets or multi-lingual text-only datasets for pre-training. This approach not only fails to exploit existing pre-trained state-of-the-art English multi-modal models, but also is not a viable solution for low-resource languages where translation quality is not as reliable. Therefore, we propose xMDETR, a multi-lingual grounded vision-language model based on the state-of-the-art model MDETR, by adapting it to new languages without machine-translated data, while also keeping most of the pre-trained weights frozen. xMDETR leverages mono-lingual pre-trained MDETR to achieve results competitive to state of the art on xGQA, a standard multilingual VQA benchmark. It is also interpretable, providing bounding boxes for key phrases in the multi-lingual questions. Our method utilizes several architectural as well as data-driven techniques such as training a new embedding space with a Masked Language Modeling (MLM) objective, code-switching, and adapters for efficient and modular training. We also explore contrastive losses to enforce the bridging of multi-modal and multi-lingual representations on multi-lingual multi-modal data, when available. We evaluate xMDETR on xGQA in both zero-shot and few-shot settings, improving results on Portuguese, Indonesian and Bengali, while remaining competitive on other languages.",
          "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
          "doi": "10.1109/CVPRW59228.2023.00258",
          "url": "https://www.semanticscholar.org/paper/6be0cf37d8b05c1b8fb4235217895b3bf1b6c368",
          "authors": [
            "Y. Wang",
            "Jonas Pfeiffer",
            "Nicolas Carion",
            "Yann LeCun",
            "Aishwarya Kamath"
          ]
        },
        {
          "title": "A Cookbook of Self-Supervised Learning",
          "year": 2023,
          "citations": 349,
          "abstract": "Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2304.12210",
          "url": "https://www.semanticscholar.org/paper/6bfafb32b423c3f0456a10984814f89046def489",
          "authors": [
            "Randall Balestriero",
            "Mark Ibrahim",
            "Vlad Sobal",
            "Ari S. Morcos",
            "Shashank Shekhar",
            "T. Goldstein",
            "Florian Bordes",
            "Adrien Bardes",
            "G. Mialon",
            "Yuandong Tian",
            "Avi Schwarzschild",
            "A. Wilson",
            "Jonas Geiping",
            "Q. Garrido",
            "Pierre Fernandez",
            "Amir Bar",
            "H. Pirsiavash",
            "Yann LeCun",
            "Micah Goldblum"
          ]
        },
        {
          "title": "URLOST: Unsupervised Representation Learning without Stationarity or Topology",
          "year": 2023,
          "citations": 1,
          "abstract": "Unsupervised representation learning has seen tremendous progress. However, it is constrained by its reliance on domain specific stationarity and topology, a limitation not found in biological intelligence systems. For instance, unlike computer vision, human vision can process visual signals sampled from highly irregular and non-stationary sensors. We introduce a novel framework that learns from high-dimensional data without prior knowledge of stationarity and topology. Our model, abbreviated as URLOST, combines a learnable self-organizing layer, spectral clustering, and a masked autoencoder (MAE). We evaluate its effectiveness on three diverse data modalities including simulated biological vision data, neural recordings from the primary visual cortex, and gene expressions. Compared to state-of-the-art unsupervised learning methods like SimCLR and MAE, our model excels at learning meaningful representations across diverse modalities without knowing their stationarity or topology. It also outperforms other methods that are not dependent on these factors, setting a new benchmark in the field. We position this work as a step toward unsupervised learning methods capable of generalizing across diverse high-dimensional data modalities.",
          "venue": "International Conference on Learning Representations",
          "doi": "10.48550/arXiv.2310.04496",
          "url": "https://www.semanticscholar.org/paper/782d400ba7aac6ccb2d4b6d3cadbf4b7c2600d50",
          "authors": [
            "Zeyu Yun",
            "Juexiao Zhang",
            "B. Olshausen",
            "Yann LeCun",
            "Yubei Chen"
          ]
        },
        {
          "title": "Introduction to latent variable energy-based models: a path toward autonomous machine intelligence",
          "year": 2023,
          "citations": 41,
          "abstract": "Current automated systems have crucial limitations that need to be addressed before artificial intelligence can reach human-like levels and bring new technological revolutions. Among others, our societies still lack level-5 self-driving cars, domestic robots, and virtual assistants that learn reliable world models, reason, and plan complex action sequences. In these notes, we summarize the main ideas behind the architecture of autonomous intelligence of the future proposed by Yann LeCun. In particular, we introduce energy-based and latent variable models and combine their advantages in the building block of LeCun’s proposal, that is, in the hierarchical joint-embedding predictive architecture.",
          "venue": "Journal of Statistical Mechanics: Theory and Experiment",
          "doi": "10.1088/1742-5468/ad292b",
          "url": "https://www.semanticscholar.org/paper/9502c180be0ebc92fcf2085ea90c3cb45280a6bc",
          "authors": [
            "Anna Dawid",
            "Yann LeCun"
          ]
        },
        {
          "title": "To Compress or Not to Compress—Self-Supervised Learning and Information Theory: A Review",
          "year": 2023,
          "citations": 95,
          "abstract": "Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory has shaped deep neural networks, particularly the information bottleneck principle. This principle optimizes the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the self-supervised information-theoretic learning problem. This framework includes multiple encoders and decoders, suggesting that all existing work on self-supervised learning can be seen as specific instances. We aim to unify these approaches to understand their underlying principles better and address the main challenge: many works present different frameworks with differing theories that may seem contradictory. By weaving existing research into a cohesive narrative, we delve into contemporary self-supervised methodologies, spotlight potential research areas, and highlight inherent challenges. Moreover, we discuss how to estimate information-theoretic quantities and their associated empirical problems. Overall, this paper provides a comprehensive review of the intersection of information theory, self-supervised learning, and deep neural networks, aiming for a better understanding through our proposed unified approach.",
          "venue": "Entropy",
          "doi": "10.3390/e26030252",
          "url": "https://www.semanticscholar.org/paper/97b1f4980fc173e59ff3a3bdaf1b9a13965fb32e",
          "authors": [
            "Ravid Shwartz-Ziv",
            "Yann LeCun"
          ]
        },
        {
          "title": "Blockwise Self-Supervised Learning at Scale",
          "year": 2023,
          "citations": 21,
          "abstract": "Current state-of-the-art deep networks are all powered by backpropagation. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.",
          "venue": "Trans. Mach. Learn. Res.",
          "doi": "10.48550/arXiv.2302.01647",
          "url": "https://www.semanticscholar.org/paper/a09a197325be3fb2e865692b164e8827042201d1",
          "authors": [
            "Shoaib Ahmed Siddiqui",
            "David Krueger",
            "Yann LeCun",
            "Stéphane Deny"
          ]
        },
        {
          "title": "Reverse Engineering Self-Supervised Learning",
          "year": 2023,
          "citations": 43,
          "abstract": "Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provide valuable insights into SSL's representation learning mechanisms and their impact on performance across different sets of classes.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2305.15614",
          "url": "https://www.semanticscholar.org/paper/a2b8ff257658b8291deb9e40ec1c164c8fefeb06",
          "authors": [
            "Ido Ben-Shaul",
            "Ravid Shwartz-Ziv",
            "Tomer Galanti",
            "S. Dekel",
            "Yann LeCun"
          ]
        },
        {
          "title": "Variance-Covariance Regularization Improves Representation Learning",
          "year": 2023,
          "citations": 8,
          "abstract": "Transfer learning plays a key role in advancing machine learning models, yet conventional supervised pretraining often undermines feature transferability by prioritizing features that minimize the pretraining loss. In this work, we adapt a self-supervised learning regularization technique from the VICReg method to supervised learning contexts, introducing Variance-Covariance Regularization (VCReg). This adaptation encourages the network to learn high-variance, low-covariance representations, promoting learning more diverse features. We outline best practices for an efficient implementation of our framework, including applying it to the intermediate representations. Through extensive empirical evaluation, we demonstrate that our method significantly enhances transfer learning for images and videos, achieving state-of-the-art performance across numerous tasks and datasets. VCReg also improves performance in scenarios like long-tail learning and hierarchical classification. Additionally, we show its effectiveness may stem from its success in addressing challenges like gradient starvation and neural collapse. In summary, VCReg offers a universally applicable regularization framework that significantly advances transfer learning and highlights the connection between gradient starvation, neural collapse, and feature transferability.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2306.13292",
          "url": "https://www.semanticscholar.org/paper/c6990a1e568f5458240643688ee797b6450c9f1f",
          "authors": [
            "Jiachen Zhu",
            "Ravid Shwartz-Ziv",
            "Yubei Chen",
            "Yann LeCun"
          ]
        },
        {
          "title": "Stochastic positional embeddings improve masked image modeling",
          "year": 2023,
          "citations": 5,
          "abstract": "Masked Image Modeling (MIM) is a promising self-supervised learning approach that enables learning from unlabeled images. Despite its recent success, learning good representations through MIM remains challenging because it requires predicting the right semantic content in accurate locations. For example, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose to incorporate location uncertainty into MIM by using stochastic positional embeddings (StoP). Specifically, we condition the model on stochastic masked token positions drawn from a Gaussian distribution. StoP reduces overfitting to location features and guides the model toward learning features that are more robust to location uncertainties. Quantitatively, StoP improves downstream MIM performance on a variety of downstream tasks, including $+1.7\\%$ on ImageNet linear probing using ViT-B, and $+2.5\\%$ for ViT-H using $1\\%$ of the data.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/da136b4651e035c2bafd0bfd34433faf7af2619e",
          "authors": [
            "Amir Bar",
            "Florian Bordes",
            "Assaf Shocher",
            "Mahmoud Assran",
            "Pascal Vincent",
            "Nicolas Ballas",
            "Trevor Darrell",
            "A. Globerson",
            "Yann LeCun"
          ]
        },
        {
          "title": "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
          "year": 2023,
          "citations": 518,
          "abstract": "This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.",
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR52729.2023.01499",
          "url": "https://www.semanticscholar.org/paper/ee57e4d7a125f4ca8916284a857c3760d7d378d3",
          "authors": [
            "Mahmoud Assran",
            "Quentin Duval",
            "Ishan Misra",
            "Piotr Bojanowski",
            "Pascal Vincent",
            "Michael G. Rabbat",
            "Yann LeCun",
            "Nicolas Ballas"
          ]
        },
        {
          "title": "Language, common sense, and the Winograd schema challenge",
          "year": 2023,
          "citations": 25,
          "abstract": null,
          "venue": "Artificial Intelligence",
          "doi": "10.1016/j.artint.2023.104031",
          "url": "https://www.semanticscholar.org/paper/f875c2de4a3ccee670cc76a81b1dfd111bd40f64",
          "authors": [
            "Jacob Browning",
            "Yann LeCun"
          ]
        },
        {
          "title": "Separating the World and Ego Models for Self-Driving",
          "year": 2022,
          "citations": 5,
          "abstract": "Training self-driving systems to be robust to the long-tail of driving scenarios is a critical problem. Model-based approaches leverage simulation to emulate a wide range of scenarios without putting users at risk in the real world. One promising path to faithful simulation is to train a forward model of the world to predict the future states of both the environment and the ego-vehicle given past states and a sequence of actions. In this paper, we argue that it is beneficial to model the state of the ego-vehicle, which often has simple, predictable and deterministic behavior, separately from the rest of the environment, which is much more complex and highly multimodal. We propose to model the ego-vehicle using a simple and differentiable kinematic model, while training a stochastic convolutional forward model on raster representations of the state to predict the behavior of the rest of the environment. We explore several configurations of such decoupled models, and evaluate their performance both with Model Predictive Control (MPC) and direct policy learning. We test our methods on the task of highway driving and demonstrate lower crash rates and better stability. The code is available at https://github.com/vladisai/pytorch-PPUU/tree/ICLR2022.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2204.07184",
          "url": "https://www.semanticscholar.org/paper/06103ec8b82b705d674df3432bbaa5dcfffcceb0",
          "authors": [
            "Vlad Sobal",
            "A. Canziani",
            "Nicolas Carion",
            "Kyunghyun Cho",
            "Yann LeCun"
          ]
        },
        {
          "title": "Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning",
          "year": 2022,
          "citations": 8,
          "abstract": "Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches primarily learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as BagSSL. Even with 32x32 patch representation, BagSSL achieves 62% top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the SSL representation is relatively invariant at the global scale, we show that locality is preserved when we zoom into local patch-level representation. Further, we show that patch representation aggregation can improve various SOTA baseline methods by a large margin. The patch representation is considerably easier to understand, and this work makes a step to demystify self-supervised representation learning.",
          "venue": "Trans. Mach. Learn. Res.",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0bbb60fd0fe2e0ffefacb16fc2c527cb2f01a71e",
          "authors": [
            "Yubei Chen",
            "Adrien Bardes",
            "Zengyi Li",
            "Yann LeCun"
          ]
        },
        {
          "title": "VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment",
          "year": 2022,
          "citations": 26,
          "abstract": "Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision-Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-modal fusion deep into the uni-modal backbones during pre-training and removes fusion-specific transformer layers, further reducing memory requirements. Extensive experiments on a wide range of vision- and vision-language downstream tasks demonstrate the effectiveness of VoLTA on fine-grained applications without compromising the coarse-grained downstream performance, often outperforming methods using significantly more caption and box annotations.",
          "venue": "Trans. Mach. Learn. Res.",
          "doi": "10.48550/arXiv.2210.04135",
          "url": "https://www.semanticscholar.org/paper/0ee11b28a9ce49d3030cab11f1178fa5abae9c3b",
          "authors": [
            "Shraman Pramanick",
            "Li Jing",
            "Sayan Nag",
            "Jiachen Zhu",
            "Hardik Shah",
            "Yann LeCun",
            "Ramalingam Chellappa"
          ]
        },
        {
          "title": "On the duality between contrastive and non-contrastive self-supervised learning",
          "year": 2022,
          "citations": 106,
          "abstract": "Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning.",
          "venue": "International Conference on Learning Representations",
          "doi": "10.48550/arXiv.2206.02574",
          "url": "https://www.semanticscholar.org/paper/11c16254f7b61687b5d9b7637de032461a6ebb5f",
          "authors": [
            "Q. Garrido",
            "Yubei Chen",
            "Adrien Bardes",
            "Laurent Najman",
            "Yann LeCun"
          ]
        },
        {
          "title": "RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank",
          "year": 2022,
          "citations": 108,
          "abstract": "Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations but only few principled guidelines that would help practitioners to successfully deploy them. The main reason for that pitfall comes from JE-SSL's core principle of not employing any input reconstruction therefore lacking visual cues of unsuccessful training. Adding non informative loss values to that, it becomes difficult to deploy SSL on a new dataset for which no labels can help to judge the quality of the learned representation. In this study, we develop a simple unsupervised criterion that is indicative of the quality of the learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method -- coined RankMe -- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels. A further benefit of RankMe is that it does not have any training or hyper-parameters to tune. Through thorough empirical experiments involving hundreds of training episodes, we demonstrate how RankMe can be used for hyperparameter selection with nearly no reduction in final performance compared to the current selection method that involve a dataset's labels. We hope that RankMe will facilitate the deployment of JE-SSL towards domains that do not have the opportunity to rely on labels for representations' quality assessment.",
          "venue": "International Conference on Machine Learning",
          "doi": "10.48550/arXiv.2210.02885",
          "url": "https://www.semanticscholar.org/paper/127ebdb7b87fe5c8c8ff1bb9173584b75eec8f47",
          "authors": [
            "Q. Garrido",
            "Randall Balestriero",
            "Laurent Najman",
            "Yann LeCun"
          ]
        },
        {
          "title": "Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors",
          "year": 2022,
          "citations": 46,
          "abstract": "Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2205.10279",
          "url": "https://www.semanticscholar.org/paper/1e9fbd0e9d047c192d7e2a75f0034400c5c403c7",
          "authors": [
            "Ravid Shwartz-Ziv",
            "Micah Goldblum",
            "Hossein Souri",
            "Sanyam Kapoor",
            "Chen Zhu",
            "Yann LeCun",
            "A. Wilson"
          ]
        },
        {
          "title": "Open Research Online Knowledge Graph Construction with a façade: a uniﬁed method to access heterogeneous data sources on the Web",
          "year": 2022,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/346ff941ebbccdbada17a22aa074f609bf7afd28",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Neural Manifold Clustering and Embedding",
          "year": 2022,
          "citations": 48,
          "abstract": "Given a union of non-linear manifolds, non-linear subspace clustering or manifold clustering aims to cluster data points based on manifold structures and also learn to parameterize each manifold as a linear subspace in a feature space. Deep neural networks have the potential to achieve this goal under highly non-linear settings given their large capacity and flexibility. We argue that achieving manifold clustering with neural networks requires two essential ingredients: a domain-specific constraint that ensures the identification of the manifolds, and a learning algorithm for embedding each manifold to a linear subspace in the feature space. This work shows that many constraints can be implemented by data augmentation. For subspace feature learning, Maximum Coding Rate Reduction (MCR$^2$) objective can be used. Putting them together yields {\\em Neural Manifold Clustering and Embedding} (NMCE), a novel method for general purpose manifold clustering, which significantly outperforms autoencoder-based deep subspace clustering. Further, on more challenging natural image datasets, NMCE can also outperform other algorithms specifically designed for clustering. Qualitatively, we demonstrate that NMCE learns a meaningful and interpretable feature space. As the formulation of NMCE is closely related to several important Self-supervised learning (SSL) methods, we believe this work can help us build a deeper understanding on SSL representation learning.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3834edf530c639257555d41a3c67e128a22aefc0",
          "authors": [
            "Zengyi Li",
            "Yubei Chen",
            "Yann LeCun",
            "F. Sommer"
          ]
        },
        {
          "title": "A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments",
          "year": 2022,
          "citations": 20,
          "abstract": "Data-Augmentation (DA) is known to improve performance across tasks and datasets. We propose a method to theoretically analyze the effect of DA and study questions such as: how many augmented samples are needed to correctly estimate the information encoded by that DA? How does the augmentation policy impact the final parameters of a model? We derive several quantities in close-form, such as the expectation and variance of an image, loss, and model's output under a given DA distribution. Those derivations open new avenues to quantify the benefits and limitations of DA. For example, we show that common DAs require tens of thousands of samples for the loss at hand to be correctly estimated and for the model training to converge. We show that for a training loss to be stable under DA sampling, the model's saliency map (gradient of the loss with respect to the model's input) must align with the smallest eigenvector of the sample variance under the considered DA augmentation, hinting at a possible explanation on why models tend to shift their focus from edges to textures.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3f07b6b6d1fc6020cef1f92c053810f46b46ac5e",
          "authors": [
            "Randall Balestriero",
            "Ishan Misra",
            "Yann LeCun"
          ]
        },
        {
          "title": "Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution",
          "year": 2022,
          "citations": 53,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2210.08340",
          "url": "https://www.semanticscholar.org/paper/497e2d07c1dbf84913110c02cd2990eaea57a7cb",
          "authors": [
            "A. Zador",
            "B. Richards",
            "Bence Olveczky",
            "Sean Escola",
            "Y. Bengio",
            "K. Boahen",
            "M. Botvinick",
            "D. Chklovskii",
            "A. Churchland",
            "C. Clopath",
            "J. DiCarlo",
            "S. Ganguli",
            "J. Hawkins",
            "Konrad Koerding",
            "A. Koulakov",
            "Yann LeCun",
            "T. Lillicrap",
            "A. Marblestone",
            "B. Olshausen",
            "A. Pouget",
            "Cristina Savin",
            "T. Sejnowski",
            "Eero P. Simoncelli",
            "S. Solla",
            "David Sussillo",
            "A. Tolias",
            "Doris Y. Tsao"
          ]
        },
        {
          "title": "Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone",
          "year": 2022,
          "citations": 145,
          "abstract": "Vision-language (VL) pre-training has recently received considerable attention. However, most existing end-to-end pre-training approaches either only aim to tackle VL tasks such as image-text retrieval, visual question answering (VQA) and image captioning that test high-level understanding of images, or only target region-level understanding for tasks such as phrase grounding and object detection. We present FIBER (Fusion-In-the-Backbone-based transformER), a new VL model architecture that can seamlessly handle both these types of tasks. Instead of having dedicated transformer layers for fusion after the uni-modal backbones, FIBER pushes multimodal fusion deep into the model by inserting cross-attention into the image and text backbones, bringing gains in terms of memory and performance. In addition, unlike previous work that is either only pre-trained on image-text data or on fine-grained data with box-level annotations, we present a two-stage pre-training strategy that uses both these kinds of data efficiently: (i) coarse-grained pre-training based on image-text data; followed by (ii) fine-grained pre-training based on image-text-box data. We conduct comprehensive experiments on a wide range of VL tasks, ranging from VQA, image captioning, and retrieval, to phrase grounding, referring expression comprehension, and object detection. Using deep multimodal fusion coupled with the two-stage pre-training, FIBER provides consistent performance improvements over strong baselines across all tasks, often outperforming methods using magnitudes more data. Code is available at https://github.com/microsoft/FIBER.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2206.07643",
          "url": "https://www.semanticscholar.org/paper/4c559d29e19f1226353f52ffe9f8068db1cef943",
          "authors": [
            "Zi-Yi Dou",
            "Aishwarya Kamath",
            "Zhe Gan",
            "Pengchuan Zhang",
            "Jianfeng Wang",
            "Linjie Li",
            "Zicheng Liu",
            "Ce Liu",
            "Yann LeCun",
            "Nanyun Peng",
            "Jianfeng Gao",
            "Lijuan Wang"
          ]
        },
        {
          "title": "A Generalization of ViT/MLP-Mixer to Graphs",
          "year": 2022,
          "citations": 117,
          "abstract": "Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning. Standard GNNs define a local message-passing mechanism which propagates information over the whole graph domain by stacking multiple layers. This paradigm suffers from two major limitations, over-squashing and poor long-range dependencies, that can be solved using global attention but significantly increases the computational cost to quadratic complexity. In this work, we propose an alternative approach to overcome these structural limitations by leveraging the ViT/MLP-Mixer architectures introduced in computer vision. We introduce a new class of GNNs, called Graph ViT/MLP-Mixer, that holds three key properties. First, they capture long-range dependency and mitigate the issue of over-squashing as demonstrated on Long Range Graph Benchmark and TreeNeighbourMatch datasets. Second, they offer better speed and memory efficiency with a complexity linear to the number of nodes and edges, surpassing the related Graph Transformer and expressive GNN models. Third, they show high expressivity in terms of graph isomorphism as they can distinguish at least 3-WL non-isomorphic graphs. We test our architecture on 4 simulated datasets and 7 real-world benchmarks, and show highly competitive results on all of them. The source code is available for reproducibility at: \\url{https://github.com/XiaoxinHe/Graph-ViT-MLPMixer}.",
          "venue": "International Conference on Machine Learning",
          "doi": "10.48550/arXiv.2212.13350",
          "url": "https://www.semanticscholar.org/paper/517802b9381246dff16756fe5299fa62bb29e228",
          "authors": [
            "Xiaoxin He",
            "Bryan Hooi",
            "T. Laurent",
            "Adam Perold",
            "Yann LeCun",
            "X. Bresson"
          ]
        },
        {
          "title": "Minimalistic Unsupervised Learning with the Sparse Manifold Transform",
          "year": 2022,
          "citations": 8,
          "abstract": "We describe a minimalistic and interpretable method for unsupervised learning, without resorting to data augmentation, hyperparameter tuning, or other engineering designs, that achieves performance close to the SOTA SSL methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic sparse manifold transform, one can achieve 99.3% KNN top-1 accuracy on MNIST, 81.1% KNN top-1 accuracy on CIFAR-10 and 53.2% on CIFAR-100. With a simple gray-scale augmentation, the model gets 83.2% KNN top-1 accuracy on CIFAR-10 and 57% on CIFAR-100. These results significantly close the gap between simplistic\"white-box\"methods and the SOTA methods. Additionally, we provide visualization to explain how an unsupervised representation transform is formed. The proposed method is closely connected to latent-embedding self-supervised methods and can be treated as the simplest form of VICReg. Though there remains a small performance gap between our simple constructive model and SOTA methods, the evidence points to this as a promising direction for achieving a principled and white-box approach to unsupervised learning.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2209.15261",
          "url": "https://www.semanticscholar.org/paper/537166437212aac4b4297121e0ba4b7e545d4e54",
          "authors": [
            "Yubei Chen",
            "Zeyu Yun",
            "Y. Ma",
            "B. Olshausen",
            "Yann LeCun"
          ]
        },
        {
          "title": "Catalyzing next-generation Artificial Intelligence through NeuroAI",
          "year": 2022,
          "citations": 239,
          "abstract": "One of the ambitions of computational neuroscience is that we will continue to make improvements in the field of artificial intelligence that will be informed by advances in our understanding of how the brains of various species evolved to process information. To that end, here the authors propose an expanded version of the Turing test that involves embodied sensorimotor interactions with the world as a new framework for accelerating progress in artificial intelligence. Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities – inherited from over 500 million years of evolution – that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI.",
          "venue": "Nature Communications",
          "doi": "10.1038/s41467-023-37180-x",
          "url": "https://www.semanticscholar.org/paper/5cba4b2a4d0b74c8aad0c94b6f468f6c86ee3db9",
          "authors": [
            "A. Zador",
            "Sean Escola",
            "B. Richards",
            "B. Ölveczky",
            "Y. Bengio",
            "K. Boahen",
            "M. Botvinick",
            "D. Chklovskii",
            "A. Churchland",
            "C. Clopath",
            "J. DiCarlo",
            "Surya",
            "Ganguli",
            "J. Hawkins",
            "Konrad Paul Kording",
            "A. Koulakov",
            "Yann LeCun",
            "T. Lillicrap",
            "Adam",
            "Marblestone",
            "B. Olshausen",
            "A. Pouget",
            "Cristina Savin",
            "T. Sejnowski",
            "Eero P. Simoncelli",
            "S. Solla",
            "David Sussillo",
            "A. Tolias",
            "D. Tsao"
          ]
        },
        {
          "title": "Unsupervised Learning of Structured Representations via Closed-Loop Transcription",
          "year": 2022,
          "citations": 9,
          "abstract": "This paper proposes an unsupervised method for learning a unified representation that serves both discriminative and generative purposes. While most existing unsupervised learning approaches focus on a representation for only one of these two goals, we show that a unified representation can enjoy the mutual benefits of having both. Such a representation is attainable by generalizing the recently proposed \\textit{closed-loop transcription} framework, known as CTRL, to the unsupervised setting. This entails solving a constrained maximin game over a rate reduction objective that expands features of all samples while compressing features of augmentations of each sample. Through this process, we see discriminative low-dimensional structures emerge in the resulting representations. Under comparable experimental conditions and network complexities, we demonstrate that these structured representations enable classification performance close to state-of-the-art unsupervised discriminative representations, and conditionally generated image quality significantly higher than that of state-of-the-art unsupervised generative models. Source code can be found at https://github.com/Delay-Xili/uCTRL.",
          "venue": "CPAL",
          "doi": "10.48550/arXiv.2210.16782",
          "url": "https://www.semanticscholar.org/paper/5e071a75a6906434daffe24f612fd291db4e1496",
          "authors": [
            "Shengbang Tong",
            "Xili Dai",
            "Yubei Chen",
            "Mingyang Li",
            "Zengyi Li",
            "Brent Yi",
            "Yann LeCun",
            "Y. Ma"
          ]
        },
        {
          "title": "Police: Provably Optimal Linear Constraint Enforcement For Deep Neural Networks",
          "year": 2022,
          "citations": 17,
          "abstract": "Deep Neural Networks (DNNs) outshine alternative function approximators in many settings thanks to their modularity in composing any desired differentiable operator. The formed parametrized functional is then tuned to solve a task at hand from simple gradient descent. This modularity comes at the cost of making strict enforcement of constraints on DNNs, e.g. from a priori knowledge of the task, or from desired physical properties, an open challenge. In this paper we propose the first provable affine constraint enforcement method for DNNs that only requires minimal changes into a given DNN’s forward-pass, that is computationally friendly, and that leaves the optimization of the DNN’s parameter to be unconstrained, i.e. standard gradient-based method can be employed. Our method does not require any sampling and provably ensures that the DNN fulfills the affine constraint on a given input space’s region at any point during training, and testing. We coin this method POLICE, standing for Provably Optimal LInear Constraint Enforcement. Github: https://github.com/RandallBalestriero/POLICE",
          "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
          "doi": "10.1109/ICASSP49357.2023.10096520",
          "url": "https://www.semanticscholar.org/paper/6f0aca58e13339fe78ee04b948253999a7cf85a3",
          "authors": [
            "Randall Balestriero",
            "Yann LeCun"
          ]
        },
        {
          "title": "A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27",
          "year": 2022,
          "citations": 526,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/775f42ed458b8c5b0f2094ea4ff5b64c557b1a34",
          "authors": [
            "Yann LeCun",
            "Courant"
          ]
        },
        {
          "title": "A Data-Augmentation Is Worth A Thousand Samples: Analytical Moments And Sampling-Free Training",
          "year": 2022,
          "citations": 17,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/825257577638307bd13487888fb91cc1ae11501b",
          "authors": [
            "Randall Balestriero",
            "Ishan Misra",
            "Yann LeCun"
          ]
        },
        {
          "title": "TiCo: Transformation Invariance and Covariance Contrast for Self-Supervised Visual Representation Learning",
          "year": 2022,
          "citations": 25,
          "abstract": "We present Transformation Invariance and Covariance Contrast (TiCo) for self-supervised visual representation learning. Similar to other recent self-supervised learning methods, our method is based on maximizing the agreement among embeddings of different distorted versions of the same image, which pushes the encoder to produce transformation invariant representations. To avoid the trivial solution where the encoder generates constant vectors, we regularize the covariance matrix of the embeddings from different images by penalizing low rank solutions. By jointly minimizing the transformation invariance loss and covariance contrast loss, we get an encoder that is able to produce useful representations for downstream tasks. We analyze our method and show that it can be viewed as a variant of MoCo with an implicit memory bank of unlimited size at no extra memory cost. This makes our method perform better than alternative methods when using small batch sizes. TiCo can also be seen as a modification of Barlow Twins. By connecting the contrastive and redundancy-reduction methods together, TiCo gives us new insights into how joint embedding methods work.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2206.10698",
          "url": "https://www.semanticscholar.org/paper/9539f3b6ccabf66cf54acdafd8b95421b9c2b683",
          "authors": [
            "Jiachen Zhu",
            "R. M. Moraes",
            "Serkan Karakulak",
            "Vlad Sobol",
            "A. Canziani",
            "Yann LeCun"
          ]
        },
        {
          "title": "Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations",
          "year": 2022,
          "citations": 10,
          "abstract": "Self-Supervised Learning (SSL) methods such as VICReg, Barlow Twins or W-MSE avoid collapse of their joint embedding architectures by constraining or regularizing the covariance matrix of their projector's output. This study highlights important properties of such strategy, which we coin Variance-Covariance regularization (VCReg). More precisely, we show that {\\em VCReg combined to a MLP projector enforces pairwise independence between the features of the learned representation}. This result emerges by bridging VCReg applied on the projector's output to kernel independence criteria applied on the projector's input. We empirically validate our findings where (i) we put in evidence which projector's characteristics favor pairwise independence, (ii) we demonstrate pairwise independence to be beneficial for out-of-domain generalization, (iii) we demonstrate that the scope of VCReg goes beyond SSL by using it to solve Independent Component Analysis. This provides the first theoretical motivation and explanation of MLP projectors in SSL.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2209.14905",
          "url": "https://www.semanticscholar.org/paper/95c729ce4469ba0513380759b82d3a50d648bd9b",
          "authors": [
            "G. Mialon",
            "Randall Balestriero",
            "Yann LeCun"
          ]
        },
        {
          "title": "Light-weight probing of unsupervised representations for Reinforcement Learning",
          "year": 2022,
          "citations": 20,
          "abstract": "Unsupervised visual representation learning offers the opportunity to leverage large corpora of unlabeled trajectories to form useful visual representations, which can benefit the training of reinforcement learning (RL) algorithms. However, evaluating the fitness of such representations requires training RL algorithms which is computationally intensive and has high variance outcomes. Inspired by the vision community, we study whether linear probing can be a proxy evaluation task for the quality of unsupervised RL representation. Specifically, we probe for the observed reward in a given state and the action of an expert in a given state, both of which are generally applicable to many RL domains. Through rigorous experimentation, we show that the probing tasks are strongly rank correlated with the downstream RL performance on the Atari100k Benchmark, while having lower variance and up to 600x lower computational cost. This provides a more efficient method for exploring the space of pretraining algorithms and identifying promising pretraining recipes without the need to run RL evaluations for every setting. Leveraging this framework, we further improve existing self-supervised learning (SSL) recipes for RL, highlighting the importance of the forward model, the size of the visual backbone, and the precise formulation of the unsupervised objective.",
          "venue": "RLJ",
          "doi": "10.48550/arXiv.2208.12345",
          "url": "https://www.semanticscholar.org/paper/b62f6f765f033c1f023c4a424a20571564e61d97",
          "authors": [
            "Wancong Zhang",
            "Anthony GX-Chen",
            "Vlad Sobal",
            "Yann LeCun",
            "Nicolas Carion"
          ]
        },
        {
          "title": "What Do We Maximize in Self-Supervised Learning?",
          "year": 2022,
          "citations": 22,
          "abstract": "In this paper, we examine self-supervised learning methods, particularly VICReg, to provide an information-theoretical understanding of their construction. As a first step, we demonstrate how information-theoretic quantities can be obtained for a deterministic network, offering a possible alternative to prior work that relies on stochastic models. This enables us to demonstrate how VICReg can be (re)discovered from first principles and its assumptions about data distribution. Furthermore, we empirically demonstrate the validity of our assumptions, confirming our novel understanding of VICReg. Finally, we believe that the derivation and insights we obtain can be generalized to many other SSL methods, opening new avenues for theoretical and practical understanding of SSL and transfer learning.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2207.10081",
          "url": "https://www.semanticscholar.org/paper/b83fc5d7de2cc6a5ce6e44b8a1dd9169eec62720",
          "authors": [
            "Ravid Shwartz-Ziv",
            "Randall Balestriero",
            "Yann LeCun"
          ]
        },
        {
          "title": "Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods",
          "year": 2022,
          "citations": 152,
          "abstract": "Self-Supervised Learning (SSL) surmises that inputs and pairwise positive relationships are enough to learn meaningful representations. Although SSL has recently reached a milestone: outperforming supervised methods in many modalities\\dots the theoretical foundations are limited, method-specific, and fail to provide principled design guidelines to practitioners. In this paper, we propose a unifying framework under the helm of spectral manifold learning to address those limitations. Through the course of this study, we will rigorously demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous spectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al. This unification will then allow us to obtain (i) the closed-form optimal representation for each method, (ii) the closed-form optimal network parameters in the linear regime for each method, (iii) the impact of the pairwise relations used during training on each of those quantities and on downstream task performances, and most importantly, (iv) the first theoretical bridge between contrastive and non-contrastive methods towards global and local spectral embedding methods respectively, hinting at the benefits and limitations of each. For example, (i) if the pairwise relation is aligned with the downstream task, any SSL method can be employed successfully and will recover the supervised method, but in the low data regime, VICReg's invariance hyper-parameter should be high; (ii) if the pairwise relation is misaligned with the downstream task, VICReg with small invariance hyper-parameter should be preferred over SimCLR or BarlowTwins.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b845c9d7f3fa05f56e7394f273c0c7536ee0e671",
          "authors": [
            "Randall Balestriero",
            "Yann LeCun"
          ]
        },
        {
          "title": "VICRegL: Self-Supervised Learning of Local Visual Features",
          "year": 2022,
          "citations": 148,
          "abstract": "Most recent self-supervised methods for learning image representations focus on either producing a global feature with invariance properties, or producing a set of local features. The former works best for classification tasks while the latter is best for detection and segmentation tasks. This paper explores the fundamental trade-off between learning local and global features. A new method called VICRegL is proposed that learns good global and local features simultaneously, yielding excellent performance on detection and segmentation tasks while maintaining good performance on classification tasks. Concretely, two identical branches of a standard convolutional net architecture are fed two differently distorted versions of the same image. The VICReg criterion is applied to pairs of global feature vectors. Simultaneously, the VICReg criterion is applied to pairs of local feature vectors occurring before the last pooling layer. Two local feature vectors are attracted to each other if their l2-distance is below a threshold or if their relative locations are consistent with a known geometric transformation between the two input images. We demonstrate strong performance on linear classification and segmentation transfer tasks. Code and pretrained models are publicly available at: https://github.com/facebookresearch/VICRegL",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2210.01571",
          "url": "https://www.semanticscholar.org/paper/b8d4beeb8df994db12688226cf2a619f1d633b72",
          "authors": [
            "Adrien Bardes",
            "J. Ponce",
            "Yann LeCun"
          ]
        },
        {
          "title": "Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding",
          "year": 2022,
          "citations": 17,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2206.08954",
          "url": "https://www.semanticscholar.org/paper/cf209f6cfa987e7b9320c24a8e9cb70bf39f31a7",
          "authors": [
            "Yubei Chen",
            "Adrien Bardes",
            "Zengyi Li",
            "Yann LeCun"
          ]
        },
        {
          "title": "Extracting structural folding pattern of chromatin using chromatin condensation data",
          "year": 2022,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d36dcc52d9d8bc1a4b43469977653cebce579dd8",
          "authors": [
            "Samira Mali",
            "N. L. V. Berkum",
            "Louise Williams",
            "Maxim Imakaev",
            "T. Ragoczy",
            "A. Telling",
            "N. A. Kinney",
            "I. Sharakhov",
            "A. Onufriev",
            "Jia Deng",
            "Wei Dong",
            "R. Socher",
            "Lijuan Li",
            "Kaixia Li",
            "Yann LeCun",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard"
          ]
        },
        {
          "title": "projUNN: efficient method for training deep networks with unitary matrices",
          "year": 2022,
          "citations": 34,
          "abstract": "In learning with recurrent or very deep feed-forward networks, employing unitary matrices in each layer can be very effective at maintaining long-range stability. However, restricting network parameters to be unitary typically comes at the cost of expensive parameterizations or increased training runtime. We propose instead an efficient method based on rank-$k$ updates -- or their rank-$k$ approximation -- that maintains performance at a nearly optimal training runtime. We introduce two variants of this method, named Direct (projUNN-D) and Tangent (projUNN-T) projected Unitary Neural Networks, that can parameterize full $N$-dimensional unitary or orthogonal matrices with a training runtime scaling as $O(kN^2)$. Our method either projects low-rank gradients onto the closest unitary matrix (projUNN-T) or transports unitary matrices in the direction of the low-rank gradient (projUNN-D). Even in the fastest setting ($k=1$), projUNN is able to train a model's unitary parameters to reach comparable performances against baseline implementations. In recurrent neural network settings, projUNN closely matches or exceeds benchmarked results from prior unitary neural networks. Finally, we preliminarily explore projUNN in training orthogonal convolutional neural networks, which are currently unable to outperform state of the art models but can potentially enhance stability and robustness at large depth.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2203.05483",
          "url": "https://www.semanticscholar.org/paper/d43f665fad45256659dce9e9d2c2a05a6383e5b6",
          "authors": [
            "B. Kiani",
            "Randall Balestriero",
            "Yann LeCun",
            "S. Lloyd"
          ]
        },
        {
          "title": "The Effects of Regularization and Data Augmentation are Class Dependent",
          "year": 2022,
          "citations": 109,
          "abstract": "Regularization is a fundamental technique to prevent over-fitting and to improve generalization performances by constraining a model's complexity. Current Deep Networks heavily rely on regularizers such as Data-Augmentation (DA) or weight-decay, and employ structural risk minimization, i.e. cross-validation, to select the optimal regularization hyper-parameters. In this study, we demonstrate that techniques such as DA or weight decay produce a model with a reduced complexity that is unfair across classes. The optimal amount of DA or weight decay found from cross-validation leads to disastrous model performances on some classes e.g. on Imagenet with a resnet50, the\"barn spider\"classification test accuracy falls from $68\\%$ to $46\\%$ only by introducing random crop DA during training. Even more surprising, such performance drop also appears when introducing uninformative regularization techniques such as weight decay. Those results demonstrate that our search for ever increasing generalization performance -- averaged over all classes and samples -- has left us with models and regularizers that silently sacrifice performances on some classes. This scenario can become dangerous when deploying a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on INaturalist sees its performances fall from $70\\%$ to $30\\%$ on class \\#8889 when introducing random crop DA during the Imagenet pre-training phase. Those results demonstrate that designing novel regularizers without class-dependent bias remains an open research question.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2204.03632",
          "url": "https://www.semanticscholar.org/paper/dcda4897113ed03c920e2e94a90ee33e09781759",
          "authors": [
            "Randall Balestriero",
            "L. Bottou",
            "Yann LeCun"
          ]
        },
        {
          "title": "Masked Siamese ConvNets",
          "year": 2022,
          "citations": 37,
          "abstract": "Self-supervised learning has shown superior performances over supervised methods on various vision benchmarks. The siamese network, which encourages embeddings to be invariant to distortions, is one of the most successful self-supervised visual representation learning approaches. Among all the augmentation methods, masking is the most general and straightforward method that has the potential to be applied to all kinds of input and requires the least amount of domain knowledge. However, masked siamese networks require particular inductive bias and practically only work well with Vision Transformers. This work empirically studies the problems behind masked siamese networks with ConvNets. We propose several empirical designs to overcome these problems gradually. Our method performs competitively on low-shot image classification and outperforms previous methods on object detection benchmarks. We discuss several remaining issues and hope this work can provide useful data points for future general-purpose self-supervised learning.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2206.07700",
          "url": "https://www.semanticscholar.org/paper/df1af149a7b9aaa2ef0bb05adf639e9ae6b56249",
          "authors": [
            "L. Jing",
            "Jiachen Zhu",
            "Yann LeCun"
          ]
        },
        {
          "title": "Joint Embedding Self-Supervised Learning in the Kernel Regime",
          "year": 2022,
          "citations": 15,
          "abstract": "The fundamental goal of self-supervised learning (SSL) is to produce useful representations of data without access to any labels for classifying the data. Modern methods in SSL, which form representations based on known or constructed relationships between samples, have been particularly effective at this task. Here, we aim to extend this framework to incorporate algorithms based on kernel methods where embeddings are constructed by linear maps acting on the feature space of a kernel. In this kernel regime, we derive methods to find the optimal form of the output representations for contrastive and non-contrastive loss functions. This procedure produces a new representation space with an inner product denoted as the induced kernel which generally correlates points which are related by an augmentation in kernel space and de-correlates points otherwise. We analyze our kernel model on small datasets to identify common features of self-supervised learning algorithms and gain theoretical insights into their performance on downstream tasks.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2209.14884",
          "url": "https://www.semanticscholar.org/paper/ebcccb72117b4641df9b88bac5b51dc5d2cead27",
          "authors": [
            "B. Kiani",
            "Randall Balestriero",
            "Yubei Chen",
            "S. Lloyd",
            "Yann LeCun"
          ]
        },
        {
          "title": "Joint Embedding Predictive Architectures Focus on Slow Features",
          "year": 2022,
          "citations": 12,
          "abstract": "Many common methods for learning a world model for pixel-based environments use generative architectures trained with pixel-level reconstruction objectives. Recently proposed Joint Embedding Predictive Architectures (JEPA) offer a reconstruction-free alternative. In this work, we analyze performance of JEPA trained with VICReg and SimCLR objectives in the fully offline setting without access to rewards, and compare the results to the performance of the generative architecture. We test the methods in a simple environment with a moving dot with various background distractors, and probe learned representations for the dot's location. We find that JEPA methods perform on par or better than reconstruction when distractor noise changes every time step, but fail when the noise is fixed. Furthermore, we provide a theoretical explanation for the poor performance of JEPA-based methods with fixed noise, highlighting an important limitation.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2211.10831",
          "url": "https://www.semanticscholar.org/paper/ed009b7423dcfec47708fb5817ec4955e4265757",
          "authors": [
            "Vlad Sobal",
            "V JyothirS",
            "Siddhartha Jalagam",
            "Nicolas Carion",
            "Kyunghyun Cho",
            "Yann LeCun"
          ]
        },
        {
          "title": "Deep learning, reinforcement learning, and world models",
          "year": 2022,
          "citations": 375,
          "abstract": null,
          "venue": "Neural Networks",
          "doi": "10.1016/j.neunet.2022.03.037",
          "url": "https://www.semanticscholar.org/paper/effa7077e6580a8ab22c30d3c876744b4e51cd6e",
          "authors": [
            "Yu Matsuo",
            "Yann LeCun",
            "M. Sahani",
            "Doina Precup",
            "David Silver",
            "Masashi Sugiyama",
            "E. Uchibe",
            "J. Morimoto"
          ]
        },
        {
          "title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning",
          "year": 2021,
          "citations": 1074,
          "abstract": "Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0d0cf5f64c052aa7edc5bb638203616a620557f6",
          "authors": [
            "Adrien Bardes",
            "J. Ponce",
            "Yann LeCun"
          ]
        },
        {
          "title": "Compact and Optimal Deep Learning with Recurrent Parameter Generators",
          "year": 2021,
          "citations": 5,
          "abstract": "Deep learning has achieved tremendous success by training increasingly large models, which are then compressed for practical deployment. We propose a drastically different approach to compact and optimal deep learning: We decouple the Degrees of freedom (DoF) and the actual number of parameters of a model, optimize a small DoF with predefined random linear constraints for a large model of an arbitrary architecture, in one-stage end-to-end learning.Specifically, we create a recurrent parameter generator (RPG), which repeatedly fetches parameters from a ring and unpacks them onto a large model with random permutation and sign flipping to promote parameter decorrelation. We show that gradient descent can automatically find the best model under constraints with in fact faster convergence.Our extensive experimentation reveals a log-linear relationship between model DoF and accuracy. Our RPG demonstrates remarkable DoF reduction, and can be further pruned and quantized for additional run-time performance gain. For example, in terms of top-1 accuracy on ImageNet, RPG achieves 96% of ResNet18’s performance with only 18% DoF (the equivalent of one convolutional layer) and 52% of ResNet34’s performance with only 0.25% DoF! Our work shows significant potential of constrained neural opti-mization in compact and optimal deep learning.",
          "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
          "doi": "10.1109/WACV56688.2023.00389",
          "url": "https://www.semanticscholar.org/paper/15dc19495e95703f96989bd66135eb3bc4057976",
          "authors": [
            "Jiayun Wang",
            "Yubei Chen",
            "Stella X. Yu",
            "Brian Cheung",
            "Yann LeCun"
          ]
        },
        {
          "title": "Understanding Dimensional Collapse in Contrastive Self-supervised Learning",
          "year": 2021,
          "citations": 412,
          "abstract": "Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/28c17db217f2d7af12482a087d197851f0a97db0",
          "authors": [
            "Li Jing",
            "Pascal Vincent",
            "Yann LeCun",
            "Yuandong Tian"
          ]
        },
        {
          "title": "Recurrent Parameter Generators",
          "year": 2021,
          "citations": 3,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/310215110bb72f8dfce8be73a904ea1c4968739f",
          "authors": [
            "Jiayun Wang",
            "Yubei Chen",
            "Stella X. Yu",
            "Brian Cheung",
            "Yann LeCun"
          ]
        },
        {
          "title": "Decoupled Contrastive Learning",
          "year": 2021,
          "citations": 219,
          "abstract": "Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented\"views\"of the same image as positive to be pulled closer, and all other images as negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, including large sample batches, extensive training epochs, etc. We are thus motivated to tackle these issues and establish a simple, efficient, yet competitive baseline of contrastive learning. Specifically, we identify, from theoretical and empirical studies, a noticeable negative-positive-coupling (NPC) effect in the widely used InfoNCE loss, leading to unsuitable learning efficiency concerning the batch size. By removing the NPC effect, we propose decoupled contrastive learning (DCL) loss, which removes the positive term from the denominator and significantly improves the learning efficiency. DCL achieves competitive performance with less sensitivity to sub-optimal hyperparameters, requiring neither large batches in SimCLR, momentum encoding in MoCo, or large epochs. We demonstrate with various benchmarks while manifesting robustness as much less sensitive to suboptimal hyperparameters. Notably, SimCLR with DCL achieves 68.2% ImageNet-1K top-1 accuracy using batch size 256 within 200 epochs pre-training, outperforming its SimCLR baseline by 6.4%. Further, DCL can be combined with the SOTA contrastive learning method, NNCLR, to achieve 72.3% ImageNet-1K top-1 accuracy with 512 batch size in 400 epochs, which represents a new SOTA in contrastive learning. We believe DCL provides a valuable baseline for future contrastive SSL studies.",
          "venue": "European Conference on Computer Vision",
          "doi": "10.1007/978-3-031-19809-0_38",
          "url": "https://www.semanticscholar.org/paper/40b68df4635298c32725891bc46ee0201dac56c1",
          "authors": [
            "Chun-Hsiao Yeh",
            "Cheng-Yao Hong",
            "Yen-Chi Hsu",
            "Tyng-Luh Liu",
            "Yubei Chen",
            "Yann LeCun"
          ]
        },
        {
          "title": "Sparse Coding with Multi-Layer Decoders using Variance Regularization",
          "year": 2021,
          "citations": 13,
          "abstract": "Sparse representations of images are useful in many computer vision applications. Sparse coding with an $l_1$ penalty and a learned linear dictionary requires regularization of the dictionary to prevent a collapse in the $l_1$ norms of the codes. Typically, this regularization entails bounding the Euclidean norms of the dictionary's elements. In this work, we propose a novel sparse coding protocol which prevents a collapse in the codes without the need to regularize the decoder. Our method regularizes the codes directly so that each latent code component has variance greater than a fixed threshold over a set of sparse representations for a given set of inputs. Furthermore, we explore ways to effectively train sparse coding systems with multi-layer decoders since they can model more complex relationships than linear dictionaries. In our experiments with MNIST and natural image patches, we show that decoders learned with our approach have interpretable features both in the linear and multi-layer case. Moreover, we show that sparse autoencoders with multi-layer decoders trained using our variance regularization method produce higher quality reconstructions with sparser representations when compared to autoencoders with linear dictionaries. Additionally, sparse representations obtained with our variance regularization approach are useful in the downstream tasks of denoising and classification in the low-data regime.",
          "venue": "Trans. Mach. Learn. Res.",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6e08acdb27e83a26ef7e67539a11e9cb3588e822",
          "authors": [
            "Katrina Evtimova",
            "Yann LeCun"
          ]
        },
        {
          "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding",
          "year": 2021,
          "citations": 1023,
          "abstract": "Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.",
          "venue": "IEEE International Conference on Computer Vision",
          "doi": "10.1109/ICCV48922.2021.00180",
          "url": "https://www.semanticscholar.org/paper/7ba9c013988eaff5cd186d73704af329d027872d",
          "authors": [
            "Aishwarya Kamath",
            "Mannat Singh",
            "Yann LeCun",
            "Ishan Misra",
            "Gabriel Synnaeve",
            "Nicolas Carion"
          ]
        },
        {
          "title": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors",
          "year": 2021,
          "citations": 101,
          "abstract": "Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these ‘black boxes’ as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at: https://github.com/zeyuyun1/TransformerVis.",
          "venue": "Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out",
          "doi": "10.18653/V1/2021.DEELIO-1.1",
          "url": "https://www.semanticscholar.org/paper/7cc88a1a904e8bb6edc1123c0800d1c5a0ea435d",
          "authors": [
            "Zeyu Yun",
            "Yubei Chen",
            "B. Olshausen",
            "Yann LeCun"
          ]
        },
        {
          "title": "Inspirational Adversarial Image Generation",
          "year": 2021,
          "citations": 3,
          "abstract": "The task of image generation started receiving some attention from artists and designers, providing inspiration for new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control over the output. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user’s choosing by performing several optimization steps to recover optimal parameters from the model’s latent space. We tested several exploration methods from classical gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so they can even be used without numerical criterion nor inspirational image, only with human preferences. Thus, by iterating on one’s preferences we can make robust facial composite or fashion generation algorithms. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.",
          "venue": "IEEE Transactions on Image Processing",
          "doi": "10.1109/TIP.2021.3065845",
          "url": "https://www.semanticscholar.org/paper/836e4a8b665f46729e55e7674b4d8b0df6c1d091",
          "authors": [
            "Baptiste Rozière",
            "M. Rivière",
            "O. Teytaud",
            "J. Rapin",
            "Yann LeCun",
            "C. Couprie"
          ]
        },
        {
          "title": "Deep learning for AI",
          "year": 2021,
          "citations": 560,
          "abstract": "How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language?",
          "venue": "Communications of the ACM",
          "doi": "10.1145/3448250",
          "url": "https://www.semanticscholar.org/paper/87d5b61f5b6fdb0a57fc66b5c5bb428c398eaa86",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun",
            "Geoffrey E. Hinton"
          ]
        },
        {
          "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
          "year": 2021,
          "citations": 2675,
          "abstract": "Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8a9d84d86ac0d76e63914802f9738325c3bece9c",
          "authors": [
            "Jure Zbontar",
            "Li Jing",
            "Ishan Misra",
            "Yann LeCun",
            "Stéphane Deny"
          ]
        },
        {
          "title": "Neural Potts Model",
          "year": 2021,
          "citations": 8,
          "abstract": "We propose the Neural Potts Model objective as an amortized optimization problem. The objective enables training a single model with shared parameters to explicitly model energy landscapes across multiple protein families. Given a protein sequence as input, the model is trained to predict a pairwise coupling matrix for a Potts model energy function describing the local evolutionary landscape of the sequence. Couplings can be predicted for novel sequences. A controlled ablation experiment assessing unsupervised contact prediction on sets of related protein families finds a gain from amortization for low-depth multiple sequence alignments; the result is then confirmed on a database with broad coverage of protein sequences.",
          "venue": "bioRxiv",
          "doi": "10.1101/2021.04.08.439084",
          "url": "https://www.semanticscholar.org/paper/94526a3b070927dc7a0f066cf0a17b8abaf5ce1f",
          "authors": [
            "Tom Sercu",
            "Robert Verkuil",
            "Joshua Meier",
            "Brandon Amos",
            "Zeming Lin",
            "Caroline Chen",
            "Jason Liu",
            "Yann LeCun",
            "Alexander Rives"
          ]
        },
        {
          "title": "Handwritten Digit Recognition Using TensorFlow Lite Micro on i.MX RT devices",
          "year": 2021,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b9ec816bc53bd86dc3849545bf8516bc0cb8cb9f",
          "authors": [
            "Yann LeCun",
            "Corinna Cortes"
          ]
        },
        {
          "title": "Optimization of Artificial Neural Network Hyperparameters For Processing Retrospective Information",
          "year": 2021,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/be59058dd2320648826021b8c04d4cfcf0c18431",
          "authors": [
            "A. Rogachev",
            "F. Scholle",
            "Yann LeCun",
            "Yoshua Bengio",
            "I. L. Kashirin",
            "M. Demchenko"
          ]
        },
        {
          "title": "Learning in High Dimension Always Amounts to Extrapolation",
          "year": 2021,
          "citations": 113,
          "abstract": "The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample $x$ whenever this sample falls inside or on the boundary of the given dataset's convex hull. Extrapolation occurs when $x$ falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional ($>$100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation definition as an indicator of generalization performances.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e75c388b60cf447be7148be25feeee3e10d12cf4",
          "authors": [
            "Randall Balestriero",
            "J. Pesenti",
            "Yann LeCun"
          ]
        },
        {
          "title": "Implicit Rank-Minimizing Autoencoder",
          "year": 2020,
          "citations": 51,
          "abstract": "An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate the validity of the method on several image generation and representation learning tasks.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/10266bc9d3f6cf1966c846217b66ab5f26006446",
          "authors": [
            "Li Jing",
            "Jure Zbontar",
            "Yann LeCun"
          ]
        },
        {
          "title": "Jean piaget theory in punjabi pdf",
          "year": 2020,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/759f608c5246b7d634dfe13a21adc4731da4bddd",
          "authors": [
            "Jean PiagetPiaget",
            "H. Bergson",
            "P. Janet",
            "A. Binet",
            "Théodore Simon",
            "S. Spielrein",
            "Shlomo Wolbe",
            "B. Inhelder",
            "J. Bruner",
            "K. Kaye",
            "Citation",
            "L. Kohlberg",
            "R. Kegan",
            "H. Gardner",
            "T. Kuhn",
            "J. Flavell",
            "Yann LeCun",
            "J. Peterson",
            "J. Piaget"
          ]
        },
        {
          "title": "The Mind of a Mouse",
          "year": 2020,
          "citations": 160,
          "abstract": null,
          "venue": "Cell",
          "doi": "10.1016/j.cell.2020.08.010",
          "url": "https://www.semanticscholar.org/paper/a5fd28aa9240835a14b1c285096d62e3a3ba5722",
          "authors": [
            "L. Abbott",
            "D. Bock",
            "E. Callaway",
            "W. Denk",
            "C. Dulac",
            "A. Fairhall",
            "I. Fiete",
            "Kristen M. Harris",
            "M. Helmstaedter",
            "Viren Jain",
            "N. Kasthuri",
            "Yann LeCun",
            "J. Lichtman",
            "P. Littlewood",
            "L. Luo",
            "John H. R. Maunsell",
            "R. Reid",
            "B. Rosen",
            "G. Rubin",
            "T. Sejnowski",
            "H. Seung",
            "K. Svoboda",
            "D. Tank",
            "Doris Y. Tsao",
            "D. C. Essen"
          ]
        },
        {
          "title": "Sticky Reasoning within Learning Representations",
          "year": 2019,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0633bf78c0b5aa041c881bd334906733302a4427",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic",
          "year": 2019,
          "citations": 125,
          "abstract": "Learning a policy using only observational data is challenging because the distribution of states it induces at execution time may differ from the distribution observed during training. We propose to train a policy by unrolling a learned model of the environment dynamics over multiple time steps while explicitly penalizing two costs: the original cost the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on. We measure this second cost by using the uncertainty of the dynamics model about its own predictions, using recent ideas from uncertainty estimation for deep networks. We evaluate our approach using a large-scale observational dataset of driving behavior recorded from traffic cameras, and show that we are able to learn effective driving policies from purely observational data, with no environment interaction.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2eeace98cf3c105a8d37884dc8d33c50ae4b7ddb",
          "authors": [
            "Mikael Henaff",
            "A. Canziani",
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning about an exponential amount of conditional distributions",
          "year": 2019,
          "citations": 29,
          "abstract": "We introduce the Neural Conditioner (NC), a self-supervised machine able to learn about all the conditional distributions of a random vector $X$. The NC is a function $NC(x \\cdot a, a, r)$ that leverages adversarial training to match each conditional distribution $P(X_r|X_a=x_a)$. After training, the NC generalizes to sample from conditional distributions never seen, including the joint distribution. The NC is also able to auto-encode examples, providing data representations useful for downstream classification tasks. In sum, the NC integrates different self-supervised tasks (each being the estimation of a conditional distribution) and levels of supervision (partially observed data) seamlessly into a single learning experience.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/4e48a5a00a79cb586c1c912507ec6e52890cdeab",
          "authors": [
            "Mohamed Ishmael Belghazi",
            "M. Oquab",
            "Yann LeCun",
            "David Lopez-Paz"
          ]
        },
        {
          "title": "The role of over-parametrization in generalization of neural networks",
          "year": 2019,
          "citations": 202,
          "abstract": null,
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6e8001afb2e24b648ae9ceb4e00c20ded4a5ee99",
          "authors": [
            "Behnam Neyshabur",
            "Zhiyuan Li",
            "Srinadh Bhojanapalli",
            "Yann LeCun",
            "N. Srebro"
          ]
        },
        {
          "title": "Inspirational Adversarial Image Generation",
          "year": 2019,
          "citations": 24,
          "abstract": "The task of image generation started to receive some attention from artists and designers to inspire them in new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control on them. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user choice by performing several optimization steps to recover optimal parameters from the model's latent space. We tested several exploration methods starting with classic gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so that they can even be used without numerical criterion, without inspirational image, but with only with human preference. Thus, by iterating on one's preferences we could make robust Facial Composite or Fashion Generation algorithms. High resolution of the produced design generations are obtained using progressive growing of GANs. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8881f1b45ec6f0e1805627c828629cdd925bcfc6",
          "authors": [
            "Baptiste Rozière",
            "M. Rivière",
            "O. Teytaud",
            "J. Rapin",
            "Yann LeCun",
            "C. Couprie"
          ]
        },
        {
          "title": "1.1 Deep Learning Hardware: Past, Present, and Future",
          "year": 2019,
          "citations": 115,
          "abstract": "Historically, progress in neural networks and deep learning research has been greatly influenced by the available hardware and software tools. This paper identifies trends in deep learning research that will influence hardware architectures and software platforms of the future.",
          "venue": "IEEE International Solid-State Circuits Conference",
          "doi": "10.1109/ISSCC.2019.8662396",
          "url": "https://www.semanticscholar.org/paper/91788beae2cd32b13c9ece1e724fd1988d9d9629",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Unsupervised Image Matching and Object Discovery as Optimization",
          "year": 2019,
          "citations": 66,
          "abstract": "Learning with complete or partial supervision is power- ful but relies on ever-growing human annotation efforts. As a way to mitigate this serious problem, as well as to serve specific applications, unsupervised learning has emerged as an important field of research. In computer vision, unsu- pervised learning comes in various guises. We focus here on the unsupervised discovery and matching of object cate- gories among images in a collection, following the work of Cho et al. [12]. We show that the original approach can be reformulated and solved as a proper optimization problem. Experiments on several benchmarks establish the merit of our approach.",
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2019.00848",
          "url": "https://www.semanticscholar.org/paper/96f68a831d723dc0d841daa8e5c1f457603140d3",
          "authors": [
            "Huy V. Vo",
            "F. Bach",
            "Minsu Cho",
            "Kai Han",
            "Yann LeCun",
            "P. Pérez",
            "J. Ponce"
          ]
        },
        {
          "title": "Lesson planning",
          "year": 2019,
          "citations": 0,
          "abstract": null,
          "venue": "Healthcare Simulation at a Glance",
          "doi": "10.1007/978-1-4419-1428-6_4758",
          "url": "https://www.semanticscholar.org/paper/d6df4299193a145080896c67f5dfe34b9ad61fea",
          "authors": [
            "Cecilia Ka Yuk Chan",
            "Zoya A. Zorina",
            "Jesse R. Sparks",
            "S. López Ornat",
            "Christine D. Tsang",
            "Elena L. Grigorenko",
            "R. Lubow",
            "John C. Malone",
            "Michael Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "Thomas R. Minor",
            "Traci N. Plumb",
            "Hendrik Drachsler",
            "P. A. Kirschner",
            "Minoru Nakayama",
            "Rowena Santiago",
            "Ali Simsek",
            "Fang-Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "Richard P. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "Luis C. Lamb",
            "Artur d’Avila Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "Tracy Packiam Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "Gabriele Kern-Isberner",
            "Joost Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C.-J. Olsson",
            "Lars Nyberg",
            "R. Freivalds",
            "Lynne Hall",
            "M. Hall",
            "Ulrike Hanke",
            "Lin S. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "Laura-Rose Thorogood",
            "M. Udell",
            "C. Wynne",
            "Paul C. Burnett",
            "M. Cannon",
            "Amy C. Edmondson",
            "Pitoyo Hartono",
            "A. Callender",
            "Rachel Barr",
            "Natalie Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohd. Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "David A. Kolb",
            "R. Reilly",
            "Klaus Nielsen",
            "P. Couvillon",
            "Heather King",
            "Justin Dillon",
            "Delia Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "Peter C. R. Lane",
            "Som Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "Don M. Tucker",
            "Paul J. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "Daniel Ansari",
            "K. Lionello-DeNolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. U. Hoppe",
            "Krisztina Molnar",
            "Erica de Vries",
            "Emmanuel G. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "Tristan Cazenave",
            "Ton Jong",
            "Jan Meij",
            "J. Gavelek",
            "Ailing Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "Michael J. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "Dong-oh Seo",
            "M. Monfils",
            "Anthony A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "Michael J. Kennedy",
            "Michael Jackson",
            "Terezinha Nunes",
            "B. Jackling",
            "R. Howard",
            "William G. Kennedy",
            "L. C. Drickamer"
          ]
        },
        {
          "title": "A Spectral Regularizer for Unsupervised Disentanglement",
          "year": 2018,
          "citations": 43,
          "abstract": "A generative model with a disentangled representation allows for independent control over different aspects of the output. Learning disentangled representations has been a recent topic of great interest, but it remains poorly understood. We show that even for GANs that do not possess disentangled representations, one can find curved trajectories in latent space over which local disentanglement occurs. These trajectories are found by iteratively following the leading right-singular vectors of the Jacobian of the generator with respect to its input. Based on this insight, we describe an efficient regularizer that aligns these vectors with the coordinate axes, and show that it can be used to induce disentangled representations in GANs, in a completely unsupervised manner.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0ac54252eab1ac7641e03bc9b7344e2064e06405",
          "authors": [
            "A. Ramesh",
            "Youngduck Choi",
            "Yann LeCun"
          ]
        },
        {
          "title": "Byte-Level Recursive Convolutional Auto-Encoder for Text",
          "year": 2018,
          "citations": 3,
          "abstract": "This article proposes to auto-encode text at byte-level using convolutional networks with a recursive architecture. The motivation is to explore whether it is possible to have scalable and homogeneous text generation at byte-level in a non-sequential fashion through the simple task of auto-encoding. We show that non-sequential text generation from a fixed-length representation is not only possible, but also achieved much better auto-encoding results than recurrent networks. The proposed model is a multi-stage deep convolutional encoder-decoder framework using residual connections, containing up to 160 parameterized layers. Each encoder or decoder contains a shared group of modules that consists of either pooling or upsampling layers, making the network recursive in terms of abstraction levels in representation. Results for 6 large-scale paragraph datasets are reported, in 3 languages including Arabic, Chinese and English. Analyses are conducted to study several properties of the proposed model.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/290c96ff9be2fb517ff2eed3f55fdde919e44f05",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun"
          ]
        },
        {
          "title": "GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations",
          "year": 2018,
          "citations": 36,
          "abstract": "Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden unit), or embedding-free units such as image pixels.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/446efa0bcf3528b51332a12495cb56784dd8bad3",
          "authors": [
            "Zhilin Yang",
            "J. Zhao",
            "Bhuwan Dhingra",
            "Kaiming He",
            "William W. Cohen",
            "R. Salakhutdinov",
            "Yann LeCun"
          ]
        },
        {
          "title": "Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks",
          "year": 2018,
          "citations": 393,
          "abstract": "Despite existing work on ensuring generalization of neural networks in terms of scale sensitive complexity measures, such as norms, margin and sharpness, these complexity measures do not offer an explanation of why neural networks generalize better with over-parametrization. In this work we suggest a novel complexity measure based on unit-wise capacities resulting in a tighter generalization bound for two layer ReLU networks. Our capacity bound correlates with the behavior of test error with increasing network sizes, and could potentially explain the improvement in generalization with over-parametrization. We further present a matching lower bound for the Rademacher complexity that improves over previous capacity lower bounds for neural networks.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/51063caeb691f9a20e34feb721f14660a0df968e",
          "authors": [
            "Behnam Neyshabur",
            "Zhiyuan Li",
            "Srinadh Bhojanapalli",
            "Yann LeCun",
            "N. Srebro"
          ]
        },
        {
          "title": "GLoMo: Unsupervised Learning of Transferable Relational Graphs",
          "year": 2018,
          "citations": 23,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/684e712f59f11d2bdc98be4c210824ab9e6f11f4",
          "authors": [
            "Zhilin Yang",
            "J. Zhao",
            "Bhuwan Dhingra",
            "Kaiming He",
            "William W. Cohen",
            "R. Salakhutdinov",
            "Yann LeCun"
          ]
        },
        {
          "title": "AAAS AMA: Hi, we’re researchers from Google, Microsoft, and Facebook who study Artificial Intelligence. Ask us anything!",
          "year": 2018,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": "10.15200/WINN.151896.65484",
          "url": "https://www.semanticscholar.org/paper/7abb55c10bbc1cd590723edd0e1bb457c1b021ee",
          "authors": [
            "Yann LeCun",
            "AI Facebook",
            "New Research",
            "NY Eric York",
            "Horvitz",
            "Eric Horvitz"
          ]
        },
        {
          "title": "The Power and Limits of Deep Learning",
          "year": 2018,
          "citations": 32,
          "abstract": "Artificial intelligence (AI) is advancing very rapidly. I’ve had a front-row seat for a lot of the recent progress—first at Bell Labs (which was renamed AT&T Labs in 1996, while I was there) and th...",
          "venue": "Research technology management",
          "doi": "10.1080/08956308.2018.1516928",
          "url": "https://www.semanticscholar.org/paper/9e6006531597c0f8422e25de7d62c068ad9e68ee",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning with Reflective Likelihoods",
          "year": 2018,
          "citations": 5,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ad81013e8afc5e2fd5d279468dd6c9bb504179e7",
          "authors": [
            "A. B. Dieng",
            "Kyunghyun Cho",
            "D. Blei",
            "Yann LeCun"
          ]
        },
        {
          "title": "Adversarially-Trained Normalized Noisy-Feature Auto-Encoder for Text Generation",
          "year": 2018,
          "citations": 0,
          "abstract": "This article proposes Adversarially-Trained Normalized Noisy-Feature Auto-Encoder (ATNNFAE) for byte-level text generation. An ATNNFAE consists of an auto-encoder where the internal code is normalized on the unit sphere and corrupted by additive noise. Simultaneously, a replica of the decoder (sharing the same parameters as the AE decoder) is used as the generator and fed with random latent vectors. An adversarial discriminator is trained to distinguish training samples reconstructed from the AE from samples produced through the random-input generator, making the entire generator-discriminator path differentiable for discrete data like text. The combined effect of noise injection in the code and shared weights between the decoder and the generator can prevent the mode collapsing phenomenon commonly observed in GANs. Since perplexity cannot be applied to non-sequential text generation, we propose a new evaluation method using the total variance distance between frequencies of hash-coded byte-level n-grams (NGTVD). NGTVD is a single benchmark that can characterize both the quality and the diversity of the generated texts. Experiments are offered in 6 large-scale datasets in Arabic, Chinese and English, with comparisons against n-gram baselines and recurrent neural networks (RNNs). Ablation study on both the noise level and the discriminator is performed. We find that RNNs have trouble competing with the n-gram baselines, and the ATNNFAE results are generally competitive.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c03607de715a6c578ac720a074d7fb494e37cc92",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun"
          ]
        },
        {
          "title": "Backpropagation for Implicit Spectral Densities",
          "year": 2018,
          "citations": 10,
          "abstract": "Most successful machine intelligence systems rely on gradient-based learning, which is made possible by backpropagation. Some systems are designed to aid us in interpreting data when explicit goals cannot be provided. These unsupervised systems are commonly trained by backpropagating through a likelihood function. We introduce a tool that allows us to do this even when the likelihood is not explicitly set, by instead using the implicit likelihood of the model. Explicitly defining the likelihood often entails making heavy-handed assumptions that impede our ability to solve challenging tasks. On the other hand, the implicit likelihood of the model is accessible without the need for such assumptions. Our tool, which we call spectral backpropagation, allows us to optimize it in much greater generality than what has been attempted before. GANs can also be viewed as a technique for optimizing implicit likelihoods. We study them using spectral backpropagation in order to demonstrate robustness for high-dimensional problems, and identify two novel properties of the generator G: (1) there exist aberrant, nonsensical outputs to which G assigns very high likelihood, and (2) the eigenvectors of the metric induced by G over latent space correspond to quasi-disentangled explanatory factors.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c52fd8ed00b4d3275889532f2f345130af462848",
          "authors": [
            "A. Ramesh",
            "Yann LeCun"
          ]
        },
        {
          "title": "Predicting Future Instance Segmentations by Forecasting Convolutional Features",
          "year": 2018,
          "citations": 96,
          "abstract": "Anticipating future events is an important prerequisite towards intelligent behavior. Video forecasting has been studied as a proxy task towards this goal. Recent work has shown that to predict semantic segmentation of future frames, forecasting at the semantic level is more effective than forecasting RGB frames and then segmenting these. In this paper we consider the more challenging problem of future instance segmentation, which additionally segments out individual objects. To deal with a varying number of output labels per image, we develop a predictive model in the space of fixed-sized convolutional features of the Mask R-CNN instance segmentation model. We apply the \"detection head\" of Mask R-CNN on the predicted features to produce the instance segmentation of future frames. Experiments show that this approach significantly improves over baselines based on optical flow.",
          "venue": "European Conference on Computer Vision",
          "doi": "10.1007/978-3-030-01240-3_36",
          "url": "https://www.semanticscholar.org/paper/cb503d90c0bd9a555a6b99429991c4d6f39e0f70",
          "authors": [
            "Pauline Luc",
            "C. Couprie",
            "Yann LeCun",
            "Jakob Verbeek"
          ]
        },
        {
          "title": "DeSIGN: Design Inspiration from Generative Networks",
          "year": 2018,
          "citations": 131,
          "abstract": "Can an algorithm create original and compelling fashion designs to serve as an inspirational assistant? To help answer this question, we design and investigate different image generation models associated with different loss functions to boost creativity in fashion generation. The dimensions of our explorations include: (i) different Generative Adversarial Networks architectures that start from noise vectors to generate fashion items, (ii) novel loss functions that encourage novelty, inspired from Sharma-Mittal divergence, a generalized mutual information measure for the widely used relative entropies such as Kullback-Leibler, and (iii) a generation process following the key elements of fashion design (disentangling shape and texture components). A key challenge of this study is the evaluation of generated designs and the retrieval of best ones, hence we put together an evaluation protocol associating automatic metrics and human experimental studies that we hope will help ease future research. We show that our proposed creativity criterion yield better overall appreciation than the one employed in Creative Adversarial Networks. In the end, about 61% of our images are thought to be created by human designers rather than by a computer while also being considered original per our human subject experiments, and our proposed loss scores the highest compared to existing losses in both novelty and likability.",
          "venue": "ECCV Workshops",
          "doi": "10.1007/978-3-030-11015-4_5",
          "url": "https://www.semanticscholar.org/paper/e9e605d95a2884cc6a4cdf1745472dec9eb23b88",
          "authors": [
            "Othman Sbai",
            "Mohamed Elhoseiny",
            "Antoine Bordes",
            "Yann LeCun",
            "C. Couprie"
          ]
        },
        {
          "title": "Comparing dynamics: deep neural networks versus glassy systems",
          "year": 2018,
          "citations": 122,
          "abstract": "We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are (1) the complexity of the loss landscape and of the dynamics within it, and (2) to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and datasets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.",
          "venue": "International Conference on Machine Learning",
          "doi": "10.1088/1742-5468/ab3281",
          "url": "https://www.semanticscholar.org/paper/fc998c5e7b0fd1609d5a91d700fec3ec9c72838c",
          "authors": [
            "Marco Baity-Jesi",
            "Levent Sagun",
            "M. Geiger",
            "S. Spigler",
            "G. B. Arous",
            "C. Cammarota",
            "Yann LeCun",
            "M. Wyart",
            "G. Biroli"
          ]
        },
        {
          "title": "Universality in halting time",
          "year": 2017,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/00c44c3355b094dd93cb279349485356092cd07d",
          "authors": [
            "Levent Sagun",
            "T. Trogdon",
            "Yann LeCun"
          ]
        },
        {
          "title": "Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?",
          "year": 2017,
          "citations": 72,
          "abstract": "This article offers an empirical study on the different ways of encoding Chinese, Japanese, Korean (CJK) and English languages for text classification. Different encoding levels are studied, including UTF-8 bytes, characters, words, romanized characters and romanized words. For all encoding levels, whenever applicable, we provide comparisons with linear models, fastText and convolutional networks. For convolutional networks, we compare between encoding mechanisms using character glyph images, one-hot (or one-of-n) encoding, and embedding. In total there are 473 models, using 14 large-scale text classification datasets in 4 languages including Chinese, English, Japanese and Korean. Some conclusions from these results include that byte-level one-hot encoding based on UTF-8 consistently produces competitive results for convolutional networks, that word-level n-grams linear models are competitive even without perfect word segmentation, and that fastText provides the best result using character-level n-gram encoding but can overfit when the features are overly rich.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/07466fb914982f55051cc0b236fd524bdcd8bdc7",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun"
          ]
        },
        {
          "title": "Hierarchical loss for classification",
          "year": 2017,
          "citations": 18,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/15e2bbb28ef53f4b5da6a8c3934c54c1650fb947",
          "authors": [
            "C. Wu",
            "M. Tygert",
            "Yann LeCun"
          ]
        },
        {
          "title": "A hierarchical loss and its problems when classifying non-hierarchically",
          "year": 2017,
          "citations": 31,
          "abstract": "Failing to distinguish between a sheepdog and a skyscraper should be worse and penalized more than failing to distinguish between a sheepdog and a poodle; after all, sheepdogs and poodles are both breeds of dogs. However, existing metrics of failure (so-called “loss” or “win”) used in textual or visual classification/recognition via neural networks seldom leverage a-priori information, such as a sheepdog being more similar to a poodle than to a skyscraper. We define a metric that, inter alia, can penalize failure to distinguish between a sheepdog and a skyscraper more than failure to distinguish between a sheepdog and a poodle. Unlike previously employed possibilities, this metric is based on an ultrametric tree associated with any given tree organization into a semantically meaningful hierarchy of a classifier’s classes. An ultrametric tree is a tree with a so-called ultrametric distance metric such that all leaves are at the same distance from the root. Unfortunately, extensive numerical experiments indicate that the standard practice of training neural networks via stochastic gradient descent with random starting points often drives down the hierarchical loss nearly as much when minimizing the standard cross-entropy loss as when trying to minimize the hierarchical loss directly. Thus, this hierarchical loss is unreliable as an objective for plain, randomly started stochastic gradient descent to minimize; the main value of the hierarchical loss may be merely as a meaningful metric of success of a classifier.",
          "venue": "PLoS ONE",
          "doi": "10.1371/journal.pone.0226222",
          "url": "https://www.semanticscholar.org/paper/35903ec587d50db2d1ae5b26189b0f5c7771edb2",
          "authors": [
            "C. Wu",
            "M. Tygert",
            "Yann LeCun"
          ]
        },
        {
          "title": "Predicting Deeper into the Future of Semantic Segmentation",
          "year": 2017,
          "citations": 246,
          "abstract": "The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.",
          "venue": "IEEE International Conference on Computer Vision",
          "doi": "10.1109/ICCV.2017.77",
          "url": "https://www.semanticscholar.org/paper/516668a41d6106232a7cd56d20d3b3da343e5f36",
          "authors": [
            "Pauline Luc",
            "N. Neverova",
            "C. Couprie",
            "J. Verbeek",
            "Yann LeCun"
          ]
        },
        {
          "title": "Geometric Deep Learning: Going beyond Euclidean data",
          "year": 2017,
          "citations": 25,
          "abstract": null,
          "venue": "IEEE Signal Processing Magazine",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7c60897979654a5838638df7a8aa80142a2ff4bd",
          "authors": [
            "M. Bronstein",
            "Joan Bruna",
            "Yann LeCun",
            "Arthur Szlam",
            "P. Vandergheynst"
          ]
        },
        {
          "title": "Proceedings of the International Computer Music Conference 2016 A Fluid Chord Voicing Generator",
          "year": 2017,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8895fae88182fc5a5623a11c7916b026244f7093",
          "authors": [
            "D. Marcarian",
            "P. Savvidou",
            "B. Willis",
            "M. Skubic",
            "Yann LeCun",
            "K. Perlin",
            "J. Tilmanne",
            "M. Hashimoto",
            "Q. Sun",
            "J. Luo",
            "Y. He"
          ]
        },
        {
          "title": "Adversarially Regularized Autoencoders for Generating Discrete Structures",
          "year": 2017,
          "citations": 79,
          "abstract": "Generative adversarial networks are an effective approach for learning rich latent representations of continuous data, but have proven difficult to apply directly to discrete structured data, such as text sequences or discretized images. Ideally we could encode discrete structures in a continuous code space to avoid this problem, but it is difficult to learn an appropriate general-purpose encoder. In this work, we consider a simple approach for handling these two challenges jointly, employing a discrete structure autoencoder with a code space regularized by generative adversarial training. The model learns a smooth regularized code space while still being able to model the underlying data, and can be used as a discrete GAN with the ability to generate coherent discrete outputs from continuous samples. We demonstrate empirically how key properties of the data are captured in the model's latent space, and evaluate the model itself on the tasks of discrete image generation, text generation, and semi-supervised learning.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/89b139b8b507bdfb8f57b9cc0f09bfcf859bc966",
          "authors": [
            "J. Zhao",
            "Yoon Kim",
            "Kelly W. Zhang",
            "Alexander M. Rush",
            "Yann LeCun"
          ]
        },
        {
          "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition",
          "year": 2017,
          "citations": 3313,
          "abstract": "In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block \"R(2+1)D\" which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.",
          "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2018.00675",
          "url": "https://www.semanticscholar.org/paper/89c3050522a0bb9820c32dc7444e003ef0d3e2e4",
          "authors": [
            "Du Tran",
            "Heng Wang",
            "L. Torresani",
            "Jamie Ray",
            "Yann LeCun",
            "Manohar Paluri"
          ]
        },
        {
          "title": "Prediction Under Uncertainty with Error-Encoding Networks",
          "year": 2017,
          "citations": 25,
          "abstract": "In this work we introduce a new framework for performing temporal predictions in the presence of uncertainty. It is based on a simple idea of disentangling com- ponents of the future state which are predictable from those which are inherently unpredictable, and encoding the unpredictable components into a low-dimensional latent variable which is fed into the forward model. Our method uses a simple su- pervised training objective which is fast and easy to train. We evaluate it in the context of video prediction on multiple datasets and show that it is able to consi- tently generate diverse predictions without the need for alternating minimization over a latent space or adversarial training.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/bfdb693df3a04fa9645233c444ccd8ec16c6c477",
          "authors": [
            "Mikael Henaff",
            "J. Zhao",
            "Yann LeCun"
          ]
        },
        {
          "title": "Model-Based Planning with Discrete and Continuous Actions",
          "year": 2017,
          "citations": 49,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c6d78e818b0585144578d80b4d42585cd616709b",
          "authors": [
            "Mikael Henaff",
            "William F. Whitney",
            "Yann LeCun"
          ]
        },
        {
          "title": "Adversarially Regularized Autoencoders",
          "year": 2017,
          "citations": 293,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d7f55400fd032d182d465eee91581d5ab845a95d",
          "authors": [
            "J. Zhao",
            "Yoon Kim",
            "Kelly W. Zhang",
            "Alexander M. Rush",
            "Yann LeCun"
          ]
        },
        {
          "title": "Model-Based Planning in Discrete Action Spaces",
          "year": 2017,
          "citations": 17,
          "abstract": "Planning actions using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over model-free RL methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete, which may have limited its adoption. In this work, we introduce two discrete planning tasks inspired by existing question-answering datasets and show that it is in fact possible to effectively perform planning via backprop in discrete action spaces using two simple yet principled modifications. Our experiments show that this approach can significantly outperform model-free RL based methods and supervised imitation learners.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d86d17f6459978084320d7d313f38f234cf8b899",
          "authors": [
            "Mikael Henaff",
            "William F. Whitney",
            "Yann LeCun"
          ]
        },
        {
          "title": "Predicting Deeper into the Future of Semantic Segmentation — Supplementary Material —",
          "year": 2017,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/de2c8419d8e5e1501f8daa17e10ee46c27adc5e4",
          "authors": [
            "Pauline Luc",
            "N. Neverova",
            "C. Couprie",
            "Jakob Verbeek",
            "Yann LeCun"
          ]
        },
        {
          "title": "+ YY ’ Z X-Residual Prediction Error TargetPredictionObservation Latent 0 State φ f 1 f 2",
          "year": 2017,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/f85ea632dc79e779a6a1b3170013288f55af0d52",
          "authors": [
            "Mikael Henaff",
            "J. Zhao",
            "Yann LeCun"
          ]
        },
        {
          "title": "Tracking the World State with Recurrent Entity Networks",
          "year": 2016,
          "citations": 233,
          "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/033dd6cf61a6017e9aa9b46068d3c89082849cf3",
          "authors": [
            "Mikael Henaff",
            "J. Weston",
            "Arthur Szlam",
            "Antoine Bordes",
            "Yann LeCun"
          ]
        },
        {
          "title": "Geometric Deep Learning: Going beyond Euclidean data",
          "year": 2016,
          "citations": 3562,
          "abstract": "Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",
          "venue": "IEEE Signal Processing Magazine",
          "doi": "10.1109/MSP.2017.2693418",
          "url": "https://www.semanticscholar.org/paper/0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
          "authors": [
            "M. Bronstein",
            "Joan Bruna",
            "Yann LeCun",
            "Arthur Szlam",
            "P. Vandergheynst"
          ]
        },
        {
          "title": "Phase 4: DCL System Using Deep Learning Approaches for Land-Based or Ship-Based Real-Time Recognition and Localization of Marine Mammals - Distributed Processing and Big Data Applications",
          "year": 2016,
          "citations": 1,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/12a1fbbb2a33bbf9fc4bf7a24c5934fb1b9b7c6f",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ]
        },
        {
          "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs",
          "year": 2016,
          "citations": 184,
          "abstract": "Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely O(1) per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/1d782819afafe0d391e5b67151cb510e621f243d",
          "authors": [
            "Li Jing",
            "Yichen Shen",
            "T. Dubček",
            "J. Peurifoy",
            "S. Skirlo",
            "Yann LeCun",
            "Max Tegmark",
            "M. Soljačić"
          ]
        },
        {
          "title": "Energy-based Generative Adversarial Network",
          "year": 2016,
          "citations": 1131,
          "abstract": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2ba23d9b46027e47b4483243871760e315213ffe",
          "authors": [
            "J. Zhao",
            "Michaël Mathieu",
            "Yann LeCun"
          ]
        },
        {
          "title": "Fast Incremental Learning for Off-Road Robot Navigation",
          "year": 2016,
          "citations": 7,
          "abstract": "A promising approach to autonomous driving is machine learning. In such systems, training datasets are created that capture the sensory input to a vehicle as well as the desired response. A disadvantage of using a learned navigation system is that the learning process itself may require a huge number of training examples and a large amount of computing. To avoid the need to collect a large training set of driving examples, we describe a system that takes advantage of the huge number of training examples provided by ImageNet, but is able to adapt quickly using a small training set for the specific driving environment.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3f1540459ef264051f15ac15cadcc4d924dc79a8",
          "authors": [
            "Artem Provodin",
            "L. Torabi",
            "B. Flepp",
            "Yann LeCun",
            "Michael Sergio",
            "L. Jackel",
            "Urs Muller",
            "Jure Zbontar"
          ]
        },
        {
          "title": "Disentangling factors of variation in deep representation using adversarial training",
          "year": 2016,
          "citations": 509,
          "abstract": "We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentaglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/48a789a15e3572749905f036fcbf3310aee29b79",
          "authors": [
            "Michaël Mathieu",
            "J. Zhao",
            "P. Sprechmann",
            "A. Ramesh",
            "Yann LeCun"
          ]
        },
        {
          "title": "What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures?",
          "year": 2016,
          "citations": 1,
          "abstract": "(This paper was written in November 2011 and never published. It is posted on arXiv.org in its original form in June 2016). Many recent object recognition systems have proposed using a two phase training procedure to learn sparse convolutional feature hierarchies: unsupervised pre-training followed by supervised fine-tuning. Recent results suggest that these methods provide little improvement over purely supervised systems when the appropriate nonlinearities are included. This paper presents an empirical exploration of the space of learning procedures for sparse convolutional networks to assess which method produces the best performance. In our study, we introduce an augmentation of the Predictive Sparse Decomposition method that includes a discriminative term (DPSD). We also introduce a new single phase supervised learning procedure that places an L1 penalty on the output state of each layer of the network. This forces the network to produce sparse codes without the expensive pre-training phase. Using DPSD with a new, complex predictor that incorporates lateral inhibition, combined with multi-scale feature pooling, and supervised refinement, the system achieves a 70.6\\% recognition rate on Caltech-101. With the addition of convolutional training, a 77\\% recognition was obtained on the CIfAR-10 dataset.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/4b5a5d55c37ca2b5e0bfd9bc366a597f749dfc78",
          "authors": [
            "Kevin Jarrett",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun"
          ]
        },
        {
          "title": "Orthogonal RNNs and Long-Memory Tasks",
          "year": 2016,
          "citations": 43,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/4ca151307e3be93c6cd14ed403f6162892e7fbed",
          "authors": [
            "Mikael Henaff",
            "Arthur Szlam",
            "Yann LeCun"
          ]
        },
        {
          "title": "Singularity of the Hessian in Deep Learning",
          "year": 2016,
          "citations": 59,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5fbc36b30859f5f2f1af4f724531c94b1b1f926a",
          "authors": [
            "Levent Sagun",
            "L. Bottou",
            "Yann LeCun"
          ]
        },
        {
          "title": "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond",
          "year": 2016,
          "citations": 252,
          "abstract": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges that depend on the input data.",
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8151fbde700614ab25d6165b9ce5f76456c180d4",
          "authors": [
            "Levent Sagun",
            "L. Bottou",
            "Yann LeCun"
          ]
        },
        {
          "title": "Very Deep Convolutional Networks for Natural Language Processing",
          "year": 2016,
          "citations": 323,
          "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/84ca430856a92000e90cd728445ca2241c10ddc3",
          "authors": [
            "Alexis Conneau",
            "Holger Schwenk",
            "Loïc Barrault",
            "Yann LeCun"
          ]
        },
        {
          "title": "Simultaneous Learning of Trees and Representations for Extreme Classification, with Application to Language Modeling",
          "year": 2016,
          "citations": 9,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/a0049f4e6ae42da87a6edd276618abec5a2e44b0",
          "authors": [
            "Yacine Jernite",
            "A. Choromańska",
            "D. Sontag",
            "Yann LeCun"
          ]
        },
        {
          "title": "Entropy-SGD: biasing gradient descent into wide valleys",
          "year": 2016,
          "citations": 825,
          "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.",
          "venue": "International Conference on Learning Representations",
          "doi": "10.1088/1742-5468/ab39d9",
          "url": "https://www.semanticscholar.org/paper/b6583fe9c9dc52bb129aff4cefc60519349f3b4c",
          "authors": [
            "P. Chaudhari",
            "A. Choromańska",
            "Stefano Soatto",
            "Yann LeCun",
            "Carlo Baldassi",
            "C. Borgs",
            "J. Chayes",
            "Levent Sagun",
            "R. Zecchina"
          ]
        },
        {
          "title": "Phase 1: DCL System Research Using Advanced Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - HPC System Implementation",
          "year": 2016,
          "citations": 1,
          "abstract": "We aim to investigate advancing the state of the art of detection, classification and localization (DCL) in the field of bioacoustics. The two primary goals are to develop transferable technologies for detection and classification in: (1) the area of advanced algorithms, such as deep learning and other methods; and (2) advanced systems, capable of real-time and archival and processing. This project will focus on long-term, continuous datasets to provide automatic recognition, minimizing human time to annotate the signals. Effort will begin by focusing on several years of multi-channel acoustic data collected in the Stellwagen Bank National Marine Sanctuary (SBNMS) between 2006 and 2010. Our efforts will incorporate existing technologies in the bioacoustics signal processing community, advanced high performance computing (HPC) systems, and new approaches aimed at automatically detecting-classifying and measuring features for species-specific marine mammal sounds within passive acoustic data.",
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ba19e986dffe20dca60b502aeb097018d0af0f53",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ]
        },
        {
          "title": "Phase 2: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Machine Learning Detection Algorithms",
          "year": 2016,
          "citations": 2,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c05d6dc329a0c7a52794badaeb6d86df38a27b8e",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ]
        },
        {
          "title": "Phase 3: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Bioacoustic Applicaitons",
          "year": 2016,
          "citations": 0,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e1240e95706d52160a7c51c4f3b1fed4e42658a2",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ]
        },
        {
          "title": "Recurrent Orthogonal Networks and Long-Memory Tasks",
          "year": 2016,
          "citations": 137,
          "abstract": "Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ef3152106e7f4d05ad8d32a5b90d3790c5cdef24",
          "authors": [
            "Mikael Henaff",
            "Arthur Szlam",
            "Yann LeCun"
          ]
        },
        {
          "title": "Very Deep Convolutional Networks for Text Classification",
          "year": 2016,
          "citations": 999,
          "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.",
          "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
          "doi": "10.18653/V1/E17-1104",
          "url": "https://www.semanticscholar.org/paper/f797fd44b9ddd5845611eb7a705ca9464a8819d1",
          "authors": [
            "Holger Schwenk",
            "Loïc Barrault",
            "Alexis Conneau",
            "Yann LeCun"
          ]
        },
        {
          "title": "Guest Editorial: Deep Learning",
          "year": 2015,
          "citations": 35,
          "abstract": null,
          "venue": "International Journal of Computer Vision",
          "doi": "10.1007/s11263-015-0813-1",
          "url": "https://www.semanticscholar.org/paper/08ee53dac878ee6ab1e5cf06824713bed614e17c",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Geoffrey E. Hinton",
            "Yann LeCun"
          ]
        },
        {
          "title": "Universality in halting time and its applications in optimization",
          "year": 2015,
          "citations": 5,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0bc66655d3c9ad7ab4f44e19b6777add3d83643c",
          "authors": [
            "Levent Sagun",
            "T. Trogdon",
            "Yann LeCun"
          ]
        },
        {
          "title": "Complex-valued convolutional networks yield data-driven multiscale windowed spectra",
          "year": 2015,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/13e03191c8d83903661de9fba9b2309d990e2c84",
          "authors": [
            "Joan Bruna",
            "Soumith Chintala",
            "Yann LeCun",
            "Serkan Piantino",
            "Arthur Szlam",
            "M. Tygert"
          ]
        },
        {
          "title": "Binary embeddings with structured hashed projections",
          "year": 2015,
          "citations": 33,
          "abstract": "We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudorandom projection is described by a matrix, where not all entries are independent random variables but instead a fixed \"budget of randomness\" is distributed across the matrix. Such matrices can be efficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/16a26289d7c37e6a0179fa57b14a327286696d33",
          "authors": [
            "A. Choromańska",
            "K. Choromanski",
            "Mariusz Bojarski",
            "Tony Jebara",
            "Sanjiv Kumar",
            "Yann LeCun"
          ]
        },
        {
          "title": "Deep multi-scale video prediction beyond mean square error",
          "year": 2015,
          "citations": 1939,
          "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681",
          "authors": [
            "Michaël Mathieu",
            "C. Couprie",
            "Yann LeCun"
          ]
        },
        {
          "title": "Deep Learning",
          "year": 2015,
          "citations": 39366,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb",
          "authors": [
            "Yann LeCun",
            "Yoshua Bengio",
            "Geoffrey E. Hinton"
          ]
        },
        {
          "title": "A theoretical argument for complex-valued convolutional networks",
          "year": 2015,
          "citations": 10,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2eff9844bddfcabe7b3f16c07fe5dad20ccedd53",
          "authors": [
            "Joan Bruna",
            "Soumith Chintala",
            "Yann LeCun",
            "Serkan Piantino",
            "Arthur Szlam",
            "M. Tygert"
          ]
        },
        {
          "title": "Universal halting times in optimization and machine learning",
          "year": 2015,
          "citations": 9,
          "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that, after centering and scaling, remains unchanged even when the distribution on the landscape is changed. We observe two qualitative classes: A Gumbel-like distribution that appears in Google searches, human decision times, the QR eigenvalue algorithm and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.",
          "venue": "",
          "doi": "10.1090/qam/1483",
          "url": "https://www.semanticscholar.org/paper/30dd56bcb013893330d48829f5c0d7c02ee2da64",
          "authors": [
            "Levent Sagun",
            "T. Trogdon",
            "Yann LeCun"
          ]
        },
        {
          "title": "Unsupervised Feature Learning from Temporal Data",
          "year": 2015,
          "citations": 35,
          "abstract": "Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/394cbf2d589eadcfbdbdaaed65c77532b9c856af",
          "authors": [
            "Ross Goroshin",
            "Joan Bruna",
            "Jonathan Tompson",
            "D. Eigen",
            "Yann LeCun"
          ]
        },
        {
          "title": "Character-level Convolutional Networks for Text Classification",
          "year": 2015,
          "citations": 6578,
          "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/51a55df1f023571a7e07e338ee45a3e3d66ef73e",
          "authors": [
            "Xiang Zhang",
            "J. Zhao",
            "Yann LeCun"
          ]
        },
        {
          "title": "High Performance Computer Acoustic Data Accelerator: A New System for Exploring Marine Mammal Acoustics for Big Data Applications",
          "year": 2015,
          "citations": 15,
          "abstract": "This paper presents a new software model designed for distributed sonic signal detection runtime using machine learning algorithms called DeLMA. A new algorithm--Acoustic Data-mining Accelerator (ADA)--is also presented. ADA is a robust yet scalable solution for efficiently processing big sound archives using distributing computing technologies. Together, DeLMA and the ADA algorithm provide a powerful tool currently being used by the Bioacoustics Research Program (BRP) at the Cornell Lab of Ornithology, Cornell University. This paper provides a high level technical overview of the system, and discusses various aspects of the design. Basic runtime performance and project summary are presented. The DeLMA-ADA baseline performance comparing desktop serial configuration to a 64 core distributed HPC system shows as much as a 44 times faster increase in runtime execution. Performance tests using 48 cores on the HPC shows a 9x to 12x efficiency over a 4 core desktop solution. Project summary results for 19 east coast deployments show that the DeLMA-ADA solution has processed over three million channel hours of sound to date.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6df3dc585e32f3b1cb49228d94a5469c30d79d2b",
          "authors": [
            "Peter J. Dugan",
            "J. Zollweg",
            "Marian Popescu",
            "D. Risch",
            "H. Glotin",
            "Yann LeCun",
            "C. Clark"
          ]
        },
        {
          "title": "Very deep multilingual convolutional neural networks for LVCSR",
          "year": 2015,
          "citations": 225,
          "abstract": "Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3×3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6% relative) over the best published CNN result so far.",
          "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
          "doi": "10.1109/ICASSP.2016.7472620",
          "url": "https://www.semanticscholar.org/paper/77b839147551e024a26b07d896677b38d2b327b7",
          "authors": [
            "Tom Sercu",
            "Christian Puhrsch",
            "Brian Kingsbury",
            "Yann LeCun"
          ]
        },
        {
          "title": "Source separation with scattering Non-Negative Matrix Factorization",
          "year": 2015,
          "citations": 14,
          "abstract": null,
          "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
          "doi": "10.1109/ICASSP.2015.7178296",
          "url": "https://www.semanticscholar.org/paper/810c3027863e021041074a23af5deb6307a0f1ca",
          "authors": [
            "Joan Bruna",
            "P. Sprechmann",
            "Yann LeCun"
          ]
        },
        {
          "title": "Universum Prescription: Regularization Using Unlabeled Data",
          "year": 2015,
          "citations": 31,
          "abstract": "\n \n This paper shows that simply prescribing \"none of the above\" labels to unlabeled data has a beneficial regularization effect to supervised learning. We call it universum prescription by the fact that the prescribed labels cannot be one of the supervised labels. In spite of its simplicity, universum prescription obtained competitive results in training deep convolutional networks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitative justification of these approaches using Rademacher complexity is presented. The effect of a regularization parameter — probability of sampling from unlabeled data — is also studied empirically.\n \n",
          "venue": "AAAI Conference on Artificial Intelligence",
          "doi": "10.1609/aaai.v31i1.10768",
          "url": "https://www.semanticscholar.org/paper/a9b4fc31e6c0253a8924d6fcd19c70c4ac6f3db2",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun"
          ]
        },
        {
          "title": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches",
          "year": 2015,
          "citations": 1428,
          "abstract": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.",
          "venue": "Journal of machine learning research",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ad3e7515c61ccdc2c36887e7d4929c78904881db",
          "authors": [
            "Jure Zbontar",
            "Yann LeCun"
          ]
        },
        {
          "title": "Stacked What-Where Auto-encoders",
          "year": 2015,
          "citations": 262,
          "abstract": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/cba5fbd40767a27d20e346a108b8867ac8591a27",
          "authors": [
            "J. Zhao",
            "Michaël Mathieu",
            "Ross Goroshin",
            "Yann LeCun"
          ]
        },
        {
          "title": "Super-Resolution with Deep Convolutional Sufficient Statistics",
          "year": 2015,
          "citations": 337,
          "abstract": "Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep convolutional neural networks. The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed approach in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/cd108ed4f69b754cf0a5f3eb74d6c1949ea6674d",
          "authors": [
            "Joan Bruna",
            "P. Sprechmann",
            "Yann LeCun"
          ]
        },
        {
          "title": "Open Problem: The landscape of the loss surfaces of multilayer networks",
          "year": 2015,
          "citations": 127,
          "abstract": null,
          "venue": "Annual Conference Computational Learning Theory",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d6739de41e880e3db525af91b9d19b2986aa0655",
          "authors": [
            "A. Choromańska",
            "Yann LeCun",
            "G. B. Arous"
          ]
        },
        {
          "title": "Learning to Linearize Under Uncertainty",
          "year": 2015,
          "citations": 144,
          "abstract": "Training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/dd2fa69647160bb2ea25dcc7b2f6409b01e40222",
          "authors": [
            "Ross Goroshin",
            "Michaël Mathieu",
            "Yann LeCun"
          ]
        },
        {
          "title": "Deep Convolutional Networks on Graph-Structured Data",
          "year": 2015,
          "citations": 1632,
          "abstract": "Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. \nIn this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e49ff72d420c8d72e62a9353e3abc053445e59bd",
          "authors": [
            "Mikael Henaff",
            "Joan Bruna",
            "Yann LeCun"
          ]
        },
        {
          "title": "Text Understanding from Scratch",
          "year": 2015,
          "citations": 564,
          "abstract": "This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/f1b0df6b2977d28e55df82576b108e4f5d87e044",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun"
          ]
        },
        {
          "title": "A Mathematical Motivation for Complex-Valued Convolutional Networks",
          "year": 2015,
          "citations": 107,
          "abstract": "Abstract A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors, followed by (2) taking the absolute value of every entry of the resulting vectors, followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as data-driven multiscale windowed power spectra, data-driven multiscale windowed absolute spectra, data-driven multiwavelet absolute values, or (in their most general configuration) data-driven nonlinear multiwavelet packets. Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs), sigmoidal (e.g., logistic or tanh) nonlinearities, or max pooling, for example, do not obviously exhibit the same exact correspondence with data-driven wavelets (whereas for complex-valued convnets, the correspondence is much more than just a vague analogy). Courtesy of the exact correspondence, the remarkably rich and rigorous body of mathematical analysis for wavelets applies directly to (complex-valued) convnets.",
          "venue": "Neural Computation",
          "doi": "10.1162/NECO_a_00824",
          "url": "https://www.semanticscholar.org/paper/f55fe2b4344f015927a834d8ad6f52a35c3a8c8e",
          "authors": [
            "M. Tygert",
            "Joan Bruna",
            "Soumith Chintala",
            "Yann LeCun",
            "Serkan Piantino",
            "Arthur Szlam"
          ]
        },
        {
          "title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation",
          "year": 2014,
          "citations": 1582,
          "abstract": "This paper proposes a new hybrid architecture that consists of a deep Convolu-tional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/12ecc2d786080f638a01b9999518e9386baa157d",
          "authors": [
            "Jonathan Tompson",
            "Arjun Jain",
            "Yann LeCun",
            "C. Bregler"
          ]
        },
        {
          "title": "Computing the stereo matching cost with a convolutional neural network",
          "year": 2014,
          "citations": 796,
          "abstract": "We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61% on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset.",
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2015.7298767",
          "url": "https://www.semanticscholar.org/paper/2fdd82708a99cec2bcd45c2c7f337a9beccced04",
          "authors": [
            "Jure Zbontar",
            "Yann LeCun"
          ]
        },
        {
          "title": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation",
          "year": 2014,
          "citations": 354,
          "abstract": "We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/326d65827307862ddc3d39b84ebc662e83ff95b3",
          "authors": [
            "Nicolas Vasilache",
            "Jeff Johnson",
            "Michaël Mathieu",
            "Soumith Chintala",
            "Serkan Piantino",
            "Yann LeCun"
          ]
        },
        {
          "title": "Explorations on high dimensional landscapes",
          "year": 2014,
          "citations": 65,
          "abstract": "Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. We provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/540393e544757f103efd549ac5196de117950f94",
          "authors": [
            "Levent Sagun",
            "V. U. Güney",
            "Yann LeCun"
          ]
        },
        {
          "title": "The bottlenecks in human letter recognition: a computational model",
          "year": 2014,
          "citations": 5,
          "abstract": null,
          "venue": "",
          "doi": "10.7490/F1000RESEARCH.1095738.1",
          "url": "https://www.semanticscholar.org/paper/553ad7d282e2c2bbad301c119e9aaaa9e8fa369a",
          "authors": [
            "Avi Ziskind",
            "Olivier J. Hénaff",
            "Yann LeCun",
            "D. Pelli"
          ]
        },
        {
          "title": "The Loss Surface of Multilayer Networks",
          "year": 2014,
          "citations": 146,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6d9cb3d3c0330a6c2f42d159a3a706b6b49744b2",
          "authors": [
            "A. Choromańska",
            "Mikael Henaff",
            "Michaël Mathieu",
            "G. B. Arous",
            "Yann LeCun"
          ]
        },
        {
          "title": "Toward real-time indoor semantic segmentation using depth information",
          "year": 2014,
          "citations": 17,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/84e611d7b24323fda931a0e10aee59d8824e58bf",
          "authors": [
            "C. Couprie",
            "C. Farabet",
            "Laurent Najman",
            "Yann LeCun"
          ]
        },
        {
          "title": "Convolutional nets and watershed cuts for real-time semantic Labeling of RGBD videos",
          "year": 2014,
          "citations": 36,
          "abstract": null,
          "venue": "Journal of machine learning research",
          "doi": "10.5555/2627435.2697077",
          "url": "https://www.semanticscholar.org/paper/8934e44350ca741f7623bf26f89d43835ece0113",
          "authors": [
            "C. Couprie",
            "C. Farabet",
            "Laurent Najman",
            "Yann LeCun"
          ]
        },
        {
          "title": "Differentially- and non-differentially-private random decision trees",
          "year": 2014,
          "citations": 31,
          "abstract": "We consider supervised learning with random decision trees, where the tree construction is completely random. The method is popularly used and works well in practice despite the simplicity of the setting, but its statistical mechanism is not yet well-understood. In this paper we provide strong theoretical guarantees regarding learning with random decision trees. We analyze and compare three different variants of the algorithm that have minimal memory requirements: majority voting, threshold averaging and probabilistic averaging. The random structure of the tree enables us to adapt these methods to a differentially-private setting thus we also propose differentially-private versions of all three schemes. We give upper-bounds on the generalization error and mathematically explain how the accuracy depends on the number of random decision trees. Furthermore, we prove that only logarithmic (in the size of the dataset) number of independently selected random decision trees suffice to correctly classify most of the data, even when differential-privacy guarantees must be maintained. We empirically show that majority voting and threshold averaging give the best accuracy, also for conservative users requiring high privacy guarantees. Furthermore, we demonstrate that a simple majority voting rule is an especially good candidate for the differentially-private classifier since it is much less sensitive to the choice of forest parameters than other methods.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/a49c17b31a6d3a526b030927dace6919b6a68603",
          "authors": [
            "Mariusz Bojarski",
            "A. Choromańska",
            "K. Choromanski",
            "Yann LeCun"
          ]
        },
        {
          "title": "Audio Source Separation with Discriminative Scattering Networks",
          "year": 2014,
          "citations": 4,
          "abstract": "Many monaural signal decomposition techniques proposed in the literature operate on a feature space consisting of a time-frequency representation of the input data. A challenge faced by these approaches is to effectively exploit the temporal dependencies of the signals at scales larger than the duration of a time-frame. In this work we propose to tackle this problem by modeling the signals using a time-frequency representation with multiple temporal resolutions. For this reason we use a signal representation that consists of a pyramid of wavelet scattering operators, which generalizes Constant Q Transforms CQT with extra layers of convolution and complex modulus. We first show that learning standard models with this multi-resolution setting improves source separation results over fixed-resolution methods. As study case, we use Non-Negative Matrix Factorizations NMF that has been widely considered in many audio application. Then, we investigate the inclusion of the proposed multi-resolution setting into a discriminative training regime. We discuss several alternatives using different deep neural network architectures, and our preliminary experiments suggest that in this task, finite impulse, multi-resolution Convolutional Networks are a competitive baseline compared to recurrent alternatives.",
          "venue": "Latent Variable Analysis and Signal Separation",
          "doi": "10.1007/978-3-319-22482-4_30",
          "url": "https://www.semanticscholar.org/paper/a7cc2b78afcdbbfa2f8ba41addec2827c33f89c7",
          "authors": [
            "P. Sprechmann",
            "Joan Bruna",
            "Yann LeCun"
          ]
        },
        {
          "title": "The Loss Surfaces of Multilayer Networks",
          "year": 2014,
          "citations": 1247,
          "abstract": "We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between largeand small-size networks where for the latter poor quality local minima have nonzero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.",
          "venue": "International Conference on Artificial Intelligence and Statistics",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ad8a12a19e74d9788f8fe92f5c0dfea7b6a52aba",
          "authors": [
            "A. Choromańska",
            "Mikael Henaff",
            "Michaël Mathieu",
            "G. B. Arous",
            "Yann LeCun"
          ]
        },
        {
          "title": "Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks",
          "year": 2014,
          "citations": 801,
          "abstract": null,
          "venue": "ACM Transactions on Graphics",
          "doi": "10.1145/2629500",
          "url": "https://www.semanticscholar.org/paper/ae639e4bdd2e6a11bc44ff7f1ae53cd25462042b",
          "authors": [
            "Jonathan Tompson",
            "Murphy Stein",
            "Yann LeCun",
            "K. Perlin"
          ]
        },
        {
          "title": "Fast Approximation of Rotations and Hessians matrices",
          "year": 2014,
          "citations": 26,
          "abstract": "A new method to represent and approximate rotation matrices is introduced. The method represents approximations of a rotation matrix $Q$ with linearithmic complexity, i.e. with $\\frac{1}{2}n\\lg(n)$ rotations over pairs of coordinates, arranged in an FFT-like fashion. The approximation is \"learned\" using gradient descent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ is a diagonal matrix. It can be used to approximate covariance matrix of Gaussian models in order to speed up inference, or to estimate and track the inverse Hessian of an objective function by relating changes in parameters to changes in gradient along the trajectory followed by the optimization procedure. Experiments were conducted to approximate synthetic matrices, covariance matrices of real data, and Hessian matrices of objective functions involved in machine learning problems.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/cd7c5cbc246e42ebb1368f81ef403c988a1d4c89",
          "authors": [
            "Michaël Mathieu",
            "Yann LeCun"
          ]
        },
        {
          "title": "Unsupervised Learning of Spatiotemporally Coherent Metrics",
          "year": 2014,
          "citations": 163,
          "abstract": "Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity priors. We establish a connection between slow feature learning and metric learning. Using this connection we define \"temporal coherence\" -- a criterion which can be used to set hyper-parameters in a principled and automated manner. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labels.",
          "venue": "IEEE International Conference on Computer Vision",
          "doi": "10.1109/ICCV.2015.465",
          "url": "https://www.semanticscholar.org/paper/d0a8b5cf98b6721b743571ee13e6032ff5598aea",
          "authors": [
            "Ross Goroshin",
            "Joan Bruna",
            "Jonathan Tompson",
            "D. Eigen",
            "Yann LeCun"
          ]
        },
        {
          "title": "Deep learning with Elastic Averaging SGD",
          "year": 2014,
          "citations": 630,
          "abstract": "We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d1e4365de165463e51134f10bf3939f2b00a6667",
          "authors": [
            "Sixin Zhang",
            "A. Choromańska",
            "Yann LeCun"
          ]
        },
        {
          "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation",
          "year": 2014,
          "citations": 1760,
          "abstract": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2 x, while keeping the accuracy within 1% of the original model.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e5ae8ab688051931b4814f6d32b18391f8d1fa8d",
          "authors": [
            "Emily L. Denton",
            "Wojciech Zaremba",
            "Joan Bruna",
            "Yann LeCun",
            "R. Fergus"
          ]
        },
        {
          "title": "Efficient object localization using Convolutional Networks",
          "year": 2014,
          "citations": 1413,
          "abstract": "Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPII-human-pose dataset [1].",
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2015.7298664",
          "url": "https://www.semanticscholar.org/paper/ebcea2d842d3d4e320500086aff0deb4cb4412ff",
          "authors": [
            "Jonathan Tompson",
            "Ross Goroshin",
            "Arjun Jain",
            "Yann LeCun",
            "C. Bregler"
          ]
        },
        {
          "title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation",
          "year": 2014,
          "citations": 176,
          "abstract": "In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion (This dataset can be downloaded from http://cs.nyu.edu/~ajain/accv2014/.), that extends the FLIC dataset [1] with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.",
          "venue": "Asian Conference on Computer Vision",
          "doi": "10.1007/978-3-319-16808-1_21",
          "url": "https://www.semanticscholar.org/paper/f8f4481e521ff34683df44c0e467d03ab5dee2b0",
          "authors": [
            "Arjun Jain",
            "Jonathan Tompson",
            "Yann LeCun",
            "C. Bregler"
          ]
        },
        {
          "title": "Assignment 2 - Deep Learning with Sparse Coding",
          "year": 2013,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/016c85fb230e3be91ebc7e0d75eb12c9ddc7661c",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun",
            "J. Langford"
          ]
        },
        {
          "title": "Saturating Auto-Encoders",
          "year": 2013,
          "citations": 44,
          "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0a9cbc7484a0da0962b39ca880ee63b398746170",
          "authors": [
            "Rostislav Goroshin",
            "Yann LeCun"
          ]
        },
        {
          "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
          "year": 2013,
          "citations": 5090,
          "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/1109b663453e78a59e4f66446d71720ac58cec25",
          "authors": [
            "P. Sermanet",
            "D. Eigen",
            "Xiang Zhang",
            "Michaël Mathieu",
            "R. Fergus",
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning Hierarchical Features for Scene Labeling",
          "year": 2013,
          "citations": 2757,
          "abstract": null,
          "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
          "doi": "10.1109/TPAMI.2012.231",
          "url": "https://www.semanticscholar.org/paper/237a04dd8291cbdb59b6dc4b53e689af743fe2a3",
          "authors": [
            "C. Farabet",
            "C. Couprie",
            "Laurent Najman",
            "Yann LeCun"
          ]
        },
        {
          "title": "Neural Information Processing (ICONIP 2013)",
          "year": 2013,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/297bcea9cca46cbe3181508972c724dc1c5d3f75",
          "authors": [
            "T. Vatanen",
            "T. Raiko",
            "Harri Valpola",
            "Yann LeCun"
          ]
        },
        {
          "title": "AAAI Workshop - Technical Report: Preface",
          "year": 2013,
          "citations": 0,
          "abstract": null,
          "venue": "AAAI Conference on Artificial Intelligence",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/31dbf596f6f6b472aba9ea3434e2463b20c98511",
          "authors": [
            "Marc Pickett",
            "B. Kuipers",
            "Yann LeCun",
            "Clayton T. Morrison"
          ]
        },
        {
          "title": "Regularization of Neural Networks using DropConnect",
          "year": 2013,
          "citations": 2608,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/38f35dd624cd1cf827416e31ac5e0e0454028eca",
          "authors": [
            "Li Wan",
            "Matthew D. Zeiler",
            "Sixin Zhang",
            "Yann LeCun",
            "R. Fergus"
          ]
        },
        {
          "title": "Signal recovery from Pooling Representations",
          "year": 2013,
          "citations": 101,
          "abstract": "In this work we compute lower Lipschitz bounds of lp pooling operators for p = 1, 2, ∞ as well as lp pooling operators preceded by half-rectification layers. These give sufficient conditions for the design of invertible neural network layers. Numerical experiments on MNIST and image patches confirm that pooling layers can be inverted with phase recovery algorithms. Moreover, the regularity of the inverse pooling, controlled by the lower Lipschitz constant, is empirically verified with a nearest neighbor regression.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/47f1b4ce7674b15d293f5a3c53d6da44bc8e137e",
          "authors": [
            "Joan Bruna",
            "Arthur Szlam",
            "Yann LeCun"
          ]
        },
        {
          "title": "Spectral Networks and Locally Connected Networks on Graphs",
          "year": 2013,
          "citations": 5173,
          "abstract": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5e925a9f1e20df61d1e860a7aa71894b35a1c186",
          "authors": [
            "Joan Bruna",
            "Wojciech Zaremba",
            "Arthur Szlam",
            "Yann LeCun"
          ]
        },
        {
          "title": "Real-time adaptive off-road vehicle navigation and terrain classification",
          "year": 2013,
          "citations": 12,
          "abstract": null,
          "venue": "Defense, Security, and Sensing",
          "doi": "10.1117/12.2015533",
          "url": "https://www.semanticscholar.org/paper/5fd1714d7fc5b3edfcfb755e11f914c46d377cc5",
          "authors": [
            "Urs Muller",
            "L. Jackel",
            "Yann LeCun",
            "B. Flepp"
          ]
        },
        {
          "title": "Discriminative recurrent sparse auto-encoders: 1st International Conference on Learning Representations, ICLR 2013",
          "year": 2013,
          "citations": 2,
          "abstract": null,
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6ef907db1e8c7b0cde28c960267cdb6a7b9a56d1",
          "authors": [
            "J. Rolfe",
            "Yann LeCun"
          ]
        },
        {
          "title": "Discriminative Recurrent Sparse Auto-Encoders",
          "year": 2013,
          "citations": 73,
          "abstract": "We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. \nFrom an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/72c35d7eb807ed46fb82024922ffbf45218f5a95",
          "authors": [
            "J. Rolfe",
            "Yann LeCun"
          ]
        },
        {
          "title": "No More Pesky Learning Rates : Supplementary Material",
          "year": 2013,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7ac497addf456522d02d4426ea70d4fb20c86431",
          "authors": [
            "T. Schaul",
            "Sixin Zhang",
            "Yann LeCun"
          ]
        },
        {
          "title": "Feature learning and deep architectures: new directions for music informatics",
          "year": 2013,
          "citations": 135,
          "abstract": null,
          "venue": "Journal of Intelligence and Information Systems",
          "doi": "10.1007/s10844-013-0248-5",
          "url": "https://www.semanticscholar.org/paper/8b930259e50c9e7ef47d6ad2d57ff377d630361a",
          "authors": [
            "Eric J. Humphrey",
            "J. Bello",
            "Yann LeCun"
          ]
        },
        {
          "title": "Indoor Semantic Segmentation using depth information",
          "year": 2013,
          "citations": 497,
          "abstract": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8f3c7fc72635c3b95dc3530fdaceee1d4681548e",
          "authors": [
            "C. Couprie",
            "C. Farabet",
            "Laurent Najman",
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning Stable Group Invariant Representations with Convolutional Networks",
          "year": 2013,
          "citations": 39,
          "abstract": "Transformation groups, such as translations or rotations, effectively express part of the variability observed in many recognition problems. The group structure enables the construction of invariant signal representations with appealing mathematical properties, where convolutions, together with pooling operators, bring stability to additive and geometric perturbations of the input. Whereas physical transformation groups are ubiquitous in image and audio applications, they do not account for all the variability of complex signal classes. \nWe show that the invariance properties built by deep convolutional networks can be cast as a form of stable group invariance. The network wiring architecture determines the invariance group, while the trainable filter coefficients characterize the group action. We give explanatory examples which illustrate how the network architecture controls the resulting invariance group. We also explore the principle by which additional convolutional layers induce a group factorization enabling more abstract, powerful invariant representations.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/a219e69af081740e0faab08028557965620d8303",
          "authors": [
            "Joan Bruna",
            "Arthur Szlam",
            "Yann LeCun"
          ]
        },
        {
          "title": "Fast Training of Convolutional Networks through FFTs",
          "year": 2013,
          "citations": 628,
          "abstract": "Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/a7621b4ec18719b08f3a2a444b6d37a2e20227b7",
          "authors": [
            "Michaël Mathieu",
            "Mikael Henaff",
            "Yann LeCun"
          ]
        },
        {
          "title": "Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients",
          "year": 2013,
          "citations": 29,
          "abstract": "Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b4b0a3a748dfd2618ffcf1f94411e339e1e78775",
          "authors": [
            "T. Schaul",
            "Yann LeCun"
          ]
        },
        {
          "title": "Saturating auto-encoders: International Conference on Learning Representations, ICLR 2013",
          "year": 2013,
          "citations": 1,
          "abstract": null,
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b6c3a5c2eb0cca13d4944f549a3e29c596170e00",
          "authors": [
            "Rostislav Goroshin",
            "Yann LeCun"
          ]
        },
        {
          "title": "Causal graph-based video segmentation",
          "year": 2013,
          "citations": 26,
          "abstract": "Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. The algorithm may be trivially extended to video segmentation by considering a video as a 3D volume, however, this can not be the case for causal segmentation, when subsequent frames are unknown. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real time applications.",
          "venue": "2013 IEEE International Conference on Image Processing",
          "doi": "10.1109/ICIP.2013.6738875",
          "url": "https://www.semanticscholar.org/paper/c0d373ab0fc50663a2e638f2067dedaa9e2b2c7c",
          "authors": [
            "C. Couprie",
            "C. Farabet",
            "Yann LeCun",
            "Laurent Najman"
          ]
        },
        {
          "title": "Saturating Auto-Encoder",
          "year": 2013,
          "citations": 10,
          "abstract": null,
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/dab50f7682bf410743e2d2447eb5d2bc652f1463",
          "authors": [
            "Ross Goroshin",
            "Yann LeCun"
          ]
        },
        {
          "title": "Understanding Deep Architectures using a Recursive Convolutional Network",
          "year": 2013,
          "citations": 147,
          "abstract": "A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We find that while increasing the numbers of layers and parameters each have clear benefit, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and finds most of its benefit through the introduction of more weights. Our results (i) empirically confirm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e2d894584986b44710f634b696db371f8aff92e0",
          "authors": [
            "D. Eigen",
            "J. Rolfe",
            "R. Fergus",
            "Yann LeCun"
          ]
        },
        {
          "title": "DCL System Using Deep Learning Approaches for Land-Based or Ship-Based Real-Time Recognition and Localization of Marine Mammals",
          "year": 2013,
          "citations": 4,
          "abstract": "Abstract : The overarching goals of this work are to advance the state of the art for detection, classification, and localization (DCL) in the field of bioacoustics. This goal is primarily achieved by building a generic framework for detection-classification (DC) using a fast, efficient, and scalable architecture, demonstrating the capabilities of the system using a variety of low-frequency and mid-frequency cetacean sounds. Two primary goals are to develop transferable technologies for detection and classification in the area of advanced algorithms, such as deep learning and other methods; and in advanced systems, capable of real-time and archival processing. Currently, massive amounts of acoustic data are being collected by various institutions, corporations, and national defense agencies. The long-term goal is to provide technical capability to analyze the data using automatic algorithms for DC based on machine intelligence. The goal of the automation is to provide effective and efficient mechanisms by which to process large acoustic datasets for understanding the bioacoustic behaviors of marine mammals. This capability will provide insights into the potential ecological impacts and influences of anthropogenic ocean sounds. From Oct 2012 through Sep 2013, our research focused on five major initiatives: (1) International workshops, conferences, and data challenges; (2) Enhancements of the Acoustic Segmentation Recognition (ASR) algorithm for frequency-modulated sounds: Right Whale Study; (3) Enhancements of the ASR algorithm for pulse trains: Minke Whale Study; (4) Mining Big Data Sound Archives using High Performance Computing software and hardware; and (5) Large Pulse Train Study: Minke Vocal Activity East Coast United States.",
          "venue": "",
          "doi": "10.21236/ada573473",
          "url": "https://www.semanticscholar.org/paper/e52804a57e0f32be1dc015800a9b58d35639a17d",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ]
        },
        {
          "title": "Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities",
          "year": 2013,
          "citations": 34,
          "abstract": "Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.",
          "venue": "International Conference on Learning Representations",
          "doi": "10.1007/978-3-642-42054-2_55",
          "url": "https://www.semanticscholar.org/paper/f72c079d9179cfaada1135a7e4c77d48b6309a30",
          "authors": [
            "T. Vatanen",
            "T. Raiko",
            "Harri Valpola",
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning Through the Breach: Language Socialization",
          "year": 2012,
          "citations": 3,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/springerreference_301829",
          "url": "https://www.semanticscholar.org/paper/0cf33632c2031f475803ee9d9215578fa0af927a",
          "authors": [
            "C. Chan",
            "Z. Zorina",
            "Jesse R. Sparks",
            "S. L. Ornat",
            "C. Tsang",
            "E. Grigorenko",
            "R. Lubow",
            "J. Malone",
            "M. Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "T. Minor",
            "Traci N. Plumb",
            "H. Drachsler",
            "P. Kirschner",
            "M. Nakayama",
            "Rowena Santiago",
            "Ali Şimşek",
            "Fang‐Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "R. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "L. Lamb",
            "A. Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "T. Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "G. Kern-Isberner",
            "J. Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C. Olsson",
            "L. Nyberg",
            "R. Freivalds",
            "L. Hall",
            "M. Hall",
            "U. Hanke",
            "L. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "P. Burnett",
            "M. Cannon",
            "A. Edmondson",
            "P. Hartono",
            "A. Callender",
            "R. Barr",
            "N. Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohamad Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "D. Kolb",
            "R. Reilly",
            "K. Nielsen",
            "P. Couvillon",
            "H. King",
            "J. Dillon",
            "D. Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "P. C. Lane",
            "S. Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "D. Tucker",
            "P. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "D. Ansari",
            "K. Lionello-DeNolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo P. Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. Hoppe",
            "Krisztina Molnár",
            "E. Vries",
            "E. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "T. Cazenave",
            "T. Jong",
            "J. Meij",
            "J. Gavelek",
            "A. Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "M. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "D. Seo",
            "M. Monfils",
            "A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "M. Kennedy",
            "M. Jackson",
            "T. Nunes",
            "B. Jackling",
            "R. Howard",
            "W. Kennedy",
            "L. C. Drickamer"
          ]
        },
        {
          "title": "Comparison between Frame-Constrained Fix-Pixel-Value and Frame-Free Spiking-Dynamic-Pixel ConvNets for Visual Processing",
          "year": 2012,
          "citations": 74,
          "abstract": "Most scene segmentation and categorization architectures for the extraction of features in images and patches make exhaustive use of 2D convolution operations for template matching, template search, and denoising. Convolutional Neural Networks (ConvNets) are one example of such architectures that can implement general-purpose bio-inspired vision systems. In standard digital computers 2D convolutions are usually expensive in terms of resource consumption and impose severe limitations for efficient real-time applications. Nevertheless, neuro-cortex inspired solutions, like dedicated Frame-Based or Frame-Free Spiking ConvNet Convolution Processors, are advancing real-time visual processing. These two approaches share the neural inspiration, but each of them solves the problem in different ways. Frame-Based ConvNets process frame by frame video information in a very robust and fast way that requires to use and share the available hardware resources (such as: multipliers, adders). Hardware resources are fixed- and time-multiplexed by fetching data in and out. Thus memory bandwidth and size is important for good performance. On the other hand, spike-based convolution processors are a frame-free alternative that is able to perform convolution of a spike-based source of visual information with very low latency, which makes ideal for very high-speed applications. However, hardware resources need to be available all the time and cannot be time-multiplexed. Thus, hardware should be modular, reconfigurable, and expansible. Hardware implementations in both VLSI custom integrated circuits (digital and analog) and FPGA have been already used to demonstrate the performance of these systems. In this paper we present a comparison study of these two neuro-inspired solutions. A brief description of both systems is presented and also discussions about their differences, pros and cons.",
          "venue": "Frontiers in Neuroscience",
          "doi": "10.3389/fnins.2012.00032",
          "url": "https://www.semanticscholar.org/paper/18816aa33e7afd3eabfa7ac6437e90220aa5b317",
          "authors": [
            "C. Farabet",
            "R. Paz",
            "J. Pérez-Carrasco",
            "C. Zamarreño-Ramos",
            "A. Linares-Barranco",
            "Yann LeCun",
            "E. Culurciello",
            "T. Serrano-Gotarredona",
            "B. Linares-Barranco"
          ]
        },
        {
          "title": "Semantic Road Segmentation via Multi-scale Ensembles of Learned Features",
          "year": 2012,
          "citations": 58,
          "abstract": null,
          "venue": "ECCV Workshops",
          "doi": "10.1007/978-3-642-33868-7_58",
          "url": "https://www.semanticscholar.org/paper/2239f2a97d80b63466815e2e2d6ee993d156813e",
          "authors": [
            "J. Álvarez",
            "Yann LeCun",
            "T. Gevers",
            "Antonio M. López"
          ]
        },
        {
          "title": "Learning in Informal Settings",
          "year": 2012,
          "citations": 12,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/springerreference_301826",
          "url": "https://www.semanticscholar.org/paper/27a1c8c5e228cf4d5fd020e347a58bfb00c95d46",
          "authors": [
            "C. Chan",
            "Z. Zorina",
            "Jesse R. Sparks",
            "S. L. Ornat",
            "C. Tsang",
            "E. Grigorenko",
            "R. Lubow",
            "J. Malone",
            "M. Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "T. Minor",
            "Traci N. Plumb",
            "H. Drachsler",
            "P. Kirschner",
            "M. Nakayama",
            "Rowena Santiago",
            "Ali Şimşek",
            "Fang‐Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "R. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "L. Lamb",
            "A. Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "Lynda Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "T. Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "G. Kern-Isberner",
            "J. Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C. Olsson",
            "L. Nyberg",
            "R. Freivalds",
            "L. Hall",
            "M. Hall",
            "U. Hanke",
            "L. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "P. Burnett",
            "M. Cannon",
            "A. Edmondson",
            "P. Hartono",
            "A. Callender",
            "R. Barr",
            "N. Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohamad Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "D. Kolb",
            "R. Reilly",
            "K. Nielsen",
            "P. Couvillon",
            "H. King",
            "J. Dillon",
            "D. Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "P. C. Lane",
            "S. Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "D. Tucker",
            "P. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "D. Ansari",
            "K. Lionello-denolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "Eliza Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo P. Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. Hoppe",
            "Krisztina Molnár",
            "E. Vries",
            "E. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "T. Cazenave",
            "T. Jong",
            "J. Meij",
            "J. Gavelek",
            "A. Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "M. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "D. Seo",
            "M. Monfils",
            "A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "M. Kennedy",
            "M. Jackson",
            "T. Nunes",
            "B. Jackling",
            "R. Howard",
            "W. Kennedy",
            "L. C. Drickamer"
          ]
        },
        {
          "title": "Learning from Text",
          "year": 2012,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/978-1-4419-1428-6_894",
          "url": "https://www.semanticscholar.org/paper/4b680f31d78db262fe82613e62730f6ec1356eb3",
          "authors": [
            "C. Chan",
            "Z. Zorina",
            "Jesse R. Sparks",
            "S. L. Ornat",
            "C. Tsang",
            "E. Grigorenko",
            "R. Lubow",
            "J. Malone",
            "M. Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "T. Minor",
            "Traci N. Plumb",
            "H. Drachsler",
            "P. Kirschner",
            "M. Nakayama",
            "Rowena Santiago",
            "Ali Şimşek",
            "Fang‐Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "R. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "L. Lamb",
            "A. Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "T. Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "G. Kern-Isberner",
            "J. Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C. Olsson",
            "L. Nyberg",
            "R. Freivalds",
            "L. Hall",
            "M. Hall",
            "U. Hanke",
            "L. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "P. Burnett",
            "M. Cannon",
            "A. Edmondson",
            "P. Hartono",
            "A. Callender",
            "R. Barr",
            "N. Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohamad Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "D. Kolb",
            "R. Reilly",
            "K. Nielsen",
            "P. Couvillon",
            "H. King",
            "J. Dillon",
            "D. Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "P. C. Lane",
            "S. Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "D. Tucker",
            "P. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "D. Ansari",
            "K. Lionello-DeNolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo P. Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. Hoppe",
            "Krisztina Molnár",
            "E. Vries",
            "E. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "T. Cazenave",
            "T. Jong",
            "J. Meij",
            "J. Gavelek",
            "A. Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "M. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "D. Seo",
            "M. Monfils",
            "A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "M. Kennedy",
            "M. Jackson",
            "T. Nunes",
            "B. Jackling",
            "R. Howard",
            "W. Kennedy",
            "L. C. Drickamer"
          ]
        },
        {
          "title": "Scene parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers",
          "year": 2012,
          "citations": 205,
          "abstract": "Scene parsing consists in labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features. In parallel to feature extraction, a tree of segments is computed from a graph of pixel dissimilarities. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average \"purity\" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The system yields record accuracies on the the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) and near-record accuracy on the Stanford Background Dataset (8 classes), while being an order of magnitude faster than competing approaches, producing a 320 × 240 image labeling in less than 1 second, including feature extraction.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/56b3ff898cadde865d20ddb4e7a33434de186794",
          "authors": [
            "C. Farabet",
            "C. Couprie",
            "Laurent Najman",
            "Yann LeCun"
          ]
        },
        {
          "title": "Road Scene Segmentation from a Single Image",
          "year": 2012,
          "citations": 254,
          "abstract": null,
          "venue": "European Conference on Computer Vision",
          "doi": "10.1007/978-3-642-33786-4_28",
          "url": "https://www.semanticscholar.org/paper/62cf68c67802b6e8126ecfb3fb794b867fd2ab3a",
          "authors": [
            "J. Álvarez",
            "T. Gevers",
            "Yann LeCun",
            "Antonio M. López"
          ]
        },
        {
          "title": "Moving Beyond Feature Design: Deep Architectures and Automatic Feature Learning in Music Informatics",
          "year": 2012,
          "citations": 153,
          "abstract": null,
          "venue": "International Society for Music Information Retrieval Conference",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7a42dedd7050442a78940bae6a62899919693b17",
          "authors": [
            "Eric J. Humphrey",
            "J. Bello",
            "Yann LeCun"
          ]
        },
        {
          "title": "Flexible-Cost SLAM",
          "year": 2012,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8a96d99e4fe1bfe471889ccc2e816522b967d88f",
          "authors": [
            "Yann LeCun",
            "M. Grimes"
          ]
        },
        {
          "title": "AISTAT AISTATS Fifteenth International Conference on Artificial Intelligence and Statistics (AISTAT 2012) AISTAT AISTATS, La Palma, Canary Islands, April 21-23, 2012",
          "year": 2012,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8f83cbb3f7594c7227d9e318437d700945473df7",
          "authors": [
            "T. Raiko",
            "Harri Valpola",
            "Yann LeCun"
          ]
        },
        {
          "title": "DCL System Research Using Advanced Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals",
          "year": 2012,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": "10.21236/ada572279",
          "url": "https://www.semanticscholar.org/paper/8fea0cbdf12afec45db609219a56016ecc571147",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ]
        },
        {
          "title": "NeuFlow: Dataflow vision processing system-on-a-chip",
          "year": 2012,
          "citations": 91,
          "abstract": null,
          "venue": "Midwest Symposium on Circuits and Systems",
          "doi": "10.1109/MWSCAS.2012.6292202",
          "url": "https://www.semanticscholar.org/paper/90ddfda9c97e96fbba0b5a314853e5867d813a2b",
          "authors": [
            "Phi-Hung Pham",
            "D. Jelača",
            "C. Farabet",
            "B. Martini",
            "Yann LeCun",
            "E. Culurciello"
          ]
        },
        {
          "title": "Learning and Development After School",
          "year": 2012,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/springerreference_301883",
          "url": "https://www.semanticscholar.org/paper/9f2f315d994a95e04c695c392ac3d80c0345b880",
          "authors": [
            "C. Chan",
            "Z. Zorina",
            "Jesse R. Sparks",
            "S. L. Ornat",
            "C. Tsang",
            "E. Grigorenko",
            "R. Lubow",
            "J. Malone",
            "M. Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "T. Minor",
            "Traci N. Plumb",
            "H. Drachsler",
            "P. Kirschner",
            "M. Nakayama",
            "Rowena Santiago",
            "Ali Şimşek",
            "Fang‐Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "R. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "L. Lamb",
            "A. Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "Lynda Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "T. Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "G. Kern-Isberner",
            "J. Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C. Olsson",
            "L. Nyberg",
            "R. Freivalds",
            "L. Hall",
            "M. Hall",
            "U. Hanke",
            "L. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "P. Burnett",
            "M. Cannon",
            "A. Edmondson",
            "P. Hartono",
            "A. Callender",
            "R. Barr",
            "N. Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohamad Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "D. Kolb",
            "R. Reilly",
            "K. Nielsen",
            "P. Couvillon",
            "H. King",
            "J. Dillon",
            "D. Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "P. C. Lane",
            "S. Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "D. Tucker",
            "P. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "D. Ansari",
            "K. Lionello-denolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "Eliza Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo P. Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. Hoppe",
            "Krisztina Molnár",
            "E. Vries",
            "E. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "T. Cazenave",
            "T. Jong",
            "J. Meij",
            "J. Gavelek",
            "A. Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "M. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "D. Seo",
            "M. Monfils",
            "A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "M. Kennedy",
            "M. Jackson",
            "T. Nunes",
            "B. Jackling",
            "R. Howard",
            "W. Kennedy",
            "L. C. Drickamer"
          ]
        },
        {
          "title": "Convolutional neural networks applied to house numbers digit classification",
          "year": 2012,
          "citations": 551,
          "abstract": "We classify digits of real-world house numbers using convolutional neural networks (ConvNets). Con-vNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 95.10% accuracy on the SVHN dataset (48% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.",
          "venue": "International Conference on Pattern Recognition",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/9f7f9aba0a6a966ce04e29e401ea28f9eae82f02",
          "authors": [
            "P. Sermanet",
            "Soumith Chintala",
            "Yann LeCun"
          ]
        },
        {
          "title": "Pedestrian Detection with Unsupervised Multi-stage Feature Learning",
          "year": 2012,
          "citations": 832,
          "abstract": "Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.",
          "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2013.465",
          "url": "https://www.semanticscholar.org/paper/a1306ce652f556fbb9e794d91084a29294298e6d",
          "authors": [
            "P. Sermanet",
            "K. Kavukcuoglu",
            "Soumith Chintala",
            "Yann LeCun"
          ]
        },
        {
          "title": "Efficient BackProp",
          "year": 2012,
          "citations": 3363,
          "abstract": null,
          "venue": "Neural Networks",
          "doi": "10.1007/978-3-642-35289-8_3",
          "url": "https://www.semanticscholar.org/paper/b87274e6d9aa4e6ba5148898aa92941617d2b6ed",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Genevieve B. Orr",
            "K. Müller"
          ]
        },
        {
          "title": "Deep Learning Made Easier by Linear Transformations in Perceptrons",
          "year": 2012,
          "citations": 210,
          "abstract": null,
          "venue": "International Conference on Artificial Intelligence and Statistics",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14",
          "authors": [
            "T. Raiko",
            "Harri Valpola",
            "Yann LeCun"
          ]
        },
        {
          "title": "Statistical Machine Learning and Dissolved Gas Analysis: A Review",
          "year": 2012,
          "citations": 114,
          "abstract": null,
          "venue": "IEEE Transactions on Power Delivery",
          "doi": "10.1109/TPWRD.2012.2197868",
          "url": "https://www.semanticscholar.org/paper/c58cafdd224437f2f7ee59ca70c10cf13ab260d9",
          "authors": [
            "Piotr Wojciech Mirowski",
            "Yann LeCun"
          ]
        },
        {
          "title": "1 Efficient BackProp",
          "year": 2012,
          "citations": 4,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c7f032fb72e4666f6b1763f2eec5906c86459b5b",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Genevieve B. Orr",
            "K. Müller"
          ]
        },
        {
          "title": "No more pesky learning rates",
          "year": 2012,
          "citations": 487,
          "abstract": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e5a685f40338f9c2f3e68e142efa217aad16dd56",
          "authors": [
            "T. Schaul",
            "Sixin Zhang",
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning Invariant Feature Hierarchies",
          "year": 2012,
          "citations": 161,
          "abstract": null,
          "venue": "ECCV Workshops",
          "doi": "10.1007/978-3-642-33863-2_51",
          "url": "https://www.semanticscholar.org/paper/e882a6014ac4d66b1035729305ff8aa76ba5d09d",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Fast Approximations to Structured Sparse Coding and Applications to Object Classification",
          "year": 2012,
          "citations": 39,
          "abstract": "We describe a method for fast approximation of sparse coding. A given input vector is passed through a binary tree. Each leaf of the tree contains a subset of dictionary elements. The coefficients corresponding to these dictionary elements are allowed to be nonzero and their values are calculated quickly by multiplication with a precomputed pseudoinverse. The tree parameters, the dictionary, and the subsets of the dictionary corresponding to each leaf are learned. In the process of describing this algorithm, we discuss the more general problem of learning the groups in group structured sparse modeling. We show that our method creates good sparse representations by using it in the object recognition framework of [1,2]. Implementing our own fast version of the SIFT descriptor the whole system runs at 20 frames per second on 321 ×481 sized images on a laptop with a quad-core cpu, while sacrificing very little accuracy on the Caltech 101, Caltech 256, and 15 scenes benchmarks.",
          "venue": "European Conference on Computer Vision",
          "doi": "10.1007/978-3-642-33715-4_15",
          "url": "https://www.semanticscholar.org/paper/e884feeab763a2e4a4d1279267a42d17c67c015c",
          "authors": [
            "Arthur Szlam",
            "Karol Gregor",
            "Yann LeCun"
          ]
        },
        {
          "title": "Structured sparse coding via lateral inhibition",
          "year": 2011,
          "citations": 58,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/044fddbc2e52add6d2b2c9d79544446854ebeb39",
          "authors": [
            "Arthur Szlam",
            "Karol Gregor",
            "Yann LeCun"
          ]
        },
        {
          "title": "NeuFlow: A runtime reconfigurable dataflow processor for vision",
          "year": 2011,
          "citations": 386,
          "abstract": null,
          "venue": "CVPR 2011 WORKSHOPS",
          "doi": "10.1109/CVPRW.2011.5981829",
          "url": "https://www.semanticscholar.org/paper/204710a6a6d935150b5b16daf74493dea6d1b7a2",
          "authors": [
            "C. Farabet",
            "B. Martini",
            "B. Corda",
            "Polina Akselrod",
            "E. Culurciello",
            "Yann LeCun"
          ]
        },
        {
          "title": "Unsupervised Learning of Sparse Features for Scalable Audio Classification",
          "year": 2011,
          "citations": 137,
          "abstract": null,
          "venue": "International Society for Music Information Retrieval Conference",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/34642631a6ef8130b98ce73e61298bb9c89b0c71",
          "authors": [
            "Mikael Henaff",
            "Kevin Jarrett",
            "K. Kavukcuoglu",
            "Yann LeCun"
          ]
        },
        {
          "title": "Hardware accelerated visual attention algorithm",
          "year": 2011,
          "citations": 7,
          "abstract": null,
          "venue": "Annual Conference on Information Sciences and Systems",
          "doi": "10.1109/CISS.2011.5766191",
          "url": "https://www.semanticscholar.org/paper/4c87d431433a5e16598a110691ff77160f012da0",
          "authors": [
            "Polina Akselrod",
            "Faye Zhao",
            "Ifigeneia Derekli",
            "C. Farabet",
            "B. Martini",
            "Yann LeCun",
            "E. Culurciello"
          ]
        },
        {
          "title": "Building Artificial Vision Systems with Machine Learning",
          "year": 2011,
          "citations": 1,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7ad0435f585f7bebe5c9207ee54228b2858cfc8f",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Concerto for violin and Markov model",
          "year": 2011,
          "citations": 0,
          "abstract": null,
          "venue": "Communications of the ACM",
          "doi": "10.1145/1897852.1897874",
          "url": "https://www.semanticscholar.org/paper/85c783700095ed2e6714bf2dd39e7f3dd79b72dd",
          "authors": [
            "J. Bello",
            "Yann LeCun",
            "R. Rowe"
          ]
        },
        {
          "title": "Ask the locals: Multi-way local pooling for image recognition",
          "year": 2011,
          "citations": 302,
          "abstract": null,
          "venue": "Vision",
          "doi": "10.1109/ICCV.2011.6126555",
          "url": "https://www.semanticscholar.org/paper/9791f1e47a48fa05387cb8dd93da53bf8f43c1f4",
          "authors": [
            "Y-Lan Boureau",
            "Nicolas Le Roux",
            "F. Bach",
            "J. Ponce",
            "Yann LeCun"
          ]
        },
        {
          "title": "Traffic sign recognition with multi-scale Convolutional Networks",
          "year": 2011,
          "citations": 795,
          "abstract": null,
          "venue": "The 2011 International Joint Conference on Neural Networks",
          "doi": "10.1109/IJCNN.2011.6033589",
          "url": "https://www.semanticscholar.org/paper/9ab0de951cc9cdf16887b1f841f8da6affc9c0de",
          "authors": [
            "P. Sermanet",
            "Yann LeCun"
          ]
        },
        {
          "title": "The NIPS workshop on Deep Learning and Unsupervised Feature Learning",
          "year": 2011,
          "citations": 11,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ae47286f40db22e64aaeec97497b9f522bd00943",
          "authors": [
            "T. Raiko",
            "Harri Valpola",
            "Yann LeCun"
          ]
        },
        {
          "title": "Large-Scale FPGA-based Convolutional Networks",
          "year": 2011,
          "citations": 135,
          "abstract": null,
          "venue": "",
          "doi": "10.1017/CBO9781139042918.020",
          "url": "https://www.semanticscholar.org/paper/b970c9d53c699a8e09f1d8dbe440b6f309712a89",
          "authors": [
            "C. Farabet",
            "Yann LeCun",
            "K. Kavukcuoglu",
            "B. Martini",
            "Polina Akselrod",
            "S. Talay",
            "E. Culurciello"
          ]
        },
        {
          "title": "Learning Representations by Maximizing Compression",
          "year": 2011,
          "citations": 17,
          "abstract": "We give an algorithm that learns a representation of data through compression. The algorithm 1) predicts bits sequentially from those previously seen and 2) has a structure and a number of computations similar to an autoencoder. The likelihood under the model can be calculated exactly, and arithmetic coding can be used directly for compression. When training on digits the algorithm learns filters similar to those of restricted boltzman machines and denoising autoencoders. Independent samples can be drawn from the model by a single sweep through the pixels. The algorithm has a good compression performance when compared to other methods that work under random ordering of pixels.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/bf326ddb9b9b15f5a285600af29e43c558ac890f",
          "authors": [
            "Karol Gregor",
            "Yann LeCun"
          ]
        },
        {
          "title": "Efficient Learning of Sparse Invariant Representations",
          "year": 2011,
          "citations": 5,
          "abstract": "We propose a simple and efficient algorithm for learning sparse invariant representations from unlabeled data with fast inference. When trained on short movies sequences, the learned features are selective to a range of orientations and spatial frequencies, but robust to a wide range of positions, similar to complex cells in the primary visual cortex. We give a hierarchical version of the algorithm, and give guarantees of fast convergence under certain conditions.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d427d3327cbab52468d310af6331e8cc84dea8ff",
          "authors": [
            "Karol Gregor",
            "Yann LeCun"
          ]
        },
        {
          "title": "Convolutional Matching Pursuit and Dictionary Training",
          "year": 2010,
          "citations": 49,
          "abstract": "Here, {W,Z} are the dictionary and the coefficients, respectively, and zk is the kth column of Z. K, q, and λ are user selected parameters controlling the power of the model. More recently, many models with additional structure have been proposed. For example, in [9, 2], the dictionary elements are arranged in groups and the sparsity is on the group level. In [3, 5, 7], the dictionaries are constructed to be translation invariant. In the former work, the dictionary is constructed via a non-negative matrix factorization. In the latter two works, the construction is a convolutional analogue of 1.2 or an l variant, with 0 < p < 1. In this short note we work with greedy algorithms for solving the convolutional analogues of 1.1. Specifically, we demonstrate that sparse coding by matching pursuit and dictionary learning via K-SVD [1] can be used in the translation invariant setting.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/10c8b29d7820bab2bab0610b9211b6852f272002",
          "authors": [
            "Arthur Szlam",
            "K. Kavukcuoglu",
            "Yann LeCun"
          ]
        },
        {
          "title": "Final thesis for the fulfilment of the requirements for the degree of MSc. in Applied Computing Science Improving Score Matching for learning statistical models of natural images",
          "year": 2010,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/1f6062b766dfa0dabc4edb58d3694798556c62dc",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields",
          "year": 2010,
          "citations": 72,
          "abstract": "We introduce a new neural architecture and an unsupervised algorithm for learning invariant representations from temporal sequence of images. The system uses two groups of complex cells whose outputs are combined multiplicatively: one that represents the content of the image, constrained to be constant over several consecutive frames, and one that represents the precise location of features, which is allowed to vary over time but constrained to be sparse. The architecture uses an encoder to extract features, and a decoder to reconstruct the input from the features. The method was applied to patches extracted from consecutive movie frames and produces orientation and frequency selective units analogous to the complex cells in V1. An extension of the method is proposed to train a network composed of units with local receptive field spread over a large image of arbitrary size. A layer of complex cells, subject to sparsity constraints, pool feature units over overlapping local neighborhoods, which causes the feature units to organize themselves into pinwheel patterns of orientation-selective receptive fields, similar to those observed in the mammalian visual cortex. A feed-forward encoder efficiently computes the feature representation of full images.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/31f04f8f83365fabf7ba9c9be1179c0da6815128",
          "authors": [
            "Karol Gregor",
            "Yann LeCun"
          ]
        },
        {
          "title": "A Theoretical Analysis of Feature Pooling in Visual Recognition",
          "year": 2010,
          "citations": 1366,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/405aed4b8ecdd869b2e83095dde51c396334115f",
          "authors": [
            "Y-Lan Boureau",
            "J. Ponce",
            "Yann LeCun"
          ]
        },
        {
          "title": "REASSESSING FHA RISK 1",
          "year": 2010,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/474b68356813e17dc22150df627f35703d98600c",
          "authors": [
            "Diego Aragon",
            "Andrew Caplin",
            "S. Chopra",
            "John Leahy",
            "Yann LeCun",
            "Marco Scoffier",
            "Joseph S. Tracy"
          ]
        },
        {
          "title": "Learning mid-level features for recognition",
          "year": 2010,
          "citations": 1157,
          "abstract": null,
          "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2010.5539963",
          "url": "https://www.semanticscholar.org/paper/498efaa51f5eda731dc6199c3547b9465717fa68",
          "authors": [
            "Y-Lan Boureau",
            "F. Bach",
            "Yann LeCun",
            "J. Ponce"
          ]
        },
        {
          "title": "Hybrid hessians for flexible optimization of pose graphs",
          "year": 2010,
          "citations": 15,
          "abstract": null,
          "venue": "2010 IEEE/RSJ International Conference on Intelligent Robots and Systems",
          "doi": "10.1109/IROS.2010.5650091",
          "url": "https://www.semanticscholar.org/paper/499e299ff22c9ff7b53fcd704eb38f0ce1fce1fd",
          "authors": [
            "M. Grimes",
            "Dragomir Anguelov",
            "Yann LeCun"
          ]
        },
        {
          "title": "Convolutional Learning of Spatio-temporal Features",
          "year": 2010,
          "citations": 697,
          "abstract": null,
          "venue": "European Conference on Computer Vision",
          "doi": "10.1007/978-3-642-15567-3_11",
          "url": "https://www.semanticscholar.org/paper/4d476b96be73fccc61f2076befbf5a468caa4180",
          "authors": [
            "Graham W. Taylor",
            "R. Fergus",
            "Yann LeCun",
            "C. Bregler"
          ]
        },
        {
          "title": "Time Series Modeling with Hidden Variables and Gradient-Based Algorithms",
          "year": 2010,
          "citations": 4,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5f21dd0b3eecf2a3d396f91f7183d59be6ef07fa",
          "authors": [
            "Yann LeCun",
            "Piotr Wojciech Mirowski"
          ]
        },
        {
          "title": "Bio-Inspired Vision Processor for Ultra-Fast Object Categorization",
          "year": 2010,
          "citations": 3,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6116a4b9af3c3e8313b442f62cf1cfd918b756e9",
          "authors": [
            "C. Farabet",
            "B. Martini",
            "Polina Akselrod",
            "B. Corda",
            "S. Talay",
            "Yann LeCun",
            "E. Culurciello"
          ]
        },
        {
          "title": "Predictive network modeling of the high-resolution dynamic plant transcriptome in response to nitrate",
          "year": 2010,
          "citations": 250,
          "abstract": "BackgroundNitrate, acting as both a nitrogen source and a signaling molecule, controls many aspects of plant development. However, gene networks involved in plant adaptation to fluctuating nitrate environments have not yet been identified.ResultsHere we use time-series transcriptome data to decipher gene relationships and consequently to build core regulatory networks involved in Arabidopsis root adaptation to nitrate provision. The experimental approach has been to monitor genome-wide responses to nitrate at 3, 6, 9, 12, 15 and 20 minutes using Affymetrix ATH1 gene chips. This high-resolution time course analysis demonstrated that the previously known primary nitrate response is actually preceded by a very fast gene expression modulation, involving genes and functions needed to prepare plants to use or reduce nitrate. A state-space model inferred from this microarray time-series data successfully predicts gene behavior in unlearnt conditions.ConclusionsThe experiments and methods allow us to propose a temporal working model for nitrate-driven gene networks. This network model is tested both in silico and experimentally. For example, the over-expression of a predicted gene hub encoding a transcription factor induced early in the cascade indeed leads to the modification of the kinetic nitrate response of sentinel genes such as NIR, NIA2, and NRT1.1, and several other transcription factors. The potential nitrate/hormone connections implicated by this time-series data are also evaluated.",
          "venue": "Genome Biology",
          "doi": "10.1186/gb-2010-11-12-r123",
          "url": "https://www.semanticscholar.org/paper/631b72d4f83f86f4c8d271c736ed3087c5f7be6e",
          "authors": [
            "Gabriel Krouk",
            "Piotr Wojciech Mirowski",
            "Yann LeCun",
            "D. Shasha",
            "G. Coruzzi"
          ]
        },
        {
          "title": "Regularized estimation of image statistics by Score Matching",
          "year": 2010,
          "citations": 87,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/74706fab48249b071e10615f8da60b8401fb9f3f",
          "authors": [
            "Diederik P. Kingma",
            "Yann LeCun"
          ]
        },
        {
          "title": "Dynamic auto-encoders for semantic indexing",
          "year": 2010,
          "citations": 34,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/80922663aaa7b09e86276ab97210ab2372d3f61a",
          "authors": [
            "Piotr Wojciech Mirowski",
            "Marc'Aurelio Ranzato",
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning Feature Hierarchies for Object Recognition",
          "year": 2010,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/84d2aaa96412156c6be140b0fb27731ecb822044",
          "authors": [
            "Yann LeCun",
            "K. Kavukcuoglu"
          ]
        },
        {
          "title": "Learning Convolutional Feature Hierarchies for Visual Recognition",
          "year": 2010,
          "citations": 594,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8b25a44f617c1ed3ed52c6655b0d456ff1c565bd",
          "authors": [
            "K. Kavukcuoglu",
            "P. Sermanet",
            "Y-Lan Boureau",
            "Karol Gregor",
            "Michaël Mathieu",
            "Yann LeCun"
          ]
        },
        {
          "title": "Hardware accelerated convolutional neural networks for synthetic vision systems",
          "year": 2010,
          "citations": 240,
          "abstract": null,
          "venue": "Proceedings of 2010 IEEE International Symposium on Circuits and Systems",
          "doi": "10.1109/ISCAS.2010.5537908",
          "url": "https://www.semanticscholar.org/paper/c3c82b476162d2d006e02180530875a64af18154",
          "authors": [
            "C. Farabet",
            "B. Martini",
            "Polina Akselrod",
            "S. Talay",
            "Yann LeCun",
            "E. Culurciello"
          ]
        },
        {
          "title": "Convolutional networks and applications in vision",
          "year": 2010,
          "citations": 2075,
          "abstract": null,
          "venue": "Proceedings of 2010 IEEE International Symposium on Circuits and Systems",
          "doi": "10.1109/ISCAS.2010.5537907",
          "url": "https://www.semanticscholar.org/paper/c43025c429b1fbf6f1379f61801a1b40834d62e7",
          "authors": [
            "Yann LeCun",
            "K. Kavukcuoglu",
            "C. Farabet"
          ]
        },
        {
          "title": "Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition",
          "year": 2010,
          "citations": 250,
          "abstract": "Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec",
          "authors": [
            "K. Kavukcuoglu",
            "Marc'Aurelio Ranzato",
            "Yann LeCun"
          ]
        },
        {
          "title": "Reassessing FHA Risk",
          "year": 2010,
          "citations": 24,
          "abstract": null,
          "venue": "",
          "doi": "10.3386/W15802",
          "url": "https://www.semanticscholar.org/paper/cb4ac759471a80cc5d15fd36361c6c73c407dd97",
          "authors": [
            "Diego Aragon",
            "Andrew Caplin",
            "S. Chopra",
            "John Leahy",
            "Yann LeCun",
            "Marco Scoffier",
            "Joseph S. Tracy"
          ]
        },
        {
          "title": "Learning Fast Approximations of Sparse Coding",
          "year": 2010,
          "citations": 1889,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e8f811399746c059bf4d4c3d43334045e0222209",
          "authors": [
            "Karol Gregor",
            "Yann LeCun"
          ]
        },
        {
          "title": "CNP: An FPGA-based processor for Convolutional Networks",
          "year": 2009,
          "citations": 362,
          "abstract": null,
          "venue": "International Conference on Field-Programmable Logic and Applications",
          "doi": "10.1109/FPL.2009.5272559",
          "url": "https://www.semanticscholar.org/paper/07956c7cf9bf4267b86d52aa4143c17a4aa5d0d6",
          "authors": [
            "C. Farabet",
            "Cyril Poulet",
            "Jefferson Y. Han",
            "Yann LeCun"
          ]
        },
        {
          "title": "Efficient off-road localization using visually corrected odometry",
          "year": 2009,
          "citations": 11,
          "abstract": null,
          "venue": "IEEE International Conference on Robotics and Automation",
          "doi": "10.1109/ROBOT.2009.5152880",
          "url": "https://www.semanticscholar.org/paper/083d38843c14e02970cf86d9161a5a2bf40d2bab",
          "authors": [
            "M. Grimes",
            "Yann LeCun"
          ]
        },
        {
          "title": "High-Accuracy Object Recognition with a New Convolutional Net Architecture and Learning Algorithm",
          "year": 2009,
          "citations": 1,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/10d2ea8d995c04e244bea89c66315f3a41338dd0",
          "authors": [
            "Kevin Jarrett",
            "Marc'Aurelio Ranzato",
            "K. Kavukcuoglu",
            "Yann LeCun"
          ]
        },
        {
          "title": "What is the best multi-stage architecture for object recognition?",
          "year": 2009,
          "citations": 2358,
          "abstract": null,
          "venue": "IEEE International Conference on Computer Vision",
          "doi": "10.1109/ICCV.2009.5459469",
          "url": "https://www.semanticscholar.org/paper/1f88427d7aa8225e47f946ac41a0667d7b69ac52",
          "authors": [
            "Kevin Jarrett",
            "K. Kavukcuoglu",
            "Marc'Aurelio Ranzato",
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning long‐range vision for autonomous off‐road driving",
          "year": 2009,
          "citations": 314,
          "abstract": null,
          "venue": "J. Field Robotics",
          "doi": "10.1002/rob.20276",
          "url": "https://www.semanticscholar.org/paper/2d8f527d1a96b0dae209daa6a241cf3255a6ec0d",
          "authors": [
            "R. Hadsell",
            "P. Sermanet",
            "J. Ben",
            "A. Erkan",
            "Marco Scoffier",
            "K. Kavukcuoglu",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "Unsupervised Learning of Feature Hierarchies",
          "year": 2009,
          "citations": 20,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3b3c153b09495e2f79dd973253f9d2ee763940a5",
          "authors": [
            "Yann LeCun",
            "Marc'Aurelio Ranzato"
          ]
        },
        {
          "title": "Learning invariant features through topographic filter maps",
          "year": 2009,
          "citations": 347,
          "abstract": null,
          "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2009.5206545",
          "url": "https://www.semanticscholar.org/paper/54a9c2553138932426faebcaa67a63a84a56b55d",
          "authors": [
            "K. Kavukcuoglu",
            "Marc'Aurelio Ranzato",
            "R. Fergus",
            "Yann LeCun"
          ]
        },
        {
          "title": "Dynamic Factor Graphs for Time Series Modeling",
          "year": 2009,
          "citations": 54,
          "abstract": null,
          "venue": "ECML/PKDD",
          "doi": "10.1007/978-3-642-04174-7_9",
          "url": "https://www.semanticscholar.org/paper/678d67db7b65b07b6d4b941cc138a33dcdf47b81",
          "authors": [
            "Piotr Wojciech Mirowski",
            "Yann LeCun"
          ]
        },
        {
          "title": "An FPGA-based stream processor for embedded real-time vision with Convolutional Networks",
          "year": 2009,
          "citations": 63,
          "abstract": null,
          "venue": "2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops",
          "doi": "10.1109/ICCVW.2009.5457611",
          "url": "https://www.semanticscholar.org/paper/777de11e21d10cb627d109d0f64630115ff7823f",
          "authors": [
            "C. Farabet",
            "Cyril Poulet",
            "Yann LeCun"
          ]
        },
        {
          "title": "EBLearn: Open-Source Energy-Based Learning in C++",
          "year": 2009,
          "citations": 34,
          "abstract": null,
          "venue": "IEEE International Conference on Tools with Artificial Intelligence",
          "doi": "10.1109/ICTAI.2009.28",
          "url": "https://www.semanticscholar.org/paper/78e5bca056ffc6186400ba540a0c0f43df909a12",
          "authors": [
            "P. Sermanet",
            "K. Kavukcuoglu",
            "Yann LeCun"
          ]
        },
        {
          "title": "Literacy and Learning",
          "year": 2009,
          "citations": 1,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/978-1-4419-1428-6_553",
          "url": "https://www.semanticscholar.org/paper/7ac232e5b466002a5890d5d9fd11adcc8a948b62",
          "authors": [
            "Cecilia Ka Yuk Chan",
            "Zoya A. Zorina",
            "Jesse R. Sparks",
            "S. López Ornat",
            "Christine D. Tsang",
            "Elena L. Grigorenko",
            "R. Lubow",
            "John C. Malone",
            "Michael Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "Thomas R. Minor",
            "Traci N. Plumb",
            "Hendrik Drachsler",
            "P. A. Kirschner",
            "Minoru Nakayama",
            "Rowena Santiago",
            "Ali Simsek",
            "Fang-Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "Richard P. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "Luis C. Lamb",
            "Artur d’Avila Garcez",
            "D. Németh",
            "K. Janacsek",
            "John J. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "Tracy Packiam Alloway",
            "Ulrike Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "Gabriele Kern-Isberner",
            "Joost Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C.-J. Olsson",
            "Lars Nyberg",
            "R. Freivalds",
            "Lynne Hall",
            "M. Hall",
            "Ulrike Hanke",
            "Lin S. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "Anthony Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "Paul C. Burnett",
            "M. Cannon",
            "Amy C. Edmondson",
            "Pitoyo Hartono",
            "A. Callender",
            "Rachel Barr",
            "Natalie Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohd. Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "David A. Kolb",
            "R. Reilly",
            "Klaus Nielsen",
            "Patricia Couvillon",
            "Heather King",
            "Justin Dillon",
            "Delia Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "Fernand Gobet",
            "Peter C. R. Lane",
            "Som Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "Don M. Tucker",
            "Paul J. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "Daniel Ansari",
            "K. Lionello-DeNolf",
            "John Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "Allyson F. Hadwin",
            "M. Coen",
            "Carlo Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. U. Hoppe",
            "Krisztina Molnar",
            "Erica de Vries",
            "Emmanuel G. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "Tristan Cazenave",
            "Ton Jong",
            "Jan Meij",
            "J. Gavelek",
            "Ailing Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "Stephen Billett",
            "D. Rumbaugh",
            "Michael J. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "Dong-oh Seo",
            "M. Monfils",
            "Anthony A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "Michael J. Kennedy",
            "Michael Jackson",
            "Terezinha Nunes",
            "B. Jackling",
            "R. Howard",
            "William G. Kennedy",
            "L. C. Drickamer"
          ]
        },
        {
          "title": "Workshop summary: Workshop on learning feature hierarchies",
          "year": 2009,
          "citations": 2,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": "10.1145/1553374.1553543",
          "url": "https://www.semanticscholar.org/paper/7c488cbc4103524b27f42254e9455429b23d92ca",
          "authors": [
            "Kai Yu",
            "R. Salakhutdinov",
            "Yann LeCun",
            "Geoffrey E. Hinton",
            "Yoshua Bengio"
          ]
        },
        {
          "title": "Classification of patterns of EEG synchronization for seizure prediction",
          "year": 2009,
          "citations": 397,
          "abstract": null,
          "venue": "Clinical Neurophysiology",
          "doi": "10.1016/j.clinph.2009.09.002",
          "url": "https://www.semanticscholar.org/paper/9da463da5a35149397273553c4de4626c49bf712",
          "authors": [
            "Piotr Wojciech Mirowski",
            "D. Madhavan",
            "Yann LeCun",
            "R. Kuzniecky"
          ]
        },
        {
          "title": "Learning long-range vision for autonomous off-road driving",
          "year": 2009,
          "citations": 125,
          "abstract": null,
          "venue": "",
          "doi": "10.1002/ROB.V26:2",
          "url": "https://www.semanticscholar.org/paper/a90998e0023db48b207cee3b39b0441b3935aaa7",
          "authors": [
            "R. Hadsell",
            "P. Sermanet",
            "J. Ben",
            "A. Erkan",
            "Marco Scoffier",
            "K. Kavukcuoglu",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "A multirange architecture for collision‐free off‐road robot navigation",
          "year": 2009,
          "citations": 48,
          "abstract": null,
          "venue": "J. Field Robotics",
          "doi": "10.1002/rob.20270",
          "url": "https://www.semanticscholar.org/paper/b48d78ed73144d69f6239696e55ba9596fe7813b",
          "authors": [
            "P. Sermanet",
            "R. Hadsell",
            "Marco Scoffier",
            "M. Grimes",
            "J. Ben",
            "A. Erkan",
            "Chris Crudele",
            "Urs Miller",
            "Yann LeCun"
          ]
        },
        {
          "title": "Deep belief net learning in a long-range vision system for autonomous off-road driving",
          "year": 2008,
          "citations": 100,
          "abstract": null,
          "venue": "2008 IEEE/RSJ International Conference on Intelligent Robots and Systems",
          "doi": "10.1109/IROS.2008.4651217",
          "url": "https://www.semanticscholar.org/paper/010b7587ef04d12f162cbdf55657442e289b343d",
          "authors": [
            "R. Hadsell",
            "A. Erkan",
            "P. Sermanet",
            "Marco Scoffier",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "Mapping and planning under uncertainty in mobile robots with long-range perception",
          "year": 2008,
          "citations": 38,
          "abstract": null,
          "venue": "2008 IEEE/RSJ International Conference on Intelligent Robots and Systems",
          "doi": "10.1109/IROS.2008.4651203",
          "url": "https://www.semanticscholar.org/paper/1331f67dd0c39845a7b129817a0697d798ffc548",
          "authors": [
            "P. Sermanet",
            "R. Hadsell",
            "Marco Scoffier",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "Machine Learning and the Spatial Structure of House Prices and Housing Returns",
          "year": 2008,
          "citations": 29,
          "abstract": null,
          "venue": "",
          "doi": "10.2139/ssrn.1316046",
          "url": "https://www.semanticscholar.org/paper/35d47b5d82e1cdb4d70de7bf35cdfe970d793f35",
          "authors": [
            "Andrew Caplin",
            "S. Chopra",
            "John Leahy",
            "Yann LeCun",
            "Trivikraman Thampy"
          ]
        },
        {
          "title": "Autonomous Learning for Long Range Vision in Mobile Robots",
          "year": 2008,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/39d3c37cb479cd16c027919a5c8774649daec4a9",
          "authors": [
            "R. Hadsell",
            "P. Sermanet",
            "A. Erkan",
            "K. Kavukcuoglu",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning Long-Range Vision for an Offroad Robot",
          "year": 2008,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/4b8d80f91d271f61b26db5ad627e24e59955c56a",
          "authors": [
            "Yann LeCun",
            "R. Hadsell"
          ]
        },
        {
          "title": "Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG",
          "year": 2008,
          "citations": 174,
          "abstract": null,
          "venue": "IEEE Workshop on Machine Learning for Signal Processing",
          "doi": "10.1109/MLSP.2008.4685487",
          "url": "https://www.semanticscholar.org/paper/966ce7b2567280088ee1d5816cee9e06d12fa19d",
          "authors": [
            "Piotr Wojciech Mirowski",
            "Yann LeCun",
            "D. Madhavan",
            "R. Kuzniecky"
          ]
        },
        {
          "title": "Learning maneuver dictionaries for ground robot planning",
          "year": 2008,
          "citations": 15,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/bb0eefb5b7d1a07c9abb2efa888060cde8a20bcc",
          "authors": [
            "P. Sermanet",
            "Marco Scoffier",
            "Chris Crudele",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "Factor Graphs for Relational Regression",
          "year": 2008,
          "citations": 5,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d3c3ad20aa5580a6784e445d0296c4ef09a59684",
          "authors": [
            "Yann LeCun",
            "S. Chopra"
          ]
        },
        {
          "title": "Computeur Vision and Pattern Recognition (CVPR). Conference",
          "year": 2008,
          "citations": 1,
          "abstract": null,
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1007/S11263-008-0162-4",
          "url": "https://www.semanticscholar.org/paper/d66f62ffdab3b24326bebd0cf81db5789e56dce0",
          "authors": [
            "A. Fitzgibbon",
            "C. J. Taylor",
            "Yann LeCun"
          ]
        },
        {
          "title": "Editorial",
          "year": 2008,
          "citations": 0,
          "abstract": null,
          "venue": "International Journal of Computer Vision",
          "doi": "10.1007/s11263-008-0162-4",
          "url": "https://www.semanticscholar.org/paper/f200d76bb51c822e797bc54fb12c8c0fc384d9b1",
          "authors": [
            "A. Fitzgibbon",
            "C. J. Taylor",
            "Yann LeCun"
          ]
        },
        {
          "title": "Invariant Feature Learning on a Mobile Robot",
          "year": 2008,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/f67eceab1c562ef3e265f2a64223ffead3ed3c22",
          "authors": [
            "R. Hadsell",
            "Yann LeCun"
          ]
        },
        {
          "title": "Time-Delay Neural Networks and Independent Component Analysis for EEG-Based Prediction of Epileptic Seizures Propagation",
          "year": 2007,
          "citations": 24,
          "abstract": null,
          "venue": "AAAI Conference on Artificial Intelligence",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0bb4afa755f8694a9c8bdac67149907833637b42",
          "authors": [
            "Piotr Wojciech Mirowski",
            "D. Madhavan",
            "Yann LeCun"
          ]
        },
        {
          "title": "SPEED-RANGE DILEMMAS FOR VISION-BASED NAVIGATION IN UNSTRUCTURED TERRAIN",
          "year": 2007,
          "citations": 10,
          "abstract": null,
          "venue": "",
          "doi": "10.3182/20070903-3-FR-2921.00052",
          "url": "https://www.semanticscholar.org/paper/181c805eb0b2524efb16c6b14dcf2e435140e634",
          "authors": [
            "P. Sermanet",
            "R. Hadsell",
            "J. Ben",
            "A. Erkan",
            "B. Flepp",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "Adaptive long range vision in unstructured terrain",
          "year": 2007,
          "citations": 18,
          "abstract": null,
          "venue": "2007 IEEE/RSJ International Conference on Intelligent Robots and Systems",
          "doi": "10.1109/IROS.2007.4399622",
          "url": "https://www.semanticscholar.org/paper/18966721a3dc227d8632afbaa75d95cbc1708119",
          "authors": [
            "A. Erkan",
            "R. Hadsell",
            "P. Sermanet",
            "J. Ben",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "A Unified Energy-Based Framework for Unsupervised Learning",
          "year": 2007,
          "citations": 142,
          "abstract": null,
          "venue": "International Conference on Artificial Intelligence and Statistics",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/306ddd8b7ea3ead125491efc3e8a9f738ce65b89",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "S. Chopra",
            "Yann LeCun"
          ]
        },
        {
          "title": "A Sparse and Locally Shift Invariant Feature Extractor Applied to Document Images",
          "year": 2007,
          "citations": 27,
          "abstract": null,
          "venue": "IEEE International Conference on Document Analysis and Recognition",
          "doi": "10.1109/ICDAR.2007.35",
          "url": "https://www.semanticscholar.org/paper/3cfff20568fe1964b407e2a4452f6064ca794f3c",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Yann LeCun"
          ]
        },
        {
          "title": "Sparse Feature Learning for Deep Belief Networks",
          "year": 2007,
          "citations": 939,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/41fef1a197fab9684a4608b725d3ae72e1ab4b39",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "Yann LeCun"
          ]
        },
        {
          "title": "Energy-Based Factor Graphs for Prediction in Relational Data",
          "year": 2007,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/47e5b8dcd44d3d1f595e9e8636ebe2ecdc1de744",
          "authors": [
            "S. Chopra",
            "Yann LeCun"
          ]
        },
        {
          "title": "A multi-range vision strategy for autonomous offroad navigation",
          "year": 2007,
          "citations": 15,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5fe3deeec85b63e9f10f643389a08e8a0aa8d2a6",
          "authors": [
            "R. Hadsell",
            "A. Erkan",
            "P. Sermanet",
            "J. Ben",
            "K. Kavukcuoglu",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "Online Learning for Offroad Robots: Spatial Label Propagation to Learn Long-Range Traversability",
          "year": 2007,
          "citations": 34,
          "abstract": null,
          "venue": "Robotics: Science and Systems",
          "doi": "10.15607/RSS.2007.III.003",
          "url": "https://www.semanticscholar.org/paper/6e06fa79a4a7fbbda36ae4d12cd0d5135b67d28d",
          "authors": [
            "R. Hadsell",
            "P. Sermanet",
            "J. Ben",
            "A. Erkan",
            "Jeff Han",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "Scaling learning algorithms towards AI",
          "year": 2007,
          "citations": 1368,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6fdb77260fc83dff91c44fea0f31a2cb8ed13d04",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun"
          ]
        },
        {
          "title": "Global Map Long Range Vision ( FAROD ) Cameras Vehicle Map Global planner Route to goal Global Map Goal Local candidates",
          "year": 2007,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7c305a78154c714372a10ce615accc408c0eb221",
          "authors": [
            "R. Hadsell",
            "A. Erkan",
            "P. Sermanet",
            "J. Ben",
            "K. Kavukcuoglu",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "Automatic recognition of biological particles in microscopic images",
          "year": 2007,
          "citations": 118,
          "abstract": null,
          "venue": "Pattern Recognition Letters",
          "doi": "10.1016/j.patrec.2006.06.010",
          "url": "https://www.semanticscholar.org/paper/9228aece7615fbc33394a3124aef27f9a85853c3",
          "authors": [
            "Marc'Aurelio Ranzato",
            "P. E. Taylor",
            "J. House",
            "R. Flagan",
            "Yann LeCun",
            "P. Perona"
          ]
        },
        {
          "title": "Discovering the hidden structure of house prices with a non-parametric latent manifold model",
          "year": 2007,
          "citations": 28,
          "abstract": null,
          "venue": "Knowledge Discovery and Data Mining",
          "doi": "10.1145/1281192.1281214",
          "url": "https://www.semanticscholar.org/paper/9f4028fea3fcc99776391d66d4e40edbe78388b3",
          "authors": [
            "S. Chopra",
            "Trivikraman Thampy",
            "John Leahy",
            "Andrew Caplin",
            "Yann LeCun"
          ]
        },
        {
          "title": "The Need for Open Source Software in Machine Learning",
          "year": 2007,
          "citations": 235,
          "abstract": null,
          "venue": "Journal of machine learning research",
          "doi": "10.5555/1314498.1314577",
          "url": "https://www.semanticscholar.org/paper/ab08f2a0b98fe7938d08875eb6125fa518620222",
          "authors": [
            "S. Sonnenburg",
            "M. Braun",
            "Cheng Soon Ong",
            "Samy Bengio",
            "L. Bottou",
            "G. Holmes",
            "Yann LeCun",
            "K. Müller",
            "Fernando C Pereira",
            "C. Rasmussen",
            "Gunnar Rätsch",
            "B. Scholkopf",
            "Alex Smola",
            "Pascal Vincent",
            "J. Weston",
            "R. C. Williamson"
          ]
        },
        {
          "title": "Energy-Based Models in Document Recognition and Computer Vision",
          "year": 2007,
          "citations": 60,
          "abstract": null,
          "venue": "IEEE International Conference on Document Analysis and Recognition",
          "doi": "10.1109/ICDAR.2007.107",
          "url": "https://www.semanticscholar.org/paper/c09c49d92d10a84e6efdbaf67d979bab2c22be3e",
          "authors": [
            "Yann LeCun",
            "S. Chopra",
            "Marc'Aurelio Ranzato",
            "Fu Jie Huang"
          ]
        },
        {
          "title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition",
          "year": 2007,
          "citations": 1190,
          "abstract": null,
          "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2007.383157",
          "url": "https://www.semanticscholar.org/paper/ccd52aff02b0f902f4ce7247c4fee7273014c41c",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Fu Jie Huang",
            "Y-Lan Boureau",
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning Sparse and Invariant Features Hierarchies",
          "year": 2007,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/dad2f8fd3e155c86b089b5b054a1e0c5b8f79915",
          "authors": [
            "Fu Jie Huang",
            "Yann LeCun"
          ]
        },
        {
          "title": "Message from the Program and General Chairs",
          "year": 2006,
          "citations": 0,
          "abstract": null,
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2006.184",
          "url": "https://www.semanticscholar.org/paper/1d0e3de0d4716f99008e6251261d1c7d8feb6f9a",
          "authors": [
            "A. Fitzgibbon",
            "C. J. Taylor",
            "Yann LeCun",
            "D. Huttenlocher",
            "David Forsyth"
          ]
        },
        {
          "title": "Dimensionality Reduction by Learning an Invariant Mapping",
          "year": 2006,
          "citations": 5509,
          "abstract": null,
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2006.100",
          "url": "https://www.semanticscholar.org/paper/46f30e94dd3d5902141c5fbe58d0bc9189545c76",
          "authors": [
            "R. Hadsell",
            "S. Chopra",
            "Yann LeCun"
          ]
        },
        {
          "title": "On-Line Learning of Long-Range Obstacle Detection for Off-Road Robots",
          "year": 2006,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7959e1cb872a7dd284bf082f8cf16fc2c00da750",
          "authors": [
            "R. Hadsell",
            "P. Sermanet",
            "J. Ben",
            "Jeff Han",
            "S. Chopra",
            "Marc'Aurelio Ranzato",
            "Yury Sulsky",
            "B. Flepp",
            "Urs Muller",
            "Yann LeCun"
          ]
        },
        {
          "title": "A Tutorial on Energy-Based Learning",
          "year": 2006,
          "citations": 1642,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7fc604e1a3e45cd2d2742f96d62741930a363efa",
          "authors": [
            "Yann LeCun",
            "S. Chopra",
            "R. Hadsell",
            "Aurelio Ranzato",
            "Fu Jie Huang"
          ]
        },
        {
          "title": "E cient Learning of Sparse Overcomplete Representations with an Energy-Based Model",
          "year": 2006,
          "citations": 13,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8680319b8619e71bb5bd75103d2b7f1d08f48491",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Christopher S. Poultney",
            "S. Chopra",
            "Yann LeCun"
          ]
        },
        {
          "title": "Efficient Learning of Sparse Representations with an Energy-Based Model",
          "year": 2006,
          "citations": 1342,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": "10.7551/mitpress/7503.003.0147",
          "url": "https://www.semanticscholar.org/paper/932c2a02d462abd75af018125413b1ceaa1ee3f4",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Christopher S. Poultney",
            "S. Chopra",
            "Yann LeCun"
          ]
        },
        {
          "title": "PROC OF THE IEEE NOVEMBER Gradient Based Learning Applied to Document Recognition",
          "year": 2006,
          "citations": 22,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/afc0a5d20dd7160f42fc5c27ef9746b14ebe53f4",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Yoshua Bengio"
          ]
        },
        {
          "title": "Proceedings : 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition : CVPR 2006 : June 17-22, 2006, New York, NY",
          "year": 2006,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/baf9530d8ab59e42fef6b35b51b27261e1854183",
          "authors": [
            "A. Fitzgibbon",
            "C. J. Taylor",
            "Yann LeCun"
          ]
        },
        {
          "title": "Large-scale Learning with SVM and Convolutional for Generic Object Categorization",
          "year": 2006,
          "citations": 383,
          "abstract": null,
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2006.164",
          "url": "https://www.semanticscholar.org/paper/cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb",
          "authors": [
            "Fu Jie Huang",
            "Yann LeCun"
          ]
        },
        {
          "title": "Building an Automatic Phenotyping System of Developing Embryos",
          "year": 2006,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/eafc996e3a178e93bd7efe926edbcb4129dbb0fe",
          "authors": [
            "Yann LeCun",
            "F. Ning"
          ]
        },
        {
          "title": "Toward automatic phenotyping of developing embryos from videos",
          "year": 2005,
          "citations": 290,
          "abstract": null,
          "venue": "IEEE Transactions on Image Processing",
          "doi": "10.1109/TIP.2005.852470",
          "url": "https://www.semanticscholar.org/paper/c029513aef54460ef6a468ff83f549d7ffbb646b",
          "authors": [
            "F. Ning",
            "D. Delhomme",
            "Yann LeCun",
            "F. Piano",
            "L. Bottou",
            "P. Barbano"
          ]
        },
        {
          "title": "Off-Road Obstacle Avoidance through End-to-End Learning",
          "year": 2005,
          "citations": 609,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c63d2e3b11d25971cc50cde869f59d34c62a1291",
          "authors": [
            "Yann LeCun",
            "Urs Muller",
            "J. Ben",
            "E. Cosatto",
            "B. Flepp"
          ]
        },
        {
          "title": "Learning a similarity metric discriminatively, with application to face verification",
          "year": 2005,
          "citations": 4428,
          "abstract": null,
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2005.202",
          "url": "https://www.semanticscholar.org/paper/cfaae9b6857b834043606df3342d8dc97524aa9d",
          "authors": [
            "S. Chopra",
            "R. Hadsell",
            "Yann LeCun"
          ]
        },
        {
          "title": "The mnist database of handwritten digits",
          "year": 2005,
          "citations": 7095,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2",
          "authors": [
            "Yann LeCun",
            "Corinna Cortes"
          ]
        },
        {
          "title": "Graph transformer networks for image recognition",
          "year": 2005,
          "citations": 14,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/f78017ab52a9e10d206da41363ec0c11a10e4757",
          "authors": [
            "L. Bottou",
            "Yann LeCun"
          ]
        },
        {
          "title": "Loss Functions for Discriminative Training of Energy-Based Models",
          "year": 2005,
          "citations": 160,
          "abstract": null,
          "venue": "International Conference on Artificial Intelligence and Statistics",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/fae82787fc4268f579823696bf8f54b22e253711",
          "authors": [
            "Yann LeCun",
            "Fu Jie Huang"
          ]
        },
        {
          "title": "Synergistic Face Detection and Pose Estimation with Energy-Based Models",
          "year": 2004,
          "citations": 430,
          "abstract": null,
          "venue": "Journal of machine learning research",
          "doi": "10.5555/1314498.1314539",
          "url": "https://www.semanticscholar.org/paper/6b728a7442ca158f895d07c11c77d302269a832d",
          "authors": [
            "Margarita Osadchy",
            "Yann LeCun",
            "Matthew L. Miller"
          ]
        },
        {
          "title": "Learning methods for generic object recognition with invariance to pose and lighting",
          "year": 2004,
          "citations": 1565,
          "abstract": null,
          "venue": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.",
          "doi": "10.1109/CVPR.2004.144",
          "url": "https://www.semanticscholar.org/paper/f354310098e09c1e1dc88758fca36767fd9d084d",
          "authors": [
            "Yann LeCun",
            "Fu Jie Huang",
            "L. Bottou"
          ]
        },
        {
          "title": "Large Scale Online Learning",
          "year": 2003,
          "citations": 452,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/133809cf62bf67f0a63b35e5ef5180d20c9aec19",
          "authors": [
            "L. Bottou",
            "Yann LeCun"
          ]
        },
        {
          "title": "Real Time Voice Processing with Audiovisual Feedback: Toward Autonomous Agents with Perfect Pitch",
          "year": 2002,
          "citations": 28,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6986517700641f245e908e64c2acf6dd5f333b95",
          "authors": [
            "L. Saul",
            "Daniel D. Lee",
            "C. Isbell",
            "Yann LeCun"
          ]
        },
        {
          "title": "A general segmentation scheme for DjVu document compression",
          "year": 2002,
          "citations": 21,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c4afb604c6e00c9fcc358daae260eeedb84be863",
          "authors": [
            "P. Haffner",
            "L. Bottou",
            "Yann LeCun",
            "L. Vincent"
          ]
        },
        {
          "title": "Image and video coding—emerging standards and beyond",
          "year": 2001,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": "10.1016/B978-155860651-7/50094-2",
          "url": "https://www.semanticscholar.org/paper/24232511c7cb88468b5e69f7638e1c722315f616",
          "authors": [
            "B. Haskell",
            "P. Howard",
            "Yann LeCun",
            "Atul Puri",
            "J. Ostermann",
            "M. Civanlar",
            "L. Rabiner",
            "L. Bottou",
            "P. Haffner"
          ]
        },
        {
          "title": "Advances in neural information processing systems: Proceedings of the first 12 conferences (CDROM)",
          "year": 2001,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6822d4f630b7e7de2582e77b577b6f037e32ee5c",
          "authors": [
            "M. I. Jordan",
            "Yann LeCun",
            "S. Solla"
          ]
        },
        {
          "title": "Efficient conversion of digital documents to multilayer raster formats",
          "year": 2001,
          "citations": 10,
          "abstract": null,
          "venue": "Proceedings of Sixth International Conference on Document Analysis and Recognition",
          "doi": "10.1109/ICDAR.2001.953829",
          "url": "https://www.semanticscholar.org/paper/bc95fe16a7a9f29b13ea242cdc69ec3b977f245d",
          "authors": [
            "L. Bottou",
            "P. Haffner",
            "Yann LeCun"
          ]
        },
        {
          "title": "Djvu: Un systeme de compression d'images pour la distribution reticulaire de documents numerises (Djvu: An image compression system for distributing scanned document on the internet)",
          "year": 2000,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6519eb56306e2c848d883a4f47919dd9fdcab755",
          "authors": [
            "L. Bottou",
            "P. Haffner",
            "Yann LeCun",
            "P. Howard",
            "Pascal Vincent",
            "B. Riemers"
          ]
        },
        {
          "title": "Transformation invariance in pattern recognition: Tangent distance and propagation",
          "year": 2000,
          "citations": 160,
          "abstract": null,
          "venue": "International journal of imaging systems and technology (Print)",
          "doi": "10.1002/1098-1098(2000)11:3%3C181::AID-IMA1003%3E3.0.CO;2-E",
          "url": "https://www.semanticscholar.org/paper/76c67b335b8192b61a0e9364827afcc2f0842d11",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun",
            "J. Denker",
            "B. Victorri"
          ]
        },
        {
          "title": "DjVu: analyzing and compressing scanned documents for Internet distribution",
          "year": 1999,
          "citations": 53,
          "abstract": null,
          "venue": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)",
          "doi": "10.1109/ICDAR.1999.791865",
          "url": "https://www.semanticscholar.org/paper/3089d4a41a3310a62bc2effd4a69577f79014f8f",
          "authors": [
            "P. Haffner",
            "L. Bottou",
            "P. Howard",
            "Yann LeCun"
          ]
        },
        {
          "title": "Multimedia Processing for Advanced Communications Services",
          "year": 1999,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/978-1-4471-0859-7_42",
          "url": "https://www.semanticscholar.org/paper/3fb58e78700febc4a03ebd8eccea794218bd7149",
          "authors": [
            "B. Shahraray",
            "R. Cox",
            "B. Haskell",
            "Yann LeCun",
            "L. Rabiner"
          ]
        },
        {
          "title": "Object Recognition with Gradient-Based Learning",
          "year": 1999,
          "citations": 1027,
          "abstract": null,
          "venue": "Shape, Contour and Grouping in Computer Vision",
          "doi": "10.1007/3-540-46805-6_19",
          "url": "https://www.semanticscholar.org/paper/9a5ea367f0fb05805acaa84a402f5d036eea37dc",
          "authors": [
            "Yann LeCun",
            "P. Haffner",
            "L. Bottou",
            "Yoshua Bengio"
          ]
        },
        {
          "title": "Color documents on the Web with DjVu",
          "year": 1999,
          "citations": 13,
          "abstract": null,
          "venue": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)",
          "doi": "10.1109/ICIP.1999.821605",
          "url": "https://www.semanticscholar.org/paper/dec714928a06610355e4d44c9468d2be0af2f47c",
          "authors": [
            "P. Haffner",
            "Yann LeCun",
            "L. Bottou",
            "P. Howard",
            "Pascal Vincent",
            "B. Riemers"
          ]
        },
        {
          "title": "OPTICAL CHARACTER RECOGNTION FOR AUTOMATIC TELLER MACHINES",
          "year": 1998,
          "citations": 3,
          "abstract": null,
          "venue": "",
          "doi": "10.1142/9789812816955_0044",
          "url": "https://www.semanticscholar.org/paper/0f64a60e61a466487f0db5046b2b454756ecb32f",
          "authors": [
            "L. Jackel",
            "Yann LeCun",
            "C. E. Stenard",
            "B. I. Strom",
            "D. Sharman",
            "D. Zuckert"
          ]
        },
        {
          "title": "Gradient-based learning applied to document recognition",
          "year": 1998,
          "citations": 57263,
          "abstract": null,
          "venue": "Proceedings of the IEEE",
          "doi": "10.1109/5.726791",
          "url": "https://www.semanticscholar.org/paper/162d958ff885f1462aeda91cd72582323fd6a1f4",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Yoshua Bengio",
            "P. Haffner"
          ]
        },
        {
          "title": "Unsupervised Learning of Sparse and Invariant Features Hierarchies",
          "year": 1998,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3011a4d3c3dd2d303a71949db348dd1d999c9db2",
          "authors": [
            "Y-Lan Boureau",
            "Fu Jie Huang",
            "Yann LeCun"
          ]
        },
        {
          "title": "High quality document image compression with \"DjVu\"",
          "year": 1998,
          "citations": 276,
          "abstract": null,
          "venue": "J. Electronic Imaging",
          "doi": "10.1117/1.482609",
          "url": "https://www.semanticscholar.org/paper/34103850fbd71d43b19f8335f378f421ab9a1aa8",
          "authors": [
            "L. Bottou",
            "P. Haffner",
            "P. Howard",
            "Patrice Y. Simard",
            "Yoshua Bengio",
            "Yann LeCun"
          ]
        },
        {
          "title": "DjVu: a Compression Method for Distributing Scanned Documents in Color over the Internet",
          "year": 1998,
          "citations": 9,
          "abstract": "We present a new image compression technique called “DjVu” that is specifically geared towards the compression of scanned documents in color at high revolution. DjVu enable any screen connected to the Internet to access and display images of scanned pages while faithfully reproducing the font, color, drawings, pictures, and paper texture. With DjVu, a typical magazine page in color at 300dpi can be compressed down to between 40 to 60 KB, approximately 5 to 10 times better than JPEG for a similar level of subjective quality. A real-time, memory efficient version of the decoder is available as a plug-in for popular web browsers.",
          "venue": "International Conference on Communications in Computing",
          "doi": "10.2352/CIC.1998.6.1.art00047",
          "url": "https://www.semanticscholar.org/paper/4737e136dbf34e37b8c2ee3c615e3e808d5d63fc",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "P. Haffner",
            "P. Howard"
          ]
        },
        {
          "title": "Convolutional networks for images, speech, and time series",
          "year": 1998,
          "citations": 5983,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/563e821bb5ea825efb56b77484f5287f08cf3753",
          "authors": [
            "Yann LeCun",
            "Yoshua Bengio"
          ]
        },
        {
          "title": "Browsing through high quality document images with DjVu",
          "year": 1998,
          "citations": 18,
          "abstract": null,
          "venue": "Proceedings IEEE International Forum on Research and Technology Advances in Digital Libraries -ADL'98-",
          "doi": "10.1109/ADL.1998.670431",
          "url": "https://www.semanticscholar.org/paper/714c5e9f832b12e4029bc8516b3d9fe11ae0553e",
          "authors": [
            "P. Haffner",
            "L. Bottou",
            "P. Howard",
            "Patrice Y. Simard",
            "Yoshua Bengio",
            "Yann LeCun"
          ]
        },
        {
          "title": "[Learning with computers].",
          "year": 1998,
          "citations": 48,
          "abstract": null,
          "venue": "Schweizer Monatsschrift fur Zahnmedizin = Revue mensuelle suisse d'odonto-stomatologie = Rivista mensile svizzera di odontologia e stomatologia",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/784cb31fc9fe06cfda34368b59258b0e5a7201e9",
          "authors": [
            "Cecilia Ka Yuk Chan",
            "Zoya A. Zorina",
            "Jesse R. Sparks",
            "S. López Ornat",
            "Christine D. Tsang",
            "Elena L. Grigorenko",
            "R. Lubow",
            "John C. Malone",
            "Michael Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "Thomas R. Minor",
            "Traci N. Plumb",
            "Hendrik Drachsler",
            "P. A. Kirschner",
            "Minoru Nakayama",
            "Rowena Santiago",
            "Ali Simsek",
            "Fang-Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "Richard P. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "Luis C. Lamb",
            "Artur d’Avila Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "Tracy Packiam Alloway",
            "Ulrike Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "Gabriele Kern-Isberner",
            "Joost Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C.-J. Olsson",
            "Lars Nyberg",
            "R. Freivalds",
            "Lynne Hall",
            "M. Hall",
            "Ulrike Hanke",
            "Lin S. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "Paul C. Burnett",
            "M. Cannon",
            "Amy C. Edmondson",
            "Pitoyo Hartono",
            "A. Callender",
            "Rachel Barr",
            "Natalie Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohd. Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "David A. Kolb",
            "R. Reilly",
            "Klaus Nielsen",
            "Patricia Couvillon",
            "Heather King",
            "Justin Dillon",
            "Delia Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "Fernand Gobet",
            "Peter C. R. Lane",
            "Som Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "Don M. Tucker",
            "Paul J. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "Daniel Ansari",
            "K. Lionello-DeNolf",
            "John Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. U. Hoppe",
            "Krisztina Molnar",
            "Erica de Vries",
            "Emmanuel G. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "Tristan Cazenave",
            "Ton Jong",
            "Jan Meij",
            "J. Gavelek",
            "Ailing Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "Michael J. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "Dong-oh Seo",
            "M. Monfils",
            "Anthony A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "Michael J. Kennedy",
            "Michael Jackson",
            "Terezinha Nunes",
            "B. Jackling",
            "R. Howard",
            "William G. Kennedy",
            "L. C. Drickamer"
          ]
        },
        {
          "title": "On the applications of multimedia processing to communications",
          "year": 1998,
          "citations": 58,
          "abstract": null,
          "venue": "Proceedings of the IEEE",
          "doi": "10.1109/5.664272",
          "url": "https://www.semanticscholar.org/paper/82cf2c37ed3eca25660ead9c034ecadcfb76fb4a",
          "authors": [
            "R. Cox",
            "B. Haskell",
            "Yann LeCun",
            "B. Shahraray",
            "L. Rabiner"
          ]
        },
        {
          "title": "Applications of Artificial Neural Networks to Image Processing",
          "year": 1998,
          "citations": 15,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/9c40fe0b583565d01f5d4032d574a020866bb817",
          "authors": [
            "R. Chellappa",
            "K. Fukushima",
            "A. Katsaggelos",
            "S. Kung",
            "Yann LeCun",
            "N. Nasrabadi",
            "T. Poggio"
          ]
        },
        {
          "title": "Boxlets: A Fast Convolution Algorithm for Signal Processing and Neural Networks",
          "year": 1998,
          "citations": 156,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b2ef3d741a8d267b813781a73d4222b6dbba99fa",
          "authors": [
            "Patrice Y. Simard",
            "L. Bottou",
            "P. Haffner",
            "Yann LeCun"
          ]
        },
        {
          "title": "Image and video coding-emerging standards and beyond",
          "year": 1998,
          "citations": 90,
          "abstract": null,
          "venue": "IEEE Trans. Circuits Syst. Video Technol.",
          "doi": "10.1109/76.735379",
          "url": "https://www.semanticscholar.org/paper/bb52c49e2a82cfc33491c7bdf61faea0e8251a9e",
          "authors": [
            "B. Haskell",
            "P. Howard",
            "Yann LeCun",
            "Atul Puri",
            "J. Ostermann",
            "M. Civanlar",
            "L. Rabiner",
            "L. Bottou",
            "P. Haffner"
          ]
        },
        {
          "title": "On the application of multimedia processing to telecommunications",
          "year": 1997,
          "citations": 9,
          "abstract": null,
          "venue": "Proceedings of International Conference on Image Processing",
          "doi": "10.1109/ICIP.1997.647370",
          "url": "https://www.semanticscholar.org/paper/04b5268727c322dcd9d10a14decb5d8c03a4f378",
          "authors": [
            "R. Cox",
            "B. Haskell",
            "Yann LeCun",
            "B. Shahraray",
            "L. Rabiner"
          ]
        },
        {
          "title": "Reading checks with multilayer graph transformer networks",
          "year": 1997,
          "citations": 57,
          "abstract": null,
          "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
          "doi": "10.1109/ICASSP.1997.599580",
          "url": "https://www.semanticscholar.org/paper/25de919ccc51093737e2d61cea3037ee6bd3775f",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Yoshua Bengio"
          ]
        },
        {
          "title": "Discriminative feature and model design for automatic speech recognition",
          "year": 1997,
          "citations": 19,
          "abstract": "AUTOMATIC SPEECH RECOGNITION Mazin Rahim, Yoshua Bengio and Yann LeCun AT&T Labs Research, 600 Mountain Avenue, Murray Hill, New Jersey 07974, USA ABSTRACT A system for discriminative feature and model design is presented for automatic speech recognition. Training based on minimum classi cation error with a single objective function is applied for designing a set of parallel networks performing feature transformation and a set of hidden Markov models performing speech recognition. This paper compares the use of linear and non-linear functional transformations when applied to conventional recognition features, such as spectrum or cepstrum. It also provides a framework for integrated feature and model training when using class-speci c transformations. Experimental results on telephone-based connected digit recognition are presented.",
          "venue": "EUROSPEECH",
          "doi": "10.21437/Eurospeech.1997-46",
          "url": "https://www.semanticscholar.org/paper/2a5c494c9ac68c8915d9df880a3bf6fe48a696a8",
          "authors": [
            "M. Rahim",
            "Yoshua Bengio",
            "Yann LeCun"
          ]
        },
        {
          "title": "Global training of document processing systems using graph transformer networks",
          "year": 1997,
          "citations": 122,
          "abstract": null,
          "venue": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.1997.609370",
          "url": "https://www.semanticscholar.org/paper/4f0ab1dcdc9f405ca90e36a35ea335196464d387",
          "authors": [
            "L. Bottou",
            "Yoshua Bengio",
            "Yann LeCun"
          ]
        },
        {
          "title": "Neural Networks and Gradient-Based Learning in OCR",
          "year": 1997,
          "citations": 4,
          "abstract": null,
          "venue": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop",
          "doi": "10.1109/NNSP.1997.622405",
          "url": "https://www.semanticscholar.org/paper/6c3d93bc3e3555319443a1311b6aabbcf3ffd167",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Transformation Invariance in Pattern Recognition-Tangent Distance and Tangent Propagation",
          "year": 1996,
          "citations": 578,
          "abstract": null,
          "venue": "Neural Networks",
          "doi": "10.1007/3-540-49430-8_13",
          "url": "https://www.semanticscholar.org/paper/153f64ab7c1c24b1b136d8da2f36c6333b8dbfdd",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun",
            "J. Denker",
            "B. Victorri"
          ]
        },
        {
          "title": "Effiicient BackProp",
          "year": 1996,
          "citations": 214,
          "abstract": null,
          "venue": "Neural Networks",
          "doi": "10.1007/3-540-49430-8_2",
          "url": "https://www.semanticscholar.org/paper/deede4f010369a810eb1017fd9e757d9876a44c8",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Genevieve B. Orr",
            "K. Müller"
          ]
        },
        {
          "title": "LeRec: A NN/HMM Hybrid for On-Line Handwriting Recognition",
          "year": 1995,
          "citations": 166,
          "abstract": null,
          "venue": "Neural Computation",
          "doi": "10.1162/neco.1995.7.6.1289",
          "url": "https://www.semanticscholar.org/paper/4a4192fd6efb5661eca197cce24289776a4fbcc2",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun",
            "C. Nohl",
            "C. Burges"
          ]
        },
        {
          "title": "Learning algorithms for classification: A comparison on handwritten digit recognition",
          "year": 1995,
          "citations": 568,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/842dd6d0f4b72ce0e8f3ac8e6861637c1f4645ea",
          "authors": [
            "Yann LeCun",
            "L. Jackel",
            "L. Bottou",
            "Corinna Cortes",
            "J. Denker",
            "H. Drucker",
            "Isabelle M Guyon",
            "Urs Muller",
            "E. Sackinger",
            "Patrice Y. Simard",
            "V. Vapnik"
          ]
        },
        {
          "title": "Pattern Recognition and Neural Networks",
          "year": 1995,
          "citations": 4580,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/bf7e1b6997433dcb2121b07cce1de1c61471ff2d",
          "authors": [
            "Yann LeCun",
            "Yoshua Bengio"
          ]
        },
        {
          "title": "Comparison of learning algorithms for handwritten digit recognition",
          "year": 1995,
          "citations": 726,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d50dce749321301f0104689f2dc582303a83be65",
          "authors": [
            "Yann LeCun",
            "L. Jackel",
            "L. Bottou",
            "A. Brunot",
            "Corinna Cortes",
            "J. Denker",
            "H. Drucker",
            "Isabelle M Guyon",
            "Urs Muller",
            "E. Sackinger",
            "Patrice Y. Simard",
            "V. Vapnik"
          ]
        },
        {
          "title": "Word normalization for on-line handwritten word recognition",
          "year": 1994,
          "citations": 38,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/4b336597d72bef3b5b6964a88040129edcaf8dcd",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun"
          ]
        },
        {
          "title": "Comparison of classifier methods: a case study in handwritten digit recognition",
          "year": 1994,
          "citations": 665,
          "abstract": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.",
          "venue": "Signal Processing",
          "doi": "10.1109/ICPR.1994.576879",
          "url": "https://www.semanticscholar.org/paper/55f58ee028e8d86ddf80f68b7538bfb5d6005dc8",
          "authors": [
            "L. Bottou",
            "Corinna Cortes",
            "J. Denker",
            "H. Drucker",
            "Isabelle M Guyon",
            "L. Jackel",
            "Yann LeCun",
            "U. A. Müller",
            "Eduard Säckinger",
            "Patrice Y. Simard",
            "V. Vapnik"
          ]
        },
        {
          "title": "Boosting and Other Ensemble Methods",
          "year": 1994,
          "citations": 406,
          "abstract": "We compare the performance of three types of neural network-based ensemble techniques to that of a single neural network. The ensemble algorithms are two versions of boosting and committees of neural networks trained independently. For each of the four algorithms, we experimentally determine the test and training error curves in an optical character recognition (OCR) problem as both a function of training set size and computational cost using three architectures. We show that a single machine is best for small training set size while for large training set size some version of boosting is best. However, for a given computational cost, boosting is always best. Furthermore, we show a surprising result for the original boosting algorithm: namely, that as the training set size increases, the training error decreases until it asymptotes to the test error rate. This has potential implications in the search for better training algorithms.",
          "venue": "Neural Computation",
          "doi": "10.1162/neco.1994.6.6.1289",
          "url": "https://www.semanticscholar.org/paper/6db07f39446bdf74d0178a75f526baffee1a0369",
          "authors": [
            "H. Drucker",
            "Corinna Cortes",
            "L. Jackel",
            "Yann LeCun",
            "V. Vapnik"
          ]
        },
        {
          "title": "Measuring the VC-Dimension of a Learning Machine",
          "year": 1994,
          "citations": 434,
          "abstract": null,
          "venue": "Neural Computation",
          "doi": "10.1162/neco.1994.6.5.851",
          "url": "https://www.semanticscholar.org/paper/899defb6a100af509547b8d74bb626533ee87da4",
          "authors": [
            "V. Vapnik",
            "E. Levin",
            "Yann LeCun"
          ]
        },
        {
          "title": "Predicting transportpath degradation/failure based on recent performance history",
          "year": 1994,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/96fe89e4f006c85ea3fc743dc507113715128826",
          "authors": [
            "Wan-Ping Chiang",
            "Corinna Cortes",
            "L. Jackel",
            "Yann LeCun",
            "W. Lee",
            "E. Pednault",
            "V. Vapnik"
          ]
        },
        {
          "title": "Boosting and Other Machine Learning Algorithms",
          "year": 1994,
          "citations": 41,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": "10.1016/b978-1-55860-335-6.50015-5",
          "url": "https://www.semanticscholar.org/paper/b560bb5b24cd89a68ce20bd30d4c0f22ccae7d82",
          "authors": [
            "H. Drucker",
            "Corinna Cortes",
            "L. Jackel",
            "Yann LeCun",
            "V. Vapnik"
          ]
        },
        {
          "title": "Memory-based character recognition using a transformation invariant metric",
          "year": 1994,
          "citations": 28,
          "abstract": null,
          "venue": "Signal Processing",
          "doi": "10.1109/ICPR.1994.576916",
          "url": "https://www.semanticscholar.org/paper/be09fa4b8fbf399d1f2ce2263556d90f0661fe1f",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun",
            "J. Denker"
          ]
        },
        {
          "title": "Word-level training of a handwritten word recognizer based on convolutional neural networks",
          "year": 1994,
          "citations": 77,
          "abstract": null,
          "venue": "Signal Processing",
          "doi": "10.1109/ICPR.1994.576881",
          "url": "https://www.semanticscholar.org/paper/ec8c344bb9d1e4b966f499a9b236c3e320d46362",
          "authors": [
            "Yann LeCun",
            "Yoshua Bengio"
          ]
        },
        {
          "title": "Word normalization for online handwritten word recognition",
          "year": 1994,
          "citations": 22,
          "abstract": "We introduce a new approach to normalizing words written with an electronic stylus that applies to all styles of handwriting (upper case, lower case, printed, cursive, or mixed). A geometrical model of the word spatial structure is fitted to the pen trajectory using the expectation-maximisation algorithm. The fitting process maximizes the likelihood of the trajectory given the model and a set a priors on its parameters. The method was evaluated and integrated to a recognition system that combines neural networks and hidden Markov models.",
          "venue": "Signal Processing",
          "doi": "10.1109/ICPR.1994.576966",
          "url": "https://www.semanticscholar.org/paper/f3c5f1e1edd0d63f2d1b30d308c2c88b06a829ef",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun"
          ]
        },
        {
          "title": "Neural Network Applications in Character Recognition and Document Analysis",
          "year": 1994,
          "citations": 11,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/978-1-4615-2734-3_14",
          "url": "https://www.semanticscholar.org/paper/f633dc06f89ab060607202ad96daa85d268055c7",
          "authors": [
            "L. Jackel",
            "M. Y. Battista",
            "J. Ben",
            "J. Bromley",
            "C. Burges",
            "H. Baird",
            "E. Cosatto",
            "J. Denker",
            "H. Graf",
            "H. Katseff",
            "Yann LeCun",
            "C. Nohl",
            "E. Sackinger",
            "J. H. Shamilian",
            "T. Shoemaker",
            "C. E. Stenard",
            "B. I. Strom",
            "R. Ting",
            "T. Wood",
            "C. R. Zuraw"
          ]
        },
        {
          "title": "Off Line Recognition of Handwritten Postal Words Using Neural Networks",
          "year": 1993,
          "citations": 32,
          "abstract": null,
          "venue": "International journal of pattern recognition and artificial intelligence",
          "doi": "10.1142/S0218001493000340",
          "url": "https://www.semanticscholar.org/paper/065b0af1bc05ea4f1fbd2afc50a96b0ef1698c8d",
          "authors": [
            "C. Burges",
            "J. Ben",
            "J. Denker",
            "Yann LeCun",
            "C. Nohl"
          ]
        },
        {
          "title": "Device for recognizing mark",
          "year": 1993,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/25115955f80ea56236cf30704f2ef279287422d3",
          "authors": [
            "Yann LeCun",
            "Quen-Zong Wu",
            "ゾン ウ クエン",
            "ヤン アンドレレカン"
          ]
        },
        {
          "title": "Globally Trained Handwritten Word Recognizer Using Spatial Representation, Convolutional Neural Networks, and Hidden Markov Models",
          "year": 1993,
          "citations": 94,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3461f06617b42dadd1ce240a93ffe420513b3399",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun",
            "D. Henderson"
          ]
        },
        {
          "title": "Device for recognizing mark and method for the same",
          "year": 1993,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/593701c4618ee7c8b89dcfd51639bfd9891dd0a9",
          "authors": [
            "Yann LeCun",
            "Quen-Zong Wu",
            "ゾン ウ クエン",
            "ヤン アンドレレカン"
          ]
        },
        {
          "title": "Globally trained handwritten word recognizer using spatial representation, space displacement neural networks and hidden Markov models",
          "year": 1993,
          "citations": 36,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5fd7cbbecb090f32c198a1b2cc4e2582e06ea431",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun",
            "D. Henderson"
          ]
        },
        {
          "title": "Signature Verification Using A \"Siamese\" Time Delay Neural Network",
          "year": 1993,
          "citations": 3997,
          "abstract": null,
          "venue": "International journal of pattern recognition and artificial intelligence",
          "doi": "10.1142/S0218001493000339",
          "url": "https://www.semanticscholar.org/paper/997dc5d9a058753f034422afe7bd0cc0b8ad808b",
          "authors": [
            "J. Bromley",
            "James W. Bentz",
            "L. Bottou",
            "Isabelle M Guyon",
            "Yann LeCun",
            "Cliff Moore",
            "Eduard Säckinger",
            "Roopak Shah"
          ]
        },
        {
          "title": "On-line handwriting recognition with neural networks: Spatial representation versus temporal representation",
          "year": 1993,
          "citations": 4,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c8a4f5358bde28333ab94076c935bc49c3ff26ea",
          "authors": [
            "Yann LeCun",
            "Yoshua Bengio",
            "D. Henderson",
            "A. Weisbuch",
            "H. Weissman",
            "L. Jackel"
          ]
        },
        {
          "title": "On-line recognition of limited-vocabulary Chinese character using multiple convolutional neural networks",
          "year": 1993,
          "citations": 13,
          "abstract": null,
          "venue": "1993 IEEE International Symposium on Circuits and Systems",
          "doi": "10.1109/ISCAS.1993.394256",
          "url": "https://www.semanticscholar.org/paper/cc74bcd1c131c3fd3edae602bc80b47cfebcd0fa",
          "authors": [
            "Quen-Zong Wu",
            "Yann LeCun",
            "L. Jackel",
            "Bor-Shenn Jeng"
          ]
        },
        {
          "title": "Natural Versus \"universal\" Probability, Complexity, And Entropy",
          "year": 1992,
          "citations": 12,
          "abstract": null,
          "venue": "Workshop on Physics and Computation",
          "doi": "10.1109/PHYCMP.1992.615508",
          "url": "https://www.semanticscholar.org/paper/4096248dca0986be1a6978945a58028e549403d8",
          "authors": [
            "J. Denker",
            "Yann LeCun"
          ]
        },
        {
          "title": "Reading handwritten digits: a ZIP code recognition system",
          "year": 1992,
          "citations": 54,
          "abstract": null,
          "venue": "Computer",
          "doi": "10.1109/2.144441",
          "url": "https://www.semanticscholar.org/paper/52a3dca70cc21b540ad5d7b7b6e23744c20095f7",
          "authors": [
            "O. Matan",
            "H. Baird",
            "J. Bromley",
            "C. Burges",
            "J. Denker",
            "L. Jackel",
            "Yann LeCun",
            "E. Pednault",
            "William Satterfield",
            "C. E. Stenard",
            "Timothy J. Thompson"
          ]
        },
        {
          "title": "Efficient Pattern Recognition Using a New Transformation Distance",
          "year": 1992,
          "citations": 578,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8314dda1ec43ce57ff877f8f02ed89acb68ca035",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun",
            "J. Denker"
          ]
        },
        {
          "title": "An efficient algorithm for learning invariance in adaptive classifiers",
          "year": 1992,
          "citations": 21,
          "abstract": null,
          "venue": "Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems",
          "doi": "10.1109/ICPR.1992.201861",
          "url": "https://www.semanticscholar.org/paper/99c6d1a3e73e454184f81e77563a4cb5810dc430",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun",
            "J. Denker",
            "B. Victorri"
          ]
        },
        {
          "title": "Automatic Learning Rate Maximization by On-Line Estimation of the Hessian's Eigenvectors",
          "year": 1992,
          "citations": 109,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/abc8a30694deda46c150d4da277aec291878cfeb",
          "authors": [
            "Yann LeCun",
            "Patrice Y. Simard",
            "Barak A. Pearlmutter"
          ]
        },
        {
          "title": "Application of the ANNA neural network chip to high-speed character recognition",
          "year": 1992,
          "citations": 144,
          "abstract": null,
          "venue": "IEEE Trans. Neural Networks",
          "doi": "10.1109/72.129422",
          "url": "https://www.semanticscholar.org/paper/b8bc656a1935f07e894833b608cc4671b9fa828f",
          "authors": [
            "Eduard Säckinger",
            "B. Boser",
            "J. Bromley",
            "Yann LeCun",
            "L. Jackel"
          ]
        },
        {
          "title": "Hardware requirements for neural network pattern classifiers: a case study and implementation",
          "year": 1992,
          "citations": 41,
          "abstract": null,
          "venue": "IEEE Micro",
          "doi": "10.1109/40.124378",
          "url": "https://www.semanticscholar.org/paper/bba485f09c82dd2a4261ca9648f2a4cfcd0d343f",
          "authors": [
            "B. Boser",
            "Eduard Säckinger",
            "J. Bromley",
            "Yann LeCun",
            "L. Jackel"
          ]
        },
        {
          "title": "Writer independent and writer adaptive neural network for on-line character recognition",
          "year": 1992,
          "citations": 28,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/de55322662f9538bfdc422498d6184bf2da4266c",
          "authors": [
            "Isabelle M Guyon",
            "D. Henderson",
            "P. Albrecht",
            "Yann LeCun",
            "J. Denker"
          ]
        },
        {
          "title": "Automatic Learning Rate Maximization in Large Adaptive Machines",
          "year": 1992,
          "citations": 15,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/f032d211e9db21d55b92c6349f0bb398142417a2",
          "authors": [
            "Yann LeCun",
            "Patrice Y. Simard",
            "Barak A. Pearlmutter"
          ]
        },
        {
          "title": "Improving generalization performance using double backpropagation",
          "year": 1992,
          "citations": 300,
          "abstract": null,
          "venue": "IEEE Trans. Neural Networks",
          "doi": "10.1109/72.165600",
          "url": "https://www.semanticscholar.org/paper/fbd86e19157ea0e4ffb05f14d7b94603a5667e0a",
          "authors": [
            "H. Drucker",
            "Yann LeCun"
          ]
        },
        {
          "title": "Reverse TDNN: An Architecture For Trajectory Generation",
          "year": 1991,
          "citations": 24,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/040800e88fbdff598fb85ea82c12f94c3939989f",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun"
          ]
        },
        {
          "title": "A neural network approach to handprint character recognition",
          "year": 1991,
          "citations": 11,
          "abstract": null,
          "venue": "COMPCON Spring '91 Digest of Papers",
          "doi": "10.1109/CMPCON.1991.128851",
          "url": "https://www.semanticscholar.org/paper/2fdbed2b8fffa62c86de3e5500590a197ac3327a",
          "authors": [
            "L. Jackel",
            "C. E. Stenard",
            "H. Baird",
            "B. Boser",
            "J. Bromley",
            "C. Burges",
            "J. Denker",
            "H. Graf",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "Yann LeCun",
            "O. Matan",
            "E. Pednault",
            "William Satterfield",
            "Eduard Säckinger",
            "Timothy J. Thompson"
          ]
        },
        {
          "title": "Multi-Digit Recognition Using a Space Displacement Neural Network",
          "year": 1991,
          "citations": 191,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/464e8d981df7f326c3af6e9d7bd627f83e438816",
          "authors": [
            "O. Matan",
            "C. Burges",
            "Yann LeCun",
            "J. Denker"
          ]
        },
        {
          "title": "Constrained neural networks for pattern recognition",
          "year": 1991,
          "citations": 7,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/68e9cab665e8555a5b55fb11f8bd930db32c1283",
          "authors": [
            "S. Solla",
            "Yann LeCun"
          ]
        },
        {
          "title": "An analog neural network processor with programmable topology",
          "year": 1991,
          "citations": 162,
          "abstract": null,
          "venue": "IEEE J. Solid State Circuits",
          "doi": "10.1109/4.104196",
          "url": "https://www.semanticscholar.org/paper/82795cf04f5631e82413b1952625a5a6f21b68ea",
          "authors": [
            "J. Bromley",
            "Yann LeCun",
            "L. Jackel"
          ]
        },
        {
          "title": "Design of a neural network character recognizer for a touch terminal",
          "year": 1991,
          "citations": 177,
          "abstract": null,
          "venue": "Pattern Recognition",
          "doi": "10.1016/0031-3203(91)90081-F",
          "url": "https://www.semanticscholar.org/paper/adf724f637afdb300426df8d2ff4c4342f1e7528",
          "authors": [
            "Isabelle M Guyon",
            "P. Albrecht",
            "Yann LeCun",
            "J. Denker",
            "W. Hubbard"
          ]
        },
        {
          "title": "Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network",
          "year": 1991,
          "citations": 316,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ff32cebbdb8a436ccd8ae797647428615ae32d74",
          "authors": [
            "Patrice Y. Simard",
            "B. Victorri",
            "Yann LeCun",
            "J. Denker"
          ]
        },
        {
          "title": "Second Order Properties of Error Surfaces: Learning Time and Generalization",
          "year": 1990,
          "citations": 107,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0c43153a3627c7d98cc09f909c232f3899597204",
          "authors": [
            "Yann LeCun",
            "I. Kanter",
            "S. Solla"
          ]
        },
        {
          "title": "A time delay neural network character recognizer for a touch terminal",
          "year": 1990,
          "citations": 11,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2c25cb7cb7c0be41510efeaef5cb96dea339823f",
          "authors": [
            "Isabelle M Guyon",
            "P. Albrecht",
            "Yann LeCun",
            "J. Denker",
            "W. Hubbard"
          ]
        },
        {
          "title": "Constrained neural network for unconstrained handwritten digit recognition",
          "year": 1990,
          "citations": 54,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3016c38f8f5f1ad479eb25ef199a5b357c999193",
          "authors": [
            "Yann LeCun",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "L. Jackel",
            "H. Baird"
          ]
        },
        {
          "title": "Handwritten character recognition using neural network architectures",
          "year": 1990,
          "citations": 50,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8f2b909fa1aad7e9f13603d721ff953325a4f97d",
          "authors": [
            "O. Matan",
            "R. Kiang",
            "C. E. Stenard",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "L. Jackel",
            "Yann LeCun"
          ]
        },
        {
          "title": "Handwritten zip code recognition with multilayer networks",
          "year": 1990,
          "citations": 233,
          "abstract": null,
          "venue": "[1990] Proceedings. 10th International Conference on Pattern Recognition",
          "doi": "10.1109/ICPR.1990.119325",
          "url": "https://www.semanticscholar.org/paper/a4bfe622ab32e6c645eb4be2079734355b22d304",
          "authors": [
            "Yann LeCun",
            "O. Matan",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "L. D. Jacket",
            "H. Baird"
          ]
        },
        {
          "title": "Second Order Properties of Error Surfaces",
          "year": 1990,
          "citations": 13,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d39adfe1d561f676b2075ac36c59405146533b0b",
          "authors": [
            "Yann LeCun",
            "I. Kanter",
            "S. Solla"
          ]
        },
        {
          "title": "Hardware requirements for neural-net optical character recognition",
          "year": 1990,
          "citations": 12,
          "abstract": null,
          "venue": "1990 IJCNN International Joint Conference on Neural Networks",
          "doi": "10.1109/IJCNN.1990.137801",
          "url": "https://www.semanticscholar.org/paper/dab9de4747d1286c456f36ec69415c5d668889aa",
          "authors": [
            "L. Jackel",
            "B. Boser",
            "J. Denker",
            "H. Graf",
            "Yann LeCun",
            "Isabelle M Guyon",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "S. Solla"
          ]
        },
        {
          "title": "Transforming Neural-Net Output Levels to Probability Distributions",
          "year": 1990,
          "citations": 332,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e4b370c1a04a8a6807bd73b6bbff5773e575fee7",
          "authors": [
            "J. Denker",
            "Yann LeCun"
          ]
        },
        {
          "title": "Generalization and network design strategies",
          "year": 1989,
          "citations": 1091,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/01b6affe3ea4eae1978aec54e87087feb76d9215",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Handwritten digit recognition: applications of neural network chips and automatic learning",
          "year": 1989,
          "citations": 475,
          "abstract": null,
          "venue": "IEEE Communications Magazine",
          "doi": "10.1109/35.41400",
          "url": "https://www.semanticscholar.org/paper/3aa4c691289f56f9af6cf543633cfb3917274281",
          "authors": [
            "Yann LeCun",
            "L. Jackel",
            "B. Boser",
            "J. Denker",
            "H. Graf",
            "Isabelle M Guyon",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard"
          ]
        },
        {
          "title": "Improving the convergence of back-propagation learning with second-order methods",
          "year": 1989,
          "citations": 444,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/589d377b23e2bdae7ad161b36a5d6613bcfccdde",
          "authors": [
            "S. Becker",
            "Yann LeCun"
          ]
        },
        {
          "title": "Handwritten Digit Recognition with a Back-Propagation Network",
          "year": 1989,
          "citations": 4237,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
          "authors": [
            "Yann LeCun",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "L. Jackel"
          ]
        },
        {
          "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
          "year": 1989,
          "citations": 11653,
          "abstract": null,
          "venue": "Neural Computation",
          "doi": "10.1162/neco.1989.1.4.541",
          "url": "https://www.semanticscholar.org/paper/a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
          "authors": [
            "Yann LeCun",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "L. Jackel"
          ]
        },
        {
          "title": "Handwritten Digit Recognition: Applications of Neural Net Chips and Automatic Learning",
          "year": 1989,
          "citations": 30,
          "abstract": null,
          "venue": "NATO Neurocomputing",
          "doi": "10.1007/978-3-642-76153-9_35",
          "url": "https://www.semanticscholar.org/paper/cdefacc5f4e4292936ea9bd542e0a46c6c49905c",
          "authors": [
            "Yann LeCun",
            "L. Jackel",
            "B. Boser",
            "J. Denker",
            "H. Graf",
            "Isabelle M Guyon",
            "D. Henderson",
            "R. E. Howard",
            "W. Hubbard"
          ]
        },
        {
          "title": "Optimal Brain Damage",
          "year": 1989,
          "citations": 5047,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e7297db245c3feb1897720b173a59fe7e36babb7",
          "authors": [
            "Yann LeCun",
            "J. Denker",
            "S. Solla"
          ]
        },
        {
          "title": "GEMINI: Gradient Estimation Through Matrix Inversion After Noise Injection",
          "year": 1988,
          "citations": 32,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/19ec5c7be1fe5e7088f3a042d3160ede757f5902",
          "authors": [
            "Yann LeCun",
            "C. Galland",
            "Geoffrey E. Hinton"
          ]
        },
        {
          "title": "Sn: A simulator for connectionist models",
          "year": 1988,
          "citations": 17,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5b5461d8ba70ecd7358b6edd8f39bda711f73a69",
          "authors": [
            "L. Bottou",
            "Yann LeCun"
          ]
        },
        {
          "title": "User manual: SN: A simulator for connectionist models",
          "year": 1988,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b2819d429bb7e76323ef3e836cbb608b7f700ab8",
          "authors": [
            "Yann LeCun",
            "L. Bottou"
          ]
        },
        {
          "title": "Using curvature information to improve back-propagation",
          "year": 1988,
          "citations": 2,
          "abstract": null,
          "venue": "Neural Networks",
          "doi": "10.1016/0893-6080(88)90205-5",
          "url": "https://www.semanticscholar.org/paper/e149ea080a0c8ed23319a0ae33260f4821696675",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning on automata networks",
          "year": 1987,
          "citations": 1,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3768e0e1c2c74b5049d79bfccad04db0e31954e8",
          "authors": [
            "F. F. Soulié",
            "P. Gallinari",
            "Yann LeCun",
            "S. Thiria"
          ]
        },
        {
          "title": "Generalization using back-propagation",
          "year": 1987,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/745a9bf17a4db5b91e1bef2e083a9ad5c08ec282",
          "authors": [
            "F. F. Soulié",
            "P. Gallinari",
            "Yann LeCun",
            "S. Thiria"
          ]
        },
        {
          "title": "Memoires associatives distribuees: Une comparaison (Distributed associative memories: A comparison)",
          "year": 1987,
          "citations": 22,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b667bca895b762611fe65929421e66181c7f23bc",
          "authors": [
            "P. Gallinari",
            "Yann LeCun",
            "S. Thiria",
            "F. F. Soulié"
          ]
        },
        {
          "title": "Learning processes in an asymmetric threshold network",
          "year": 1986,
          "citations": 125,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/56b771c4c3a54910dc3e7ff838940de89ed282db",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Une procedure d'apprentissage pour reseau a seuil asymmetrique (A learning scheme for asymmetric threshold networks)",
          "year": 1985,
          "citations": 223,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d007ed936c51a700d8c65d1bbfae7acc83783c31",
          "authors": [
            "Yann LeCun"
          ]
        },
        {
          "title": "Learning by teaching.",
          "year": 1977,
          "citations": 613,
          "abstract": null,
          "venue": "The Australian nurses' journal. Royal Australian Nursing Federation",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d889ec784423556c29f33060e44506ebdd80b8a0",
          "authors": [
            "Cecilia Ka Yuk Chan",
            "Z. Zorina",
            "Jesse R. Sparks",
            "S. López Ornat",
            "C. Tsang",
            "E. Grigorenko",
            "R. Lubow",
            "J. Malone",
            "M. Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "T. Minor",
            "Traci N. Plumb",
            "H. Drachsler",
            "P. Kirschner",
            "M. Nakayama",
            "Rowena Santiago",
            "Ali Şimşek",
            "Fang‐Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "Richard P. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "L. C. Lamb",
            "Artur d’Avila Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "Tracy Packiam Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "G. Kern-Isberner",
            "J. Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "CM Olsson",
            "L. Nyberg",
            "R. Freivalds",
            "L. Hall",
            "M. Hall",
            "Ulrike Hanke",
            "L. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "P. Burnett",
            "M. Cannon",
            "A. Edmondson",
            "P. Hartono",
            "A. Callender",
            "Rachel Barr",
            "N. Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohd. Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "D. Kolb",
            "R. Reilly",
            "K. Nielsen",
            "P. Couvillon",
            "H. King",
            "J. Dillon",
            "D. Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "P. C. Lane",
            "S. Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "D. Tucker",
            "P. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "D. Ansari",
            "K. Lionello-DeNolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo P. Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. Hoppe",
            "Krisztina Molnár",
            "Erica de Vries",
            "E. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "T. Cazenave",
            "T. Jong",
            "J. Meij",
            "J. Gavelek",
            "Ailing Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "M. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "D. Seo",
            "M. Monfils",
            "A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "M. Kennedy",
            "Michael C. Jackson",
            "Terezinha Nunes",
            "B. Jackling",
            "R. Howard",
            "William G. Kennedy",
            "L. C. Drickamer"
          ]
        },
        {
          "title": "General Information Registration & Reception",
          "year": null,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2c3e28967d25ef2811e3675cf9d3ab244901e63b",
          "authors": [
            "B. Dosher",
            "Yann LeCun",
            "W. Estes",
            "J. Kujala",
            "A. Z. Scholten"
          ]
        },
        {
          "title": "Energy-based Models in Document Recognition and Computer Vision. 1. Two Challenges in Machine Learning",
          "year": null,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/9adb3a467cdc39637aca2a88a988d543cf45c72d",
          "authors": [
            "Yann LeCun",
            "S. Chopra",
            "Aurelio Marc",
            "Fu-Jie Ranzato",
            "Huang"
          ]
        }
      ],
      "top_coauthors": [
        {
          "name": "L. Bottou",
          "collaborations": 34
        },
        {
          "name": "J. Denker",
          "collaborations": 29
        },
        {
          "name": "K. Kavukcuoglu",
          "collaborations": 26
        },
        {
          "name": "L. Jackel",
          "collaborations": 25
        },
        {
          "name": "Marc'Aurelio Ranzato",
          "collaborations": 24
        },
        {
          "name": "Yoshua Bengio",
          "collaborations": 22
        },
        {
          "name": "P. Sermanet",
          "collaborations": 19
        },
        {
          "name": "Urs Muller",
          "collaborations": 18
        },
        {
          "name": "R. Howard",
          "collaborations": 17
        },
        {
          "name": "R. Hadsell",
          "collaborations": 17
        }
      ]
    },
    "combined": {
      "name": "Yann LeCun",
      "affiliation": "Facebook",
      "orcid": null,
      "homepage": null,
      "interests": [],
      "metrics": {
        "citations": 259193,
        "h_index": 137,
        "i10_index": 0,
        "publication_count": 405
      },
      "sources_found": [
        "semantic_scholar"
      ],
      "total_publications_all_sources": 405,
      "publications": [
        {
          "title": "Gradient-based learning applied to document recognition",
          "year": 1998,
          "citations": 57263,
          "abstract": null,
          "venue": "Proceedings of the IEEE",
          "doi": "10.1109/5.726791",
          "url": "https://www.semanticscholar.org/paper/162d958ff885f1462aeda91cd72582323fd6a1f4",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Yoshua Bengio",
            "P. Haffner"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Deep Learning",
          "year": 2015,
          "citations": 39366,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb",
          "authors": [
            "Yann LeCun",
            "Yoshua Bengio",
            "Geoffrey E. Hinton"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
          "year": 1989,
          "citations": 11653,
          "abstract": null,
          "venue": "Neural Computation",
          "doi": "10.1162/neco.1989.1.4.541",
          "url": "https://www.semanticscholar.org/paper/a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
          "authors": [
            "Yann LeCun",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "L. Jackel"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The mnist database of handwritten digits",
          "year": 2005,
          "citations": 7095,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2",
          "authors": [
            "Yann LeCun",
            "Corinna Cortes"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Character-level Convolutional Networks for Text Classification",
          "year": 2015,
          "citations": 6578,
          "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/51a55df1f023571a7e07e338ee45a3e3d66ef73e",
          "authors": [
            "Xiang Zhang",
            "J. Zhao",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Convolutional networks for images, speech, and time series",
          "year": 1998,
          "citations": 5983,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/563e821bb5ea825efb56b77484f5287f08cf3753",
          "authors": [
            "Yann LeCun",
            "Yoshua Bengio"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Dimensionality Reduction by Learning an Invariant Mapping",
          "year": 2006,
          "citations": 5509,
          "abstract": null,
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2006.100",
          "url": "https://www.semanticscholar.org/paper/46f30e94dd3d5902141c5fbe58d0bc9189545c76",
          "authors": [
            "R. Hadsell",
            "S. Chopra",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Spectral Networks and Locally Connected Networks on Graphs",
          "year": 2013,
          "citations": 5173,
          "abstract": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5e925a9f1e20df61d1e860a7aa71894b35a1c186",
          "authors": [
            "Joan Bruna",
            "Wojciech Zaremba",
            "Arthur Szlam",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
          "year": 2013,
          "citations": 5090,
          "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/1109b663453e78a59e4f66446d71720ac58cec25",
          "authors": [
            "P. Sermanet",
            "D. Eigen",
            "Xiang Zhang",
            "Michaël Mathieu",
            "R. Fergus",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Optimal Brain Damage",
          "year": 1989,
          "citations": 5047,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e7297db245c3feb1897720b173a59fe7e36babb7",
          "authors": [
            "Yann LeCun",
            "J. Denker",
            "S. Solla"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Pattern Recognition and Neural Networks",
          "year": 1995,
          "citations": 4580,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/bf7e1b6997433dcb2121b07cce1de1c61471ff2d",
          "authors": [
            "Yann LeCun",
            "Yoshua Bengio"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning a similarity metric discriminatively, with application to face verification",
          "year": 2005,
          "citations": 4428,
          "abstract": null,
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2005.202",
          "url": "https://www.semanticscholar.org/paper/cfaae9b6857b834043606df3342d8dc97524aa9d",
          "authors": [
            "S. Chopra",
            "R. Hadsell",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Handwritten Digit Recognition with a Back-Propagation Network",
          "year": 1989,
          "citations": 4237,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
          "authors": [
            "Yann LeCun",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "L. Jackel"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Signature Verification Using A \"Siamese\" Time Delay Neural Network",
          "year": 1993,
          "citations": 3997,
          "abstract": null,
          "venue": "International journal of pattern recognition and artificial intelligence",
          "doi": "10.1142/S0218001493000339",
          "url": "https://www.semanticscholar.org/paper/997dc5d9a058753f034422afe7bd0cc0b8ad808b",
          "authors": [
            "J. Bromley",
            "James W. Bentz",
            "L. Bottou",
            "Isabelle M Guyon",
            "Yann LeCun",
            "Cliff Moore",
            "Eduard Säckinger",
            "Roopak Shah"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Geometric Deep Learning: Going beyond Euclidean data",
          "year": 2017,
          "citations": 3562,
          "abstract": "Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",
          "venue": "IEEE Signal Processing Magazine",
          "doi": "10.1109/MSP.2017.2693418",
          "url": "https://www.semanticscholar.org/paper/7c60897979654a5838638df7a8aa80142a2ff4bd",
          "authors": [
            "M. Bronstein",
            "Joan Bruna",
            "Yann LeCun",
            "Arthur Szlam",
            "P. Vandergheynst"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Efficient BackProp",
          "year": 2012,
          "citations": 3363,
          "abstract": null,
          "venue": "Neural Networks",
          "doi": "10.1007/978-3-642-35289-8_3",
          "url": "https://www.semanticscholar.org/paper/b87274e6d9aa4e6ba5148898aa92941617d2b6ed",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Genevieve B. Orr",
            "K. Müller"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition",
          "year": 2017,
          "citations": 3313,
          "abstract": "In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block \"R(2+1)D\" which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.",
          "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2018.00675",
          "url": "https://www.semanticscholar.org/paper/89c3050522a0bb9820c32dc7444e003ef0d3e2e4",
          "authors": [
            "Du Tran",
            "Heng Wang",
            "L. Torresani",
            "Jamie Ray",
            "Yann LeCun",
            "Manohar Paluri"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning Hierarchical Features for Scene Labeling",
          "year": 2013,
          "citations": 2757,
          "abstract": null,
          "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
          "doi": "10.1109/TPAMI.2012.231",
          "url": "https://www.semanticscholar.org/paper/237a04dd8291cbdb59b6dc4b53e689af743fe2a3",
          "authors": [
            "C. Farabet",
            "C. Couprie",
            "Laurent Najman",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
          "year": 2021,
          "citations": 2675,
          "abstract": "Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8a9d84d86ac0d76e63914802f9738325c3bece9c",
          "authors": [
            "Jure Zbontar",
            "Li Jing",
            "Ishan Misra",
            "Yann LeCun",
            "Stéphane Deny"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Regularization of Neural Networks using DropConnect",
          "year": 2013,
          "citations": 2608,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/38f35dd624cd1cf827416e31ac5e0e0454028eca",
          "authors": [
            "Li Wan",
            "Matthew D. Zeiler",
            "Sixin Zhang",
            "Yann LeCun",
            "R. Fergus"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "What is the best multi-stage architecture for object recognition?",
          "year": 2009,
          "citations": 2358,
          "abstract": null,
          "venue": "IEEE International Conference on Computer Vision",
          "doi": "10.1109/ICCV.2009.5459469",
          "url": "https://www.semanticscholar.org/paper/1f88427d7aa8225e47f946ac41a0667d7b69ac52",
          "authors": [
            "Kevin Jarrett",
            "K. Kavukcuoglu",
            "Marc'Aurelio Ranzato",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Convolutional networks and applications in vision",
          "year": 2010,
          "citations": 2075,
          "abstract": null,
          "venue": "Proceedings of 2010 IEEE International Symposium on Circuits and Systems",
          "doi": "10.1109/ISCAS.2010.5537907",
          "url": "https://www.semanticscholar.org/paper/c43025c429b1fbf6f1379f61801a1b40834d62e7",
          "authors": [
            "Yann LeCun",
            "K. Kavukcuoglu",
            "C. Farabet"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Deep multi-scale video prediction beyond mean square error",
          "year": 2015,
          "citations": 1939,
          "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681",
          "authors": [
            "Michaël Mathieu",
            "C. Couprie",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning Fast Approximations of Sparse Coding",
          "year": 2010,
          "citations": 1889,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e8f811399746c059bf4d4c3d43334045e0222209",
          "authors": [
            "Karol Gregor",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation",
          "year": 2014,
          "citations": 1760,
          "abstract": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2 x, while keeping the accuracy within 1% of the original model.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e5ae8ab688051931b4814f6d32b18391f8d1fa8d",
          "authors": [
            "Emily L. Denton",
            "Wojciech Zaremba",
            "Joan Bruna",
            "Yann LeCun",
            "R. Fergus"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Tutorial on Energy-Based Learning",
          "year": 2006,
          "citations": 1642,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7fc604e1a3e45cd2d2742f96d62741930a363efa",
          "authors": [
            "Yann LeCun",
            "S. Chopra",
            "R. Hadsell",
            "Aurelio Ranzato",
            "Fu Jie Huang"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Deep Convolutional Networks on Graph-Structured Data",
          "year": 2015,
          "citations": 1632,
          "abstract": "Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. \nIn this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e49ff72d420c8d72e62a9353e3abc053445e59bd",
          "authors": [
            "Mikael Henaff",
            "Joan Bruna",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation",
          "year": 2014,
          "citations": 1582,
          "abstract": "This paper proposes a new hybrid architecture that consists of a deep Convolu-tional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/12ecc2d786080f638a01b9999518e9386baa157d",
          "authors": [
            "Jonathan Tompson",
            "Arjun Jain",
            "Yann LeCun",
            "C. Bregler"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning methods for generic object recognition with invariance to pose and lighting",
          "year": 2004,
          "citations": 1565,
          "abstract": null,
          "venue": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.",
          "doi": "10.1109/CVPR.2004.144",
          "url": "https://www.semanticscholar.org/paper/f354310098e09c1e1dc88758fca36767fd9d084d",
          "authors": [
            "Yann LeCun",
            "Fu Jie Huang",
            "L. Bottou"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches",
          "year": 2015,
          "citations": 1428,
          "abstract": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.",
          "venue": "Journal of machine learning research",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ad3e7515c61ccdc2c36887e7d4929c78904881db",
          "authors": [
            "Jure Zbontar",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Efficient object localization using Convolutional Networks",
          "year": 2014,
          "citations": 1413,
          "abstract": "Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPII-human-pose dataset [1].",
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2015.7298664",
          "url": "https://www.semanticscholar.org/paper/ebcea2d842d3d4e320500086aff0deb4cb4412ff",
          "authors": [
            "Jonathan Tompson",
            "Ross Goroshin",
            "Arjun Jain",
            "Yann LeCun",
            "C. Bregler"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Scaling learning algorithms towards AI",
          "year": 2007,
          "citations": 1368,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6fdb77260fc83dff91c44fea0f31a2cb8ed13d04",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Theoretical Analysis of Feature Pooling in Visual Recognition",
          "year": 2010,
          "citations": 1366,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/405aed4b8ecdd869b2e83095dde51c396334115f",
          "authors": [
            "Y-Lan Boureau",
            "J. Ponce",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Efficient Learning of Sparse Representations with an Energy-Based Model",
          "year": 2006,
          "citations": 1342,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": "10.7551/mitpress/7503.003.0147",
          "url": "https://www.semanticscholar.org/paper/932c2a02d462abd75af018125413b1ceaa1ee3f4",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Christopher S. Poultney",
            "S. Chopra",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The Loss Surfaces of Multilayer Networks",
          "year": 2014,
          "citations": 1247,
          "abstract": "We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between largeand small-size networks where for the latter poor quality local minima have nonzero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.",
          "venue": "International Conference on Artificial Intelligence and Statistics",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ad8a12a19e74d9788f8fe92f5c0dfea7b6a52aba",
          "authors": [
            "A. Choromańska",
            "Mikael Henaff",
            "Michaël Mathieu",
            "G. B. Arous",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition",
          "year": 2007,
          "citations": 1190,
          "abstract": null,
          "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2007.383157",
          "url": "https://www.semanticscholar.org/paper/ccd52aff02b0f902f4ce7247c4fee7273014c41c",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Fu Jie Huang",
            "Y-Lan Boureau",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning mid-level features for recognition",
          "year": 2010,
          "citations": 1157,
          "abstract": null,
          "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2010.5539963",
          "url": "https://www.semanticscholar.org/paper/498efaa51f5eda731dc6199c3547b9465717fa68",
          "authors": [
            "Y-Lan Boureau",
            "F. Bach",
            "Yann LeCun",
            "J. Ponce"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Energy-based Generative Adversarial Network",
          "year": 2016,
          "citations": 1131,
          "abstract": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2ba23d9b46027e47b4483243871760e315213ffe",
          "authors": [
            "J. Zhao",
            "Michaël Mathieu",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Generalization and network design strategies",
          "year": 1989,
          "citations": 1091,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/01b6affe3ea4eae1978aec54e87087feb76d9215",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning",
          "year": 2021,
          "citations": 1074,
          "abstract": "Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0d0cf5f64c052aa7edc5bb638203616a620557f6",
          "authors": [
            "Adrien Bardes",
            "J. Ponce",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Object Recognition with Gradient-Based Learning",
          "year": 1999,
          "citations": 1027,
          "abstract": null,
          "venue": "Shape, Contour and Grouping in Computer Vision",
          "doi": "10.1007/3-540-46805-6_19",
          "url": "https://www.semanticscholar.org/paper/9a5ea367f0fb05805acaa84a402f5d036eea37dc",
          "authors": [
            "Yann LeCun",
            "P. Haffner",
            "L. Bottou",
            "Yoshua Bengio"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding",
          "year": 2021,
          "citations": 1023,
          "abstract": "Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.",
          "venue": "IEEE International Conference on Computer Vision",
          "doi": "10.1109/ICCV48922.2021.00180",
          "url": "https://www.semanticscholar.org/paper/7ba9c013988eaff5cd186d73704af329d027872d",
          "authors": [
            "Aishwarya Kamath",
            "Mannat Singh",
            "Yann LeCun",
            "Ishan Misra",
            "Gabriel Synnaeve",
            "Nicolas Carion"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Very Deep Convolutional Networks for Text Classification",
          "year": 2016,
          "citations": 999,
          "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.",
          "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
          "doi": "10.18653/V1/E17-1104",
          "url": "https://www.semanticscholar.org/paper/f797fd44b9ddd5845611eb7a705ca9464a8819d1",
          "authors": [
            "Holger Schwenk",
            "Loïc Barrault",
            "Alexis Conneau",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Sparse Feature Learning for Deep Belief Networks",
          "year": 2007,
          "citations": 939,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/41fef1a197fab9684a4608b725d3ae72e1ab4b39",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Pedestrian Detection with Unsupervised Multi-stage Feature Learning",
          "year": 2012,
          "citations": 832,
          "abstract": "Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.",
          "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2013.465",
          "url": "https://www.semanticscholar.org/paper/a1306ce652f556fbb9e794d91084a29294298e6d",
          "authors": [
            "P. Sermanet",
            "K. Kavukcuoglu",
            "Soumith Chintala",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Entropy-SGD: biasing gradient descent into wide valleys",
          "year": 2016,
          "citations": 825,
          "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.",
          "venue": "International Conference on Learning Representations",
          "doi": "10.1088/1742-5468/ab39d9",
          "url": "https://www.semanticscholar.org/paper/b6583fe9c9dc52bb129aff4cefc60519349f3b4c",
          "authors": [
            "P. Chaudhari",
            "A. Choromańska",
            "Stefano Soatto",
            "Yann LeCun",
            "Carlo Baldassi",
            "C. Borgs",
            "J. Chayes",
            "Levent Sagun",
            "R. Zecchina"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks",
          "year": 2014,
          "citations": 801,
          "abstract": null,
          "venue": "ACM Transactions on Graphics",
          "doi": "10.1145/2629500",
          "url": "https://www.semanticscholar.org/paper/ae639e4bdd2e6a11bc44ff7f1ae53cd25462042b",
          "authors": [
            "Jonathan Tompson",
            "Murphy Stein",
            "Yann LeCun",
            "K. Perlin"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Computing the stereo matching cost with a convolutional neural network",
          "year": 2014,
          "citations": 796,
          "abstract": "We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61% on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset.",
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2015.7298767",
          "url": "https://www.semanticscholar.org/paper/2fdd82708a99cec2bcd45c2c7f337a9beccced04",
          "authors": [
            "Jure Zbontar",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Traffic sign recognition with multi-scale Convolutional Networks",
          "year": 2011,
          "citations": 795,
          "abstract": null,
          "venue": "The 2011 International Joint Conference on Neural Networks",
          "doi": "10.1109/IJCNN.2011.6033589",
          "url": "https://www.semanticscholar.org/paper/9ab0de951cc9cdf16887b1f841f8da6affc9c0de",
          "authors": [
            "P. Sermanet",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Comparison of learning algorithms for handwritten digit recognition",
          "year": 1995,
          "citations": 726,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d50dce749321301f0104689f2dc582303a83be65",
          "authors": [
            "Yann LeCun",
            "L. Jackel",
            "L. Bottou",
            "A. Brunot",
            "Corinna Cortes",
            "J. Denker",
            "H. Drucker",
            "Isabelle M Guyon",
            "Urs Muller",
            "E. Sackinger",
            "Patrice Y. Simard",
            "V. Vapnik"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Convolutional Learning of Spatio-temporal Features",
          "year": 2010,
          "citations": 697,
          "abstract": null,
          "venue": "European Conference on Computer Vision",
          "doi": "10.1007/978-3-642-15567-3_11",
          "url": "https://www.semanticscholar.org/paper/4d476b96be73fccc61f2076befbf5a468caa4180",
          "authors": [
            "Graham W. Taylor",
            "R. Fergus",
            "Yann LeCun",
            "C. Bregler"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Comparison of classifier methods: a case study in handwritten digit recognition",
          "year": 1994,
          "citations": 665,
          "abstract": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.",
          "venue": "Signal Processing",
          "doi": "10.1109/ICPR.1994.576879",
          "url": "https://www.semanticscholar.org/paper/55f58ee028e8d86ddf80f68b7538bfb5d6005dc8",
          "authors": [
            "L. Bottou",
            "Corinna Cortes",
            "J. Denker",
            "H. Drucker",
            "Isabelle M Guyon",
            "L. Jackel",
            "Yann LeCun",
            "U. A. Müller",
            "Eduard Säckinger",
            "Patrice Y. Simard",
            "V. Vapnik"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Deep learning with Elastic Averaging SGD",
          "year": 2014,
          "citations": 630,
          "abstract": "We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d1e4365de165463e51134f10bf3939f2b00a6667",
          "authors": [
            "Sixin Zhang",
            "A. Choromańska",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Fast Training of Convolutional Networks through FFTs",
          "year": 2013,
          "citations": 628,
          "abstract": "Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/a7621b4ec18719b08f3a2a444b6d37a2e20227b7",
          "authors": [
            "Michaël Mathieu",
            "Mikael Henaff",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning by teaching.",
          "year": 1977,
          "citations": 613,
          "abstract": null,
          "venue": "The Australian nurses' journal. Royal Australian Nursing Federation",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d889ec784423556c29f33060e44506ebdd80b8a0",
          "authors": [
            "Cecilia Ka Yuk Chan",
            "Z. Zorina",
            "Jesse R. Sparks",
            "S. López Ornat",
            "C. Tsang",
            "E. Grigorenko",
            "R. Lubow",
            "J. Malone",
            "M. Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "T. Minor",
            "Traci N. Plumb",
            "H. Drachsler",
            "P. Kirschner",
            "M. Nakayama",
            "Rowena Santiago",
            "Ali Şimşek",
            "Fang‐Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "Richard P. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "L. C. Lamb",
            "Artur d’Avila Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "Tracy Packiam Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "G. Kern-Isberner",
            "J. Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "CM Olsson",
            "L. Nyberg",
            "R. Freivalds",
            "L. Hall",
            "M. Hall",
            "Ulrike Hanke",
            "L. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "P. Burnett",
            "M. Cannon",
            "A. Edmondson",
            "P. Hartono",
            "A. Callender",
            "Rachel Barr",
            "N. Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohd. Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "D. Kolb",
            "R. Reilly",
            "K. Nielsen",
            "P. Couvillon",
            "H. King",
            "J. Dillon",
            "D. Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "P. C. Lane",
            "S. Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "D. Tucker",
            "P. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "D. Ansari",
            "K. Lionello-DeNolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo P. Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. Hoppe",
            "Krisztina Molnár",
            "Erica de Vries",
            "E. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "T. Cazenave",
            "T. Jong",
            "J. Meij",
            "J. Gavelek",
            "Ailing Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "M. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "D. Seo",
            "M. Monfils",
            "A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "M. Kennedy",
            "Michael C. Jackson",
            "Terezinha Nunes",
            "B. Jackling",
            "R. Howard",
            "William G. Kennedy",
            "L. C. Drickamer"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Off-Road Obstacle Avoidance through End-to-End Learning",
          "year": 2005,
          "citations": 609,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c63d2e3b11d25971cc50cde869f59d34c62a1291",
          "authors": [
            "Yann LeCun",
            "Urs Muller",
            "J. Ben",
            "E. Cosatto",
            "B. Flepp"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning Convolutional Feature Hierarchies for Visual Recognition",
          "year": 2010,
          "citations": 594,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8b25a44f617c1ed3ed52c6655b0d456ff1c565bd",
          "authors": [
            "K. Kavukcuoglu",
            "P. Sermanet",
            "Y-Lan Boureau",
            "Karol Gregor",
            "Michaël Mathieu",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Transformation Invariance in Pattern Recognition-Tangent Distance and Tangent Propagation",
          "year": 1996,
          "citations": 578,
          "abstract": null,
          "venue": "Neural Networks",
          "doi": "10.1007/3-540-49430-8_13",
          "url": "https://www.semanticscholar.org/paper/153f64ab7c1c24b1b136d8da2f36c6333b8dbfdd",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun",
            "J. Denker",
            "B. Victorri"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Efficient Pattern Recognition Using a New Transformation Distance",
          "year": 1992,
          "citations": 578,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8314dda1ec43ce57ff877f8f02ed89acb68ca035",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun",
            "J. Denker"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning algorithms for classification: A comparison on handwritten digit recognition",
          "year": 1995,
          "citations": 568,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/842dd6d0f4b72ce0e8f3ac8e6861637c1f4645ea",
          "authors": [
            "Yann LeCun",
            "L. Jackel",
            "L. Bottou",
            "Corinna Cortes",
            "J. Denker",
            "H. Drucker",
            "Isabelle M Guyon",
            "Urs Muller",
            "E. Sackinger",
            "Patrice Y. Simard",
            "V. Vapnik"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Text Understanding from Scratch",
          "year": 2015,
          "citations": 564,
          "abstract": "This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/f1b0df6b2977d28e55df82576b108e4f5d87e044",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Deep learning for AI",
          "year": 2021,
          "citations": 560,
          "abstract": "How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language?",
          "venue": "Communications of the ACM",
          "doi": "10.1145/3448250",
          "url": "https://www.semanticscholar.org/paper/87d5b61f5b6fdb0a57fc66b5c5bb428c398eaa86",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun",
            "Geoffrey E. Hinton"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Convolutional neural networks applied to house numbers digit classification",
          "year": 2012,
          "citations": 551,
          "abstract": "We classify digits of real-world house numbers using convolutional neural networks (ConvNets). Con-vNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 95.10% accuracy on the SVHN dataset (48% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.",
          "venue": "International Conference on Pattern Recognition",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/9f7f9aba0a6a966ce04e29e401ea28f9eae82f02",
          "authors": [
            "P. Sermanet",
            "Soumith Chintala",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27",
          "year": 2022,
          "citations": 526,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/775f42ed458b8c5b0f2094ea4ff5b64c557b1a34",
          "authors": [
            "Yann LeCun",
            "Courant"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture",
          "year": 2023,
          "citations": 518,
          "abstract": "This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.",
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR52729.2023.01499",
          "url": "https://www.semanticscholar.org/paper/ee57e4d7a125f4ca8916284a857c3760d7d378d3",
          "authors": [
            "Mahmoud Assran",
            "Quentin Duval",
            "Ishan Misra",
            "Piotr Bojanowski",
            "Pascal Vincent",
            "Michael G. Rabbat",
            "Yann LeCun",
            "Nicolas Ballas"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Disentangling factors of variation in deep representation using adversarial training",
          "year": 2016,
          "citations": 509,
          "abstract": "We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentaglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/48a789a15e3572749905f036fcbf3310aee29b79",
          "authors": [
            "Michaël Mathieu",
            "J. Zhao",
            "P. Sprechmann",
            "A. Ramesh",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Indoor Semantic Segmentation using depth information",
          "year": 2013,
          "citations": 497,
          "abstract": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8f3c7fc72635c3b95dc3530fdaceee1d4681548e",
          "authors": [
            "C. Couprie",
            "C. Farabet",
            "Laurent Najman",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "No more pesky learning rates",
          "year": 2012,
          "citations": 487,
          "abstract": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e5a685f40338f9c2f3e68e142efa217aad16dd56",
          "authors": [
            "T. Schaul",
            "Sixin Zhang",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Handwritten digit recognition: applications of neural network chips and automatic learning",
          "year": 1989,
          "citations": 475,
          "abstract": null,
          "venue": "IEEE Communications Magazine",
          "doi": "10.1109/35.41400",
          "url": "https://www.semanticscholar.org/paper/3aa4c691289f56f9af6cf543633cfb3917274281",
          "authors": [
            "Yann LeCun",
            "L. Jackel",
            "B. Boser",
            "J. Denker",
            "H. Graf",
            "Isabelle M Guyon",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Augmented Language Models: a Survey",
          "year": 2023,
          "citations": 467,
          "abstract": "This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.",
          "venue": "Trans. Mach. Learn. Res.",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2029349c55c1dba3493c5b3bd25152f18ba21ae2",
          "authors": [
            "G. Mialon",
            "Roberto Dessì",
            "M. Lomeli",
            "Christoforos Nalmpantis",
            "Ramakanth Pasunuru",
            "R. Raileanu",
            "Baptiste Rozière",
            "Timo Schick",
            "Jane Dwivedi-Yu",
            "Asli Celikyilmaz",
            "Edouard Grave",
            "Yann LeCun",
            "Thomas Scialom"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Large Scale Online Learning",
          "year": 2003,
          "citations": 452,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/133809cf62bf67f0a63b35e5ef5180d20c9aec19",
          "authors": [
            "L. Bottou",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Improving the convergence of back-propagation learning with second-order methods",
          "year": 1989,
          "citations": 444,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/589d377b23e2bdae7ad161b36a5d6613bcfccdde",
          "authors": [
            "S. Becker",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Measuring the VC-Dimension of a Learning Machine",
          "year": 1994,
          "citations": 434,
          "abstract": null,
          "venue": "Neural Computation",
          "doi": "10.1162/neco.1994.6.5.851",
          "url": "https://www.semanticscholar.org/paper/899defb6a100af509547b8d74bb626533ee87da4",
          "authors": [
            "V. Vapnik",
            "E. Levin",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Synergistic Face Detection and Pose Estimation with Energy-Based Models",
          "year": 2004,
          "citations": 430,
          "abstract": null,
          "venue": "Journal of machine learning research",
          "doi": "10.5555/1314498.1314539",
          "url": "https://www.semanticscholar.org/paper/6b728a7442ca158f895d07c11c77d302269a832d",
          "authors": [
            "Margarita Osadchy",
            "Yann LeCun",
            "Matthew L. Miller"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Understanding Dimensional Collapse in Contrastive Self-supervised Learning",
          "year": 2021,
          "citations": 412,
          "abstract": "Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/28c17db217f2d7af12482a087d197851f0a97db0",
          "authors": [
            "Li Jing",
            "Pascal Vincent",
            "Yann LeCun",
            "Yuandong Tian"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Boosting and Other Ensemble Methods",
          "year": 1994,
          "citations": 406,
          "abstract": "We compare the performance of three types of neural network-based ensemble techniques to that of a single neural network. The ensemble algorithms are two versions of boosting and committees of neural networks trained independently. For each of the four algorithms, we experimentally determine the test and training error curves in an optical character recognition (OCR) problem as both a function of training set size and computational cost using three architectures. We show that a single machine is best for small training set size while for large training set size some version of boosting is best. However, for a given computational cost, boosting is always best. Furthermore, we show a surprising result for the original boosting algorithm: namely, that as the training set size increases, the training error decreases until it asymptotes to the test error rate. This has potential implications in the search for better training algorithms.",
          "venue": "Neural Computation",
          "doi": "10.1162/neco.1994.6.6.1289",
          "url": "https://www.semanticscholar.org/paper/6db07f39446bdf74d0178a75f526baffee1a0369",
          "authors": [
            "H. Drucker",
            "Corinna Cortes",
            "L. Jackel",
            "Yann LeCun",
            "V. Vapnik"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Classification of patterns of EEG synchronization for seizure prediction",
          "year": 2009,
          "citations": 397,
          "abstract": null,
          "venue": "Clinical Neurophysiology",
          "doi": "10.1016/j.clinph.2009.09.002",
          "url": "https://www.semanticscholar.org/paper/9da463da5a35149397273553c4de4626c49bf712",
          "authors": [
            "Piotr Wojciech Mirowski",
            "D. Madhavan",
            "Yann LeCun",
            "R. Kuzniecky"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks",
          "year": 2018,
          "citations": 393,
          "abstract": "Despite existing work on ensuring generalization of neural networks in terms of scale sensitive complexity measures, such as norms, margin and sharpness, these complexity measures do not offer an explanation of why neural networks generalize better with over-parametrization. In this work we suggest a novel complexity measure based on unit-wise capacities resulting in a tighter generalization bound for two layer ReLU networks. Our capacity bound correlates with the behavior of test error with increasing network sizes, and could potentially explain the improvement in generalization with over-parametrization. We further present a matching lower bound for the Rademacher complexity that improves over previous capacity lower bounds for neural networks.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/51063caeb691f9a20e34feb721f14660a0df968e",
          "authors": [
            "Behnam Neyshabur",
            "Zhiyuan Li",
            "Srinadh Bhojanapalli",
            "Yann LeCun",
            "N. Srebro"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "NeuFlow: A runtime reconfigurable dataflow processor for vision",
          "year": 2011,
          "citations": 386,
          "abstract": null,
          "venue": "CVPR 2011 WORKSHOPS",
          "doi": "10.1109/CVPRW.2011.5981829",
          "url": "https://www.semanticscholar.org/paper/204710a6a6d935150b5b16daf74493dea6d1b7a2",
          "authors": [
            "C. Farabet",
            "B. Martini",
            "B. Corda",
            "Polina Akselrod",
            "E. Culurciello",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Large-scale Learning with SVM and Convolutional for Generic Object Categorization",
          "year": 2006,
          "citations": 383,
          "abstract": null,
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2006.164",
          "url": "https://www.semanticscholar.org/paper/cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb",
          "authors": [
            "Fu Jie Huang",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Deep learning, reinforcement learning, and world models",
          "year": 2022,
          "citations": 375,
          "abstract": null,
          "venue": "Neural Networks",
          "doi": "10.1016/j.neunet.2022.03.037",
          "url": "https://www.semanticscholar.org/paper/effa7077e6580a8ab22c30d3c876744b4e51cd6e",
          "authors": [
            "Yu Matsuo",
            "Yann LeCun",
            "M. Sahani",
            "Doina Precup",
            "David Silver",
            "Masashi Sugiyama",
            "E. Uchibe",
            "J. Morimoto"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "CNP: An FPGA-based processor for Convolutional Networks",
          "year": 2009,
          "citations": 362,
          "abstract": null,
          "venue": "International Conference on Field-Programmable Logic and Applications",
          "doi": "10.1109/FPL.2009.5272559",
          "url": "https://www.semanticscholar.org/paper/07956c7cf9bf4267b86d52aa4143c17a4aa5d0d6",
          "authors": [
            "C. Farabet",
            "Cyril Poulet",
            "Jefferson Y. Han",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation",
          "year": 2014,
          "citations": 354,
          "abstract": "We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/326d65827307862ddc3d39b84ebc662e83ff95b3",
          "authors": [
            "Nicolas Vasilache",
            "Jeff Johnson",
            "Michaël Mathieu",
            "Soumith Chintala",
            "Serkan Piantino",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Cookbook of Self-Supervised Learning",
          "year": 2023,
          "citations": 349,
          "abstract": "Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2304.12210",
          "url": "https://www.semanticscholar.org/paper/6bfafb32b423c3f0456a10984814f89046def489",
          "authors": [
            "Randall Balestriero",
            "Mark Ibrahim",
            "Vlad Sobal",
            "Ari S. Morcos",
            "Shashank Shekhar",
            "T. Goldstein",
            "Florian Bordes",
            "Adrien Bardes",
            "G. Mialon",
            "Yuandong Tian",
            "Avi Schwarzschild",
            "A. Wilson",
            "Jonas Geiping",
            "Q. Garrido",
            "Pierre Fernandez",
            "Amir Bar",
            "H. Pirsiavash",
            "Yann LeCun",
            "Micah Goldblum"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning invariant features through topographic filter maps",
          "year": 2009,
          "citations": 347,
          "abstract": null,
          "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2009.5206545",
          "url": "https://www.semanticscholar.org/paper/54a9c2553138932426faebcaa67a63a84a56b55d",
          "authors": [
            "K. Kavukcuoglu",
            "Marc'Aurelio Ranzato",
            "R. Fergus",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Super-Resolution with Deep Convolutional Sufficient Statistics",
          "year": 2015,
          "citations": 337,
          "abstract": "Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep convolutional neural networks. The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed approach in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/cd108ed4f69b754cf0a5f3eb74d6c1949ea6674d",
          "authors": [
            "Joan Bruna",
            "P. Sprechmann",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Transforming Neural-Net Output Levels to Probability Distributions",
          "year": 1990,
          "citations": 332,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e4b370c1a04a8a6807bd73b6bbff5773e575fee7",
          "authors": [
            "J. Denker",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Very Deep Convolutional Networks for Natural Language Processing",
          "year": 2016,
          "citations": 323,
          "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/84ca430856a92000e90cd728445ca2241c10ddc3",
          "authors": [
            "Alexis Conneau",
            "Holger Schwenk",
            "Loïc Barrault",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network",
          "year": 1991,
          "citations": 316,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ff32cebbdb8a436ccd8ae797647428615ae32d74",
          "authors": [
            "Patrice Y. Simard",
            "B. Victorri",
            "Yann LeCun",
            "J. Denker"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning long‐range vision for autonomous off‐road driving",
          "year": 2009,
          "citations": 314,
          "abstract": null,
          "venue": "J. Field Robotics",
          "doi": "10.1002/rob.20276",
          "url": "https://www.semanticscholar.org/paper/2d8f527d1a96b0dae209daa6a241cf3255a6ec0d",
          "authors": [
            "R. Hadsell",
            "P. Sermanet",
            "J. Ben",
            "A. Erkan",
            "Marco Scoffier",
            "K. Kavukcuoglu",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Ask the locals: Multi-way local pooling for image recognition",
          "year": 2011,
          "citations": 302,
          "abstract": null,
          "venue": "Vision",
          "doi": "10.1109/ICCV.2011.6126555",
          "url": "https://www.semanticscholar.org/paper/9791f1e47a48fa05387cb8dd93da53bf8f43c1f4",
          "authors": [
            "Y-Lan Boureau",
            "Nicolas Le Roux",
            "F. Bach",
            "J. Ponce",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Improving generalization performance using double backpropagation",
          "year": 1992,
          "citations": 300,
          "abstract": null,
          "venue": "IEEE Trans. Neural Networks",
          "doi": "10.1109/72.165600",
          "url": "https://www.semanticscholar.org/paper/fbd86e19157ea0e4ffb05f14d7b94603a5667e0a",
          "authors": [
            "H. Drucker",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Adversarially Regularized Autoencoders",
          "year": 2017,
          "citations": 293,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d7f55400fd032d182d465eee91581d5ab845a95d",
          "authors": [
            "J. Zhao",
            "Yoon Kim",
            "Kelly W. Zhang",
            "Alexander M. Rush",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Toward automatic phenotyping of developing embryos from videos",
          "year": 2005,
          "citations": 290,
          "abstract": null,
          "venue": "IEEE Transactions on Image Processing",
          "doi": "10.1109/TIP.2005.852470",
          "url": "https://www.semanticscholar.org/paper/c029513aef54460ef6a468ff83f549d7ffbb646b",
          "authors": [
            "F. Ning",
            "D. Delhomme",
            "Yann LeCun",
            "F. Piano",
            "L. Bottou",
            "P. Barbano"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "High quality document image compression with \"DjVu\"",
          "year": 1998,
          "citations": 276,
          "abstract": null,
          "venue": "J. Electronic Imaging",
          "doi": "10.1117/1.482609",
          "url": "https://www.semanticscholar.org/paper/34103850fbd71d43b19f8335f378f421ab9a1aa8",
          "authors": [
            "L. Bottou",
            "P. Haffner",
            "P. Howard",
            "Patrice Y. Simard",
            "Yoshua Bengio",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Stacked What-Where Auto-encoders",
          "year": 2015,
          "citations": 262,
          "abstract": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/cba5fbd40767a27d20e346a108b8867ac8591a27",
          "authors": [
            "J. Zhao",
            "Michaël Mathieu",
            "Ross Goroshin",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Road Scene Segmentation from a Single Image",
          "year": 2012,
          "citations": 254,
          "abstract": null,
          "venue": "European Conference on Computer Vision",
          "doi": "10.1007/978-3-642-33786-4_28",
          "url": "https://www.semanticscholar.org/paper/62cf68c67802b6e8126ecfb3fb794b867fd2ab3a",
          "authors": [
            "J. Álvarez",
            "T. Gevers",
            "Yann LeCun",
            "Antonio M. López"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond",
          "year": 2016,
          "citations": 252,
          "abstract": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges that depend on the input data.",
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8151fbde700614ab25d6165b9ce5f76456c180d4",
          "authors": [
            "Levent Sagun",
            "L. Bottou",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Predictive network modeling of the high-resolution dynamic plant transcriptome in response to nitrate",
          "year": 2010,
          "citations": 250,
          "abstract": "BackgroundNitrate, acting as both a nitrogen source and a signaling molecule, controls many aspects of plant development. However, gene networks involved in plant adaptation to fluctuating nitrate environments have not yet been identified.ResultsHere we use time-series transcriptome data to decipher gene relationships and consequently to build core regulatory networks involved in Arabidopsis root adaptation to nitrate provision. The experimental approach has been to monitor genome-wide responses to nitrate at 3, 6, 9, 12, 15 and 20 minutes using Affymetrix ATH1 gene chips. This high-resolution time course analysis demonstrated that the previously known primary nitrate response is actually preceded by a very fast gene expression modulation, involving genes and functions needed to prepare plants to use or reduce nitrate. A state-space model inferred from this microarray time-series data successfully predicts gene behavior in unlearnt conditions.ConclusionsThe experiments and methods allow us to propose a temporal working model for nitrate-driven gene networks. This network model is tested both in silico and experimentally. For example, the over-expression of a predicted gene hub encoding a transcription factor induced early in the cascade indeed leads to the modification of the kinetic nitrate response of sentinel genes such as NIR, NIA2, and NRT1.1, and several other transcription factors. The potential nitrate/hormone connections implicated by this time-series data are also evaluated.",
          "venue": "Genome Biology",
          "doi": "10.1186/gb-2010-11-12-r123",
          "url": "https://www.semanticscholar.org/paper/631b72d4f83f86f4c8d271c736ed3087c5f7be6e",
          "authors": [
            "Gabriel Krouk",
            "Piotr Wojciech Mirowski",
            "Yann LeCun",
            "D. Shasha",
            "G. Coruzzi"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition",
          "year": 2010,
          "citations": 250,
          "abstract": "Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec",
          "authors": [
            "K. Kavukcuoglu",
            "Marc'Aurelio Ranzato",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Predicting Deeper into the Future of Semantic Segmentation",
          "year": 2017,
          "citations": 246,
          "abstract": "The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.",
          "venue": "IEEE International Conference on Computer Vision",
          "doi": "10.1109/ICCV.2017.77",
          "url": "https://www.semanticscholar.org/paper/516668a41d6106232a7cd56d20d3b3da343e5f36",
          "authors": [
            "Pauline Luc",
            "N. Neverova",
            "C. Couprie",
            "J. Verbeek",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Hardware accelerated convolutional neural networks for synthetic vision systems",
          "year": 2010,
          "citations": 240,
          "abstract": null,
          "venue": "Proceedings of 2010 IEEE International Symposium on Circuits and Systems",
          "doi": "10.1109/ISCAS.2010.5537908",
          "url": "https://www.semanticscholar.org/paper/c3c82b476162d2d006e02180530875a64af18154",
          "authors": [
            "C. Farabet",
            "B. Martini",
            "Polina Akselrod",
            "S. Talay",
            "Yann LeCun",
            "E. Culurciello"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Catalyzing next-generation Artificial Intelligence through NeuroAI",
          "year": 2022,
          "citations": 239,
          "abstract": "One of the ambitions of computational neuroscience is that we will continue to make improvements in the field of artificial intelligence that will be informed by advances in our understanding of how the brains of various species evolved to process information. To that end, here the authors propose an expanded version of the Turing test that involves embodied sensorimotor interactions with the world as a new framework for accelerating progress in artificial intelligence. Neuroscience has long been an essential driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI. A core component of this is the embodied Turing test, which challenges AI animal models to interact with the sensorimotor world at skill levels akin to their living counterparts. The embodied Turing test shifts the focus from those capabilities like game playing and language that are especially well-developed or uniquely human to those capabilities – inherited from over 500 million years of evolution – that are shared with all animals. Building models that can pass the embodied Turing test will provide a roadmap for the next generation of AI.",
          "venue": "Nature Communications",
          "doi": "10.1038/s41467-023-37180-x",
          "url": "https://www.semanticscholar.org/paper/5cba4b2a4d0b74c8aad0c94b6f468f6c86ee3db9",
          "authors": [
            "A. Zador",
            "Sean Escola",
            "B. Richards",
            "B. Ölveczky",
            "Y. Bengio",
            "K. Boahen",
            "M. Botvinick",
            "D. Chklovskii",
            "A. Churchland",
            "C. Clopath",
            "J. DiCarlo",
            "Surya",
            "Ganguli",
            "J. Hawkins",
            "Konrad Paul Kording",
            "A. Koulakov",
            "Yann LeCun",
            "T. Lillicrap",
            "Adam",
            "Marblestone",
            "B. Olshausen",
            "A. Pouget",
            "Cristina Savin",
            "T. Sejnowski",
            "Eero P. Simoncelli",
            "S. Solla",
            "David Sussillo",
            "A. Tolias",
            "D. Tsao"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The Need for Open Source Software in Machine Learning",
          "year": 2007,
          "citations": 235,
          "abstract": null,
          "venue": "Journal of machine learning research",
          "doi": "10.5555/1314498.1314577",
          "url": "https://www.semanticscholar.org/paper/ab08f2a0b98fe7938d08875eb6125fa518620222",
          "authors": [
            "S. Sonnenburg",
            "M. Braun",
            "Cheng Soon Ong",
            "Samy Bengio",
            "L. Bottou",
            "G. Holmes",
            "Yann LeCun",
            "K. Müller",
            "Fernando C Pereira",
            "C. Rasmussen",
            "Gunnar Rätsch",
            "B. Scholkopf",
            "Alex Smola",
            "Pascal Vincent",
            "J. Weston",
            "R. C. Williamson"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Tracking the World State with Recurrent Entity Networks",
          "year": 2016,
          "citations": 233,
          "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/033dd6cf61a6017e9aa9b46068d3c89082849cf3",
          "authors": [
            "Mikael Henaff",
            "J. Weston",
            "Arthur Szlam",
            "Antoine Bordes",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Handwritten zip code recognition with multilayer networks",
          "year": 1990,
          "citations": 233,
          "abstract": null,
          "venue": "[1990] Proceedings. 10th International Conference on Pattern Recognition",
          "doi": "10.1109/ICPR.1990.119325",
          "url": "https://www.semanticscholar.org/paper/a4bfe622ab32e6c645eb4be2079734355b22d304",
          "authors": [
            "Yann LeCun",
            "O. Matan",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "L. D. Jacket",
            "H. Baird"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Very deep multilingual convolutional neural networks for LVCSR",
          "year": 2015,
          "citations": 225,
          "abstract": "Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3×3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8% after cross-entropy training, a 1.4% WER improvement (10.6% relative) over the best published CNN result so far.",
          "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
          "doi": "10.1109/ICASSP.2016.7472620",
          "url": "https://www.semanticscholar.org/paper/77b839147551e024a26b07d896677b38d2b327b7",
          "authors": [
            "Tom Sercu",
            "Christian Puhrsch",
            "Brian Kingsbury",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Une procedure d'apprentissage pour reseau a seuil asymmetrique (A learning scheme for asymmetric threshold networks)",
          "year": 1985,
          "citations": 223,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d007ed936c51a700d8c65d1bbfae7acc83783c31",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Decoupled Contrastive Learning",
          "year": 2021,
          "citations": 219,
          "abstract": "Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented\"views\"of the same image as positive to be pulled closer, and all other images as negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, including large sample batches, extensive training epochs, etc. We are thus motivated to tackle these issues and establish a simple, efficient, yet competitive baseline of contrastive learning. Specifically, we identify, from theoretical and empirical studies, a noticeable negative-positive-coupling (NPC) effect in the widely used InfoNCE loss, leading to unsuitable learning efficiency concerning the batch size. By removing the NPC effect, we propose decoupled contrastive learning (DCL) loss, which removes the positive term from the denominator and significantly improves the learning efficiency. DCL achieves competitive performance with less sensitivity to sub-optimal hyperparameters, requiring neither large batches in SimCLR, momentum encoding in MoCo, or large epochs. We demonstrate with various benchmarks while manifesting robustness as much less sensitive to suboptimal hyperparameters. Notably, SimCLR with DCL achieves 68.2% ImageNet-1K top-1 accuracy using batch size 256 within 200 epochs pre-training, outperforming its SimCLR baseline by 6.4%. Further, DCL can be combined with the SOTA contrastive learning method, NNCLR, to achieve 72.3% ImageNet-1K top-1 accuracy with 512 batch size in 400 epochs, which represents a new SOTA in contrastive learning. We believe DCL provides a valuable baseline for future contrastive SSL studies.",
          "venue": "European Conference on Computer Vision",
          "doi": "10.1007/978-3-031-19809-0_38",
          "url": "https://www.semanticscholar.org/paper/40b68df4635298c32725891bc46ee0201dac56c1",
          "authors": [
            "Chun-Hsiao Yeh",
            "Cheng-Yao Hong",
            "Yen-Chi Hsu",
            "Tyng-Luh Liu",
            "Yubei Chen",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Effiicient BackProp",
          "year": 1996,
          "citations": 214,
          "abstract": null,
          "venue": "Neural Networks",
          "doi": "10.1007/3-540-49430-8_2",
          "url": "https://www.semanticscholar.org/paper/deede4f010369a810eb1017fd9e757d9876a44c8",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Genevieve B. Orr",
            "K. Müller"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Deep Learning Made Easier by Linear Transformations in Perceptrons",
          "year": 2012,
          "citations": 210,
          "abstract": null,
          "venue": "International Conference on Artificial Intelligence and Statistics",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14",
          "authors": [
            "T. Raiko",
            "Harri Valpola",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Scene parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers",
          "year": 2012,
          "citations": 205,
          "abstract": "Scene parsing consists in labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features. In parallel to feature extraction, a tree of segments is computed from a graph of pixel dissimilarities. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average \"purity\" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The system yields record accuracies on the the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) and near-record accuracy on the Stanford Background Dataset (8 classes), while being an order of magnitude faster than competing approaches, producing a 320 × 240 image labeling in less than 1 second, including feature extraction.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/56b3ff898cadde865d20ddb4e7a33434de186794",
          "authors": [
            "C. Farabet",
            "C. Couprie",
            "Laurent Najman",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The role of over-parametrization in generalization of neural networks",
          "year": 2019,
          "citations": 202,
          "abstract": null,
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6e8001afb2e24b648ae9ceb4e00c20ded4a5ee99",
          "authors": [
            "Behnam Neyshabur",
            "Zhiyuan Li",
            "Srinadh Bhojanapalli",
            "Yann LeCun",
            "N. Srebro"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Multi-Digit Recognition Using a Space Displacement Neural Network",
          "year": 1991,
          "citations": 191,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/464e8d981df7f326c3af6e9d7bd627f83e438816",
          "authors": [
            "O. Matan",
            "C. Burges",
            "Yann LeCun",
            "J. Denker"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs",
          "year": 2016,
          "citations": 184,
          "abstract": "Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely O(1) per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/1d782819afafe0d391e5b67151cb510e621f243d",
          "authors": [
            "Li Jing",
            "Yichen Shen",
            "T. Dubček",
            "J. Peurifoy",
            "S. Skirlo",
            "Yann LeCun",
            "Max Tegmark",
            "M. Soljačić"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Design of a neural network character recognizer for a touch terminal",
          "year": 1991,
          "citations": 177,
          "abstract": null,
          "venue": "Pattern Recognition",
          "doi": "10.1016/0031-3203(91)90081-F",
          "url": "https://www.semanticscholar.org/paper/adf724f637afdb300426df8d2ff4c4342f1e7528",
          "authors": [
            "Isabelle M Guyon",
            "P. Albrecht",
            "Yann LeCun",
            "J. Denker",
            "W. Hubbard"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation",
          "year": 2014,
          "citations": 176,
          "abstract": "In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion (This dataset can be downloaded from http://cs.nyu.edu/~ajain/accv2014/.), that extends the FLIC dataset [1] with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.",
          "venue": "Asian Conference on Computer Vision",
          "doi": "10.1007/978-3-319-16808-1_21",
          "url": "https://www.semanticscholar.org/paper/f8f4481e521ff34683df44c0e467d03ab5dee2b0",
          "authors": [
            "Arjun Jain",
            "Jonathan Tompson",
            "Yann LeCun",
            "C. Bregler"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG",
          "year": 2008,
          "citations": 174,
          "abstract": null,
          "venue": "IEEE Workshop on Machine Learning for Signal Processing",
          "doi": "10.1109/MLSP.2008.4685487",
          "url": "https://www.semanticscholar.org/paper/966ce7b2567280088ee1d5816cee9e06d12fa19d",
          "authors": [
            "Piotr Wojciech Mirowski",
            "Yann LeCun",
            "D. Madhavan",
            "R. Kuzniecky"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "LeRec: A NN/HMM Hybrid for On-Line Handwriting Recognition",
          "year": 1995,
          "citations": 166,
          "abstract": null,
          "venue": "Neural Computation",
          "doi": "10.1162/neco.1995.7.6.1289",
          "url": "https://www.semanticscholar.org/paper/4a4192fd6efb5661eca197cce24289776a4fbcc2",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun",
            "C. Nohl",
            "C. Burges"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Unsupervised Learning of Spatiotemporally Coherent Metrics",
          "year": 2014,
          "citations": 163,
          "abstract": "Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity priors. We establish a connection between slow feature learning and metric learning. Using this connection we define \"temporal coherence\" -- a criterion which can be used to set hyper-parameters in a principled and automated manner. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labels.",
          "venue": "IEEE International Conference on Computer Vision",
          "doi": "10.1109/ICCV.2015.465",
          "url": "https://www.semanticscholar.org/paper/d0a8b5cf98b6721b743571ee13e6032ff5598aea",
          "authors": [
            "Ross Goroshin",
            "Joan Bruna",
            "Jonathan Tompson",
            "D. Eigen",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "An analog neural network processor with programmable topology",
          "year": 1991,
          "citations": 162,
          "abstract": null,
          "venue": "IEEE J. Solid State Circuits",
          "doi": "10.1109/4.104196",
          "url": "https://www.semanticscholar.org/paper/82795cf04f5631e82413b1952625a5a6f21b68ea",
          "authors": [
            "J. Bromley",
            "Yann LeCun",
            "L. Jackel"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning Invariant Feature Hierarchies",
          "year": 2012,
          "citations": 161,
          "abstract": null,
          "venue": "ECCV Workshops",
          "doi": "10.1007/978-3-642-33863-2_51",
          "url": "https://www.semanticscholar.org/paper/e882a6014ac4d66b1035729305ff8aa76ba5d09d",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The Mind of a Mouse",
          "year": 2020,
          "citations": 160,
          "abstract": null,
          "venue": "Cell",
          "doi": "10.1016/j.cell.2020.08.010",
          "url": "https://www.semanticscholar.org/paper/a5fd28aa9240835a14b1c285096d62e3a3ba5722",
          "authors": [
            "L. Abbott",
            "D. Bock",
            "E. Callaway",
            "W. Denk",
            "C. Dulac",
            "A. Fairhall",
            "I. Fiete",
            "Kristen M. Harris",
            "M. Helmstaedter",
            "Viren Jain",
            "N. Kasthuri",
            "Yann LeCun",
            "J. Lichtman",
            "P. Littlewood",
            "L. Luo",
            "John H. R. Maunsell",
            "R. Reid",
            "B. Rosen",
            "G. Rubin",
            "T. Sejnowski",
            "H. Seung",
            "K. Svoboda",
            "D. Tank",
            "Doris Y. Tsao",
            "D. C. Essen"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Loss Functions for Discriminative Training of Energy-Based Models",
          "year": 2005,
          "citations": 160,
          "abstract": null,
          "venue": "International Conference on Artificial Intelligence and Statistics",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/fae82787fc4268f579823696bf8f54b22e253711",
          "authors": [
            "Yann LeCun",
            "Fu Jie Huang"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Transformation invariance in pattern recognition: Tangent distance and propagation",
          "year": 2000,
          "citations": 160,
          "abstract": null,
          "venue": "International journal of imaging systems and technology (Print)",
          "doi": "10.1002/1098-1098(2000)11:3%3C181::AID-IMA1003%3E3.0.CO;2-E",
          "url": "https://www.semanticscholar.org/paper/76c67b335b8192b61a0e9364827afcc2f0842d11",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun",
            "J. Denker",
            "B. Victorri"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Boxlets: A Fast Convolution Algorithm for Signal Processing and Neural Networks",
          "year": 1998,
          "citations": 156,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b2ef3d741a8d267b813781a73d4222b6dbba99fa",
          "authors": [
            "Patrice Y. Simard",
            "L. Bottou",
            "P. Haffner",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Moving Beyond Feature Design: Deep Architectures and Automatic Feature Learning in Music Informatics",
          "year": 2012,
          "citations": 153,
          "abstract": null,
          "venue": "International Society for Music Information Retrieval Conference",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7a42dedd7050442a78940bae6a62899919693b17",
          "authors": [
            "Eric J. Humphrey",
            "J. Bello",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods",
          "year": 2022,
          "citations": 152,
          "abstract": "Self-Supervised Learning (SSL) surmises that inputs and pairwise positive relationships are enough to learn meaningful representations. Although SSL has recently reached a milestone: outperforming supervised methods in many modalities\\dots the theoretical foundations are limited, method-specific, and fail to provide principled design guidelines to practitioners. In this paper, we propose a unifying framework under the helm of spectral manifold learning to address those limitations. Through the course of this study, we will rigorously demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous spectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al. This unification will then allow us to obtain (i) the closed-form optimal representation for each method, (ii) the closed-form optimal network parameters in the linear regime for each method, (iii) the impact of the pairwise relations used during training on each of those quantities and on downstream task performances, and most importantly, (iv) the first theoretical bridge between contrastive and non-contrastive methods towards global and local spectral embedding methods respectively, hinting at the benefits and limitations of each. For example, (i) if the pairwise relation is aligned with the downstream task, any SSL method can be employed successfully and will recover the supervised method, but in the low data regime, VICReg's invariance hyper-parameter should be high; (ii) if the pairwise relation is misaligned with the downstream task, VICReg with small invariance hyper-parameter should be preferred over SimCLR or BarlowTwins.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b845c9d7f3fa05f56e7394f273c0c7536ee0e671",
          "authors": [
            "Randall Balestriero",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "VICRegL: Self-Supervised Learning of Local Visual Features",
          "year": 2022,
          "citations": 148,
          "abstract": "Most recent self-supervised methods for learning image representations focus on either producing a global feature with invariance properties, or producing a set of local features. The former works best for classification tasks while the latter is best for detection and segmentation tasks. This paper explores the fundamental trade-off between learning local and global features. A new method called VICRegL is proposed that learns good global and local features simultaneously, yielding excellent performance on detection and segmentation tasks while maintaining good performance on classification tasks. Concretely, two identical branches of a standard convolutional net architecture are fed two differently distorted versions of the same image. The VICReg criterion is applied to pairs of global feature vectors. Simultaneously, the VICReg criterion is applied to pairs of local feature vectors occurring before the last pooling layer. Two local feature vectors are attracted to each other if their l2-distance is below a threshold or if their relative locations are consistent with a known geometric transformation between the two input images. We demonstrate strong performance on linear classification and segmentation transfer tasks. Code and pretrained models are publicly available at: https://github.com/facebookresearch/VICRegL",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2210.01571",
          "url": "https://www.semanticscholar.org/paper/b8d4beeb8df994db12688226cf2a619f1d633b72",
          "authors": [
            "Adrien Bardes",
            "J. Ponce",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Understanding Deep Architectures using a Recursive Convolutional Network",
          "year": 2013,
          "citations": 147,
          "abstract": "A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We find that while increasing the numbers of layers and parameters each have clear benefit, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and finds most of its benefit through the introduction of more weights. Our results (i) empirically confirm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e2d894584986b44710f634b696db371f8aff92e0",
          "authors": [
            "D. Eigen",
            "J. Rolfe",
            "R. Fergus",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The Loss Surface of Multilayer Networks",
          "year": 2014,
          "citations": 146,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6d9cb3d3c0330a6c2f42d159a3a706b6b49744b2",
          "authors": [
            "A. Choromańska",
            "Mikael Henaff",
            "Michaël Mathieu",
            "G. B. Arous",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone",
          "year": 2022,
          "citations": 145,
          "abstract": "Vision-language (VL) pre-training has recently received considerable attention. However, most existing end-to-end pre-training approaches either only aim to tackle VL tasks such as image-text retrieval, visual question answering (VQA) and image captioning that test high-level understanding of images, or only target region-level understanding for tasks such as phrase grounding and object detection. We present FIBER (Fusion-In-the-Backbone-based transformER), a new VL model architecture that can seamlessly handle both these types of tasks. Instead of having dedicated transformer layers for fusion after the uni-modal backbones, FIBER pushes multimodal fusion deep into the model by inserting cross-attention into the image and text backbones, bringing gains in terms of memory and performance. In addition, unlike previous work that is either only pre-trained on image-text data or on fine-grained data with box-level annotations, we present a two-stage pre-training strategy that uses both these kinds of data efficiently: (i) coarse-grained pre-training based on image-text data; followed by (ii) fine-grained pre-training based on image-text-box data. We conduct comprehensive experiments on a wide range of VL tasks, ranging from VQA, image captioning, and retrieval, to phrase grounding, referring expression comprehension, and object detection. Using deep multimodal fusion coupled with the two-stage pre-training, FIBER provides consistent performance improvements over strong baselines across all tasks, often outperforming methods using magnitudes more data. Code is available at https://github.com/microsoft/FIBER.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2206.07643",
          "url": "https://www.semanticscholar.org/paper/4c559d29e19f1226353f52ffe9f8068db1cef943",
          "authors": [
            "Zi-Yi Dou",
            "Aishwarya Kamath",
            "Zhe Gan",
            "Pengchuan Zhang",
            "Jianfeng Wang",
            "Linjie Li",
            "Zicheng Liu",
            "Ce Liu",
            "Yann LeCun",
            "Nanyun Peng",
            "Jianfeng Gao",
            "Lijuan Wang"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning to Linearize Under Uncertainty",
          "year": 2015,
          "citations": 144,
          "abstract": "Training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/dd2fa69647160bb2ea25dcc7b2f6409b01e40222",
          "authors": [
            "Ross Goroshin",
            "Michaël Mathieu",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Application of the ANNA neural network chip to high-speed character recognition",
          "year": 1992,
          "citations": 144,
          "abstract": null,
          "venue": "IEEE Trans. Neural Networks",
          "doi": "10.1109/72.129422",
          "url": "https://www.semanticscholar.org/paper/b8bc656a1935f07e894833b608cc4671b9fa828f",
          "authors": [
            "Eduard Säckinger",
            "B. Boser",
            "J. Bromley",
            "Yann LeCun",
            "L. Jackel"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Unified Energy-Based Framework for Unsupervised Learning",
          "year": 2007,
          "citations": 142,
          "abstract": null,
          "venue": "International Conference on Artificial Intelligence and Statistics",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/306ddd8b7ea3ead125491efc3e8a9f738ce65b89",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "S. Chopra",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Recurrent Orthogonal Networks and Long-Memory Tasks",
          "year": 2016,
          "citations": 137,
          "abstract": "Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ef3152106e7f4d05ad8d32a5b90d3790c5cdef24",
          "authors": [
            "Mikael Henaff",
            "Arthur Szlam",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Unsupervised Learning of Sparse Features for Scalable Audio Classification",
          "year": 2011,
          "citations": 137,
          "abstract": null,
          "venue": "International Society for Music Information Retrieval Conference",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/34642631a6ef8130b98ce73e61298bb9c89b0c71",
          "authors": [
            "Mikael Henaff",
            "Kevin Jarrett",
            "K. Kavukcuoglu",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Feature learning and deep architectures: new directions for music informatics",
          "year": 2013,
          "citations": 135,
          "abstract": null,
          "venue": "Journal of Intelligence and Information Systems",
          "doi": "10.1007/s10844-013-0248-5",
          "url": "https://www.semanticscholar.org/paper/8b930259e50c9e7ef47d6ad2d57ff377d630361a",
          "authors": [
            "Eric J. Humphrey",
            "J. Bello",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Large-Scale FPGA-based Convolutional Networks",
          "year": 2011,
          "citations": 135,
          "abstract": null,
          "venue": "",
          "doi": "10.1017/CBO9781139042918.020",
          "url": "https://www.semanticscholar.org/paper/b970c9d53c699a8e09f1d8dbe440b6f309712a89",
          "authors": [
            "C. Farabet",
            "Yann LeCun",
            "K. Kavukcuoglu",
            "B. Martini",
            "Polina Akselrod",
            "S. Talay",
            "E. Culurciello"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "DeSIGN: Design Inspiration from Generative Networks",
          "year": 2018,
          "citations": 131,
          "abstract": "Can an algorithm create original and compelling fashion designs to serve as an inspirational assistant? To help answer this question, we design and investigate different image generation models associated with different loss functions to boost creativity in fashion generation. The dimensions of our explorations include: (i) different Generative Adversarial Networks architectures that start from noise vectors to generate fashion items, (ii) novel loss functions that encourage novelty, inspired from Sharma-Mittal divergence, a generalized mutual information measure for the widely used relative entropies such as Kullback-Leibler, and (iii) a generation process following the key elements of fashion design (disentangling shape and texture components). A key challenge of this study is the evaluation of generated designs and the retrieval of best ones, hence we put together an evaluation protocol associating automatic metrics and human experimental studies that we hope will help ease future research. We show that our proposed creativity criterion yield better overall appreciation than the one employed in Creative Adversarial Networks. In the end, about 61% of our images are thought to be created by human designers rather than by a computer while also being considered original per our human subject experiments, and our proposed loss scores the highest compared to existing losses in both novelty and likability.",
          "venue": "ECCV Workshops",
          "doi": "10.1007/978-3-030-11015-4_5",
          "url": "https://www.semanticscholar.org/paper/e9e605d95a2884cc6a4cdf1745472dec9eb23b88",
          "authors": [
            "Othman Sbai",
            "Mohamed Elhoseiny",
            "Antoine Bordes",
            "Yann LeCun",
            "C. Couprie"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Open Problem: The landscape of the loss surfaces of multilayer networks",
          "year": 2015,
          "citations": 127,
          "abstract": null,
          "venue": "Annual Conference Computational Learning Theory",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d6739de41e880e3db525af91b9d19b2986aa0655",
          "authors": [
            "A. Choromańska",
            "Yann LeCun",
            "G. B. Arous"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic",
          "year": 2019,
          "citations": 125,
          "abstract": "Learning a policy using only observational data is challenging because the distribution of states it induces at execution time may differ from the distribution observed during training. We propose to train a policy by unrolling a learned model of the environment dynamics over multiple time steps while explicitly penalizing two costs: the original cost the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on. We measure this second cost by using the uncertainty of the dynamics model about its own predictions, using recent ideas from uncertainty estimation for deep networks. We evaluate our approach using a large-scale observational dataset of driving behavior recorded from traffic cameras, and show that we are able to learn effective driving policies from purely observational data, with no environment interaction.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2eeace98cf3c105a8d37884dc8d33c50ae4b7ddb",
          "authors": [
            "Mikael Henaff",
            "A. Canziani",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning processes in an asymmetric threshold network",
          "year": 1986,
          "citations": 125,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/56b771c4c3a54910dc3e7ff838940de89ed282db",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Comparing dynamics: deep neural networks versus glassy systems",
          "year": 2018,
          "citations": 122,
          "abstract": "We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are (1) the complexity of the loss landscape and of the dynamics within it, and (2) to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and datasets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.",
          "venue": "International Conference on Machine Learning",
          "doi": "10.1088/1742-5468/ab3281",
          "url": "https://www.semanticscholar.org/paper/fc998c5e7b0fd1609d5a91d700fec3ec9c72838c",
          "authors": [
            "Marco Baity-Jesi",
            "Levent Sagun",
            "M. Geiger",
            "S. Spigler",
            "G. B. Arous",
            "C. Cammarota",
            "Yann LeCun",
            "M. Wyart",
            "G. Biroli"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Global training of document processing systems using graph transformer networks",
          "year": 1997,
          "citations": 122,
          "abstract": null,
          "venue": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.1997.609370",
          "url": "https://www.semanticscholar.org/paper/4f0ab1dcdc9f405ca90e36a35ea335196464d387",
          "authors": [
            "L. Bottou",
            "Yoshua Bengio",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
          "year": 2023,
          "citations": 120,
          "abstract": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of explanations as features: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an LLM-to-LM interpreter to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data. Our codes and datasets are available at: https://github.com/XiaoxinHe/TAPE.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2d2b05f0969568ac3fd3c2cca5df04c4136c5416",
          "authors": [
            "Xiaoxin He",
            "X. Bresson",
            "T. Laurent",
            "Adam Perold",
            "Yann LeCun",
            "Bryan Hooi"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Automatic recognition of biological particles in microscopic images",
          "year": 2007,
          "citations": 118,
          "abstract": null,
          "venue": "Pattern Recognition Letters",
          "doi": "10.1016/j.patrec.2006.06.010",
          "url": "https://www.semanticscholar.org/paper/9228aece7615fbc33394a3124aef27f9a85853c3",
          "authors": [
            "Marc'Aurelio Ranzato",
            "P. E. Taylor",
            "J. House",
            "R. Flagan",
            "Yann LeCun",
            "P. Perona"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Generalization of ViT/MLP-Mixer to Graphs",
          "year": 2022,
          "citations": 117,
          "abstract": "Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning. Standard GNNs define a local message-passing mechanism which propagates information over the whole graph domain by stacking multiple layers. This paradigm suffers from two major limitations, over-squashing and poor long-range dependencies, that can be solved using global attention but significantly increases the computational cost to quadratic complexity. In this work, we propose an alternative approach to overcome these structural limitations by leveraging the ViT/MLP-Mixer architectures introduced in computer vision. We introduce a new class of GNNs, called Graph ViT/MLP-Mixer, that holds three key properties. First, they capture long-range dependency and mitigate the issue of over-squashing as demonstrated on Long Range Graph Benchmark and TreeNeighbourMatch datasets. Second, they offer better speed and memory efficiency with a complexity linear to the number of nodes and edges, surpassing the related Graph Transformer and expressive GNN models. Third, they show high expressivity in terms of graph isomorphism as they can distinguish at least 3-WL non-isomorphic graphs. We test our architecture on 4 simulated datasets and 7 real-world benchmarks, and show highly competitive results on all of them. The source code is available for reproducibility at: \\url{https://github.com/XiaoxinHe/Graph-ViT-MLPMixer}.",
          "venue": "International Conference on Machine Learning",
          "doi": "10.48550/arXiv.2212.13350",
          "url": "https://www.semanticscholar.org/paper/517802b9381246dff16756fe5299fa62bb29e228",
          "authors": [
            "Xiaoxin He",
            "Bryan Hooi",
            "T. Laurent",
            "Adam Perold",
            "Yann LeCun",
            "X. Bresson"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "1.1 Deep Learning Hardware: Past, Present, and Future",
          "year": 2019,
          "citations": 115,
          "abstract": "Historically, progress in neural networks and deep learning research has been greatly influenced by the available hardware and software tools. This paper identifies trends in deep learning research that will influence hardware architectures and software platforms of the future.",
          "venue": "IEEE International Solid-State Circuits Conference",
          "doi": "10.1109/ISSCC.2019.8662396",
          "url": "https://www.semanticscholar.org/paper/91788beae2cd32b13c9ece1e724fd1988d9d9629",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Statistical Machine Learning and Dissolved Gas Analysis: A Review",
          "year": 2012,
          "citations": 114,
          "abstract": null,
          "venue": "IEEE Transactions on Power Delivery",
          "doi": "10.1109/TPWRD.2012.2197868",
          "url": "https://www.semanticscholar.org/paper/c58cafdd224437f2f7ee59ca70c10cf13ab260d9",
          "authors": [
            "Piotr Wojciech Mirowski",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning in High Dimension Always Amounts to Extrapolation",
          "year": 2021,
          "citations": 113,
          "abstract": "The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample $x$ whenever this sample falls inside or on the boundary of the given dataset's convex hull. Extrapolation occurs when $x$ falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional ($>$100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation definition as an indicator of generalization performances.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e75c388b60cf447be7148be25feeee3e10d12cf4",
          "authors": [
            "Randall Balestriero",
            "J. Pesenti",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The Effects of Regularization and Data Augmentation are Class Dependent",
          "year": 2022,
          "citations": 109,
          "abstract": "Regularization is a fundamental technique to prevent over-fitting and to improve generalization performances by constraining a model's complexity. Current Deep Networks heavily rely on regularizers such as Data-Augmentation (DA) or weight-decay, and employ structural risk minimization, i.e. cross-validation, to select the optimal regularization hyper-parameters. In this study, we demonstrate that techniques such as DA or weight decay produce a model with a reduced complexity that is unfair across classes. The optimal amount of DA or weight decay found from cross-validation leads to disastrous model performances on some classes e.g. on Imagenet with a resnet50, the\"barn spider\"classification test accuracy falls from $68\\%$ to $46\\%$ only by introducing random crop DA during training. Even more surprising, such performance drop also appears when introducing uninformative regularization techniques such as weight decay. Those results demonstrate that our search for ever increasing generalization performance -- averaged over all classes and samples -- has left us with models and regularizers that silently sacrifice performances on some classes. This scenario can become dangerous when deploying a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on INaturalist sees its performances fall from $70\\%$ to $30\\%$ on class \\#8889 when introducing random crop DA during the Imagenet pre-training phase. Those results demonstrate that designing novel regularizers without class-dependent bias remains an open research question.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2204.03632",
          "url": "https://www.semanticscholar.org/paper/dcda4897113ed03c920e2e94a90ee33e09781759",
          "authors": [
            "Randall Balestriero",
            "L. Bottou",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Automatic Learning Rate Maximization by On-Line Estimation of the Hessian's Eigenvectors",
          "year": 1992,
          "citations": 109,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/abc8a30694deda46c150d4da277aec291878cfeb",
          "authors": [
            "Yann LeCun",
            "Patrice Y. Simard",
            "Barak A. Pearlmutter"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank",
          "year": 2022,
          "citations": 108,
          "abstract": "Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations but only few principled guidelines that would help practitioners to successfully deploy them. The main reason for that pitfall comes from JE-SSL's core principle of not employing any input reconstruction therefore lacking visual cues of unsuccessful training. Adding non informative loss values to that, it becomes difficult to deploy SSL on a new dataset for which no labels can help to judge the quality of the learned representation. In this study, we develop a simple unsupervised criterion that is indicative of the quality of the learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method -- coined RankMe -- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels. A further benefit of RankMe is that it does not have any training or hyper-parameters to tune. Through thorough empirical experiments involving hundreds of training episodes, we demonstrate how RankMe can be used for hyperparameter selection with nearly no reduction in final performance compared to the current selection method that involve a dataset's labels. We hope that RankMe will facilitate the deployment of JE-SSL towards domains that do not have the opportunity to rely on labels for representations' quality assessment.",
          "venue": "International Conference on Machine Learning",
          "doi": "10.48550/arXiv.2210.02885",
          "url": "https://www.semanticscholar.org/paper/127ebdb7b87fe5c8c8ff1bb9173584b75eec8f47",
          "authors": [
            "Q. Garrido",
            "Randall Balestriero",
            "Laurent Najman",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Mathematical Motivation for Complex-Valued Convolutional Networks",
          "year": 2015,
          "citations": 107,
          "abstract": "Abstract A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors, followed by (2) taking the absolute value of every entry of the resulting vectors, followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as data-driven multiscale windowed power spectra, data-driven multiscale windowed absolute spectra, data-driven multiwavelet absolute values, or (in their most general configuration) data-driven nonlinear multiwavelet packets. Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs), sigmoidal (e.g., logistic or tanh) nonlinearities, or max pooling, for example, do not obviously exhibit the same exact correspondence with data-driven wavelets (whereas for complex-valued convnets, the correspondence is much more than just a vague analogy). Courtesy of the exact correspondence, the remarkably rich and rigorous body of mathematical analysis for wavelets applies directly to (complex-valued) convnets.",
          "venue": "Neural Computation",
          "doi": "10.1162/NECO_a_00824",
          "url": "https://www.semanticscholar.org/paper/f55fe2b4344f015927a834d8ad6f52a35c3a8c8e",
          "authors": [
            "M. Tygert",
            "Joan Bruna",
            "Soumith Chintala",
            "Yann LeCun",
            "Serkan Piantino",
            "Arthur Szlam"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Second Order Properties of Error Surfaces: Learning Time and Generalization",
          "year": 1990,
          "citations": 107,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0c43153a3627c7d98cc09f909c232f3899597204",
          "authors": [
            "Yann LeCun",
            "I. Kanter",
            "S. Solla"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "On the duality between contrastive and non-contrastive self-supervised learning",
          "year": 2022,
          "citations": 106,
          "abstract": "Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning.",
          "venue": "International Conference on Learning Representations",
          "doi": "10.48550/arXiv.2206.02574",
          "url": "https://www.semanticscholar.org/paper/11c16254f7b61687b5d9b7637de032461a6ebb5f",
          "authors": [
            "Q. Garrido",
            "Yubei Chen",
            "Adrien Bardes",
            "Laurent Najman",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors",
          "year": 2021,
          "citations": 101,
          "abstract": "Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these ‘black boxes’ as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at: https://github.com/zeyuyun1/TransformerVis.",
          "venue": "Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out",
          "doi": "10.18653/V1/2021.DEELIO-1.1",
          "url": "https://www.semanticscholar.org/paper/7cc88a1a904e8bb6edc1123c0800d1c5a0ea435d",
          "authors": [
            "Zeyu Yun",
            "Yubei Chen",
            "B. Olshausen",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Signal recovery from Pooling Representations",
          "year": 2013,
          "citations": 101,
          "abstract": "In this work we compute lower Lipschitz bounds of lp pooling operators for p = 1, 2, ∞ as well as lp pooling operators preceded by half-rectification layers. These give sufficient conditions for the design of invertible neural network layers. Numerical experiments on MNIST and image patches confirm that pooling layers can be inverted with phase recovery algorithms. Moreover, the regularity of the inverse pooling, controlled by the lower Lipschitz constant, is empirically verified with a nearest neighbor regression.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/47f1b4ce7674b15d293f5a3c53d6da44bc8e137e",
          "authors": [
            "Joan Bruna",
            "Arthur Szlam",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Deep belief net learning in a long-range vision system for autonomous off-road driving",
          "year": 2008,
          "citations": 100,
          "abstract": null,
          "venue": "2008 IEEE/RSJ International Conference on Intelligent Robots and Systems",
          "doi": "10.1109/IROS.2008.4651217",
          "url": "https://www.semanticscholar.org/paper/010b7587ef04d12f162cbdf55657442e289b343d",
          "authors": [
            "R. Hadsell",
            "A. Erkan",
            "P. Sermanet",
            "Marco Scoffier",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Predicting Future Instance Segmentations by Forecasting Convolutional Features",
          "year": 2018,
          "citations": 96,
          "abstract": "Anticipating future events is an important prerequisite towards intelligent behavior. Video forecasting has been studied as a proxy task towards this goal. Recent work has shown that to predict semantic segmentation of future frames, forecasting at the semantic level is more effective than forecasting RGB frames and then segmenting these. In this paper we consider the more challenging problem of future instance segmentation, which additionally segments out individual objects. To deal with a varying number of output labels per image, we develop a predictive model in the space of fixed-sized convolutional features of the Mask R-CNN instance segmentation model. We apply the \"detection head\" of Mask R-CNN on the predicted features to produce the instance segmentation of future frames. Experiments show that this approach significantly improves over baselines based on optical flow.",
          "venue": "European Conference on Computer Vision",
          "doi": "10.1007/978-3-030-01240-3_36",
          "url": "https://www.semanticscholar.org/paper/cb503d90c0bd9a555a6b99429991c4d6f39e0f70",
          "authors": [
            "Pauline Luc",
            "C. Couprie",
            "Yann LeCun",
            "Jakob Verbeek"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "To Compress or Not to Compress—Self-Supervised Learning and Information Theory: A Review",
          "year": 2023,
          "citations": 95,
          "abstract": "Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory has shaped deep neural networks, particularly the information bottleneck principle. This principle optimizes the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the self-supervised information-theoretic learning problem. This framework includes multiple encoders and decoders, suggesting that all existing work on self-supervised learning can be seen as specific instances. We aim to unify these approaches to understand their underlying principles better and address the main challenge: many works present different frameworks with differing theories that may seem contradictory. By weaving existing research into a cohesive narrative, we delve into contemporary self-supervised methodologies, spotlight potential research areas, and highlight inherent challenges. Moreover, we discuss how to estimate information-theoretic quantities and their associated empirical problems. Overall, this paper provides a comprehensive review of the intersection of information theory, self-supervised learning, and deep neural networks, aiming for a better understanding through our proposed unified approach.",
          "venue": "Entropy",
          "doi": "10.3390/e26030252",
          "url": "https://www.semanticscholar.org/paper/97b1f4980fc173e59ff3a3bdaf1b9a13965fb32e",
          "authors": [
            "Ravid Shwartz-Ziv",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Globally Trained Handwritten Word Recognizer Using Spatial Representation, Convolutional Neural Networks, and Hidden Markov Models",
          "year": 1993,
          "citations": 94,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3461f06617b42dadd1ce240a93ffe420513b3399",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun",
            "D. Henderson"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "NeuFlow: Dataflow vision processing system-on-a-chip",
          "year": 2012,
          "citations": 91,
          "abstract": null,
          "venue": "Midwest Symposium on Circuits and Systems",
          "doi": "10.1109/MWSCAS.2012.6292202",
          "url": "https://www.semanticscholar.org/paper/90ddfda9c97e96fbba0b5a314853e5867d813a2b",
          "authors": [
            "Phi-Hung Pham",
            "D. Jelača",
            "C. Farabet",
            "B. Martini",
            "Yann LeCun",
            "E. Culurciello"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Image and video coding—emerging standards and beyond",
          "year": 2001,
          "citations": 90,
          "abstract": null,
          "venue": "IEEE Trans. Circuits Syst. Video Technol.",
          "doi": "10.1016/B978-155860651-7/50094-2",
          "url": "https://www.semanticscholar.org/paper/24232511c7cb88468b5e69f7638e1c722315f616",
          "authors": [
            "B. Haskell",
            "P. Howard",
            "Yann LeCun",
            "Atul Puri",
            "J. Ostermann",
            "M. Civanlar",
            "L. Rabiner",
            "L. Bottou",
            "P. Haffner"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Regularized estimation of image statistics by Score Matching",
          "year": 2010,
          "citations": 87,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/74706fab48249b071e10615f8da60b8401fb9f3f",
          "authors": [
            "Diederik P. Kingma",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Adversarially Regularized Autoencoders for Generating Discrete Structures",
          "year": 2017,
          "citations": 79,
          "abstract": "Generative adversarial networks are an effective approach for learning rich latent representations of continuous data, but have proven difficult to apply directly to discrete structured data, such as text sequences or discretized images. Ideally we could encode discrete structures in a continuous code space to avoid this problem, but it is difficult to learn an appropriate general-purpose encoder. In this work, we consider a simple approach for handling these two challenges jointly, employing a discrete structure autoencoder with a code space regularized by generative adversarial training. The model learns a smooth regularized code space while still being able to model the underlying data, and can be used as a discrete GAN with the ability to generate coherent discrete outputs from continuous samples. We demonstrate empirically how key properties of the data are captured in the model's latent space, and evaluate the model itself on the tasks of discrete image generation, text generation, and semi-supervised learning.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/89b139b8b507bdfb8f57b9cc0f09bfcf859bc966",
          "authors": [
            "J. Zhao",
            "Yoon Kim",
            "Kelly W. Zhang",
            "Alexander M. Rush",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Word-level training of a handwritten word recognizer based on convolutional neural networks",
          "year": 1994,
          "citations": 77,
          "abstract": null,
          "venue": "Signal Processing",
          "doi": "10.1109/ICPR.1994.576881",
          "url": "https://www.semanticscholar.org/paper/ec8c344bb9d1e4b966f499a9b236c3e320d46362",
          "authors": [
            "Yann LeCun",
            "Yoshua Bengio"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Comparison between Frame-Constrained Fix-Pixel-Value and Frame-Free Spiking-Dynamic-Pixel ConvNets for Visual Processing",
          "year": 2012,
          "citations": 74,
          "abstract": "Most scene segmentation and categorization architectures for the extraction of features in images and patches make exhaustive use of 2D convolution operations for template matching, template search, and denoising. Convolutional Neural Networks (ConvNets) are one example of such architectures that can implement general-purpose bio-inspired vision systems. In standard digital computers 2D convolutions are usually expensive in terms of resource consumption and impose severe limitations for efficient real-time applications. Nevertheless, neuro-cortex inspired solutions, like dedicated Frame-Based or Frame-Free Spiking ConvNet Convolution Processors, are advancing real-time visual processing. These two approaches share the neural inspiration, but each of them solves the problem in different ways. Frame-Based ConvNets process frame by frame video information in a very robust and fast way that requires to use and share the available hardware resources (such as: multipliers, adders). Hardware resources are fixed- and time-multiplexed by fetching data in and out. Thus memory bandwidth and size is important for good performance. On the other hand, spike-based convolution processors are a frame-free alternative that is able to perform convolution of a spike-based source of visual information with very low latency, which makes ideal for very high-speed applications. However, hardware resources need to be available all the time and cannot be time-multiplexed. Thus, hardware should be modular, reconfigurable, and expansible. Hardware implementations in both VLSI custom integrated circuits (digital and analog) and FPGA have been already used to demonstrate the performance of these systems. In this paper we present a comparison study of these two neuro-inspired solutions. A brief description of both systems is presented and also discussions about their differences, pros and cons.",
          "venue": "Frontiers in Neuroscience",
          "doi": "10.3389/fnins.2012.00032",
          "url": "https://www.semanticscholar.org/paper/18816aa33e7afd3eabfa7ac6437e90220aa5b317",
          "authors": [
            "C. Farabet",
            "R. Paz",
            "J. Pérez-Carrasco",
            "C. Zamarreño-Ramos",
            "A. Linares-Barranco",
            "Yann LeCun",
            "E. Culurciello",
            "T. Serrano-Gotarredona",
            "B. Linares-Barranco"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Discriminative Recurrent Sparse Auto-Encoders",
          "year": 2013,
          "citations": 73,
          "abstract": "We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. \nFrom an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/72c35d7eb807ed46fb82024922ffbf45218f5a95",
          "authors": [
            "J. Rolfe",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?",
          "year": 2017,
          "citations": 72,
          "abstract": "This article offers an empirical study on the different ways of encoding Chinese, Japanese, Korean (CJK) and English languages for text classification. Different encoding levels are studied, including UTF-8 bytes, characters, words, romanized characters and romanized words. For all encoding levels, whenever applicable, we provide comparisons with linear models, fastText and convolutional networks. For convolutional networks, we compare between encoding mechanisms using character glyph images, one-hot (or one-of-n) encoding, and embedding. In total there are 473 models, using 14 large-scale text classification datasets in 4 languages including Chinese, English, Japanese and Korean. Some conclusions from these results include that byte-level one-hot encoding based on UTF-8 consistently produces competitive results for convolutional networks, that word-level n-grams linear models are competitive even without perfect word segmentation, and that fastText provides the best result using character-level n-gram encoding but can overfit when the features are overly rich.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/07466fb914982f55051cc0b236fd524bdcd8bdc7",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields",
          "year": 2010,
          "citations": 72,
          "abstract": "We introduce a new neural architecture and an unsupervised algorithm for learning invariant representations from temporal sequence of images. The system uses two groups of complex cells whose outputs are combined multiplicatively: one that represents the content of the image, constrained to be constant over several consecutive frames, and one that represents the precise location of features, which is allowed to vary over time but constrained to be sparse. The architecture uses an encoder to extract features, and a decoder to reconstruct the input from the features. The method was applied to patches extracted from consecutive movie frames and produces orientation and frequency selective units analogous to the complex cells in V1. An extension of the method is proposed to train a network composed of units with local receptive field spread over a large image of arbitrary size. A layer of complex cells, subject to sparsity constraints, pool feature units over overlapping local neighborhoods, which causes the feature units to organize themselves into pinwheel patterns of orientation-selective receptive fields, similar to those observed in the mammalian visual cortex. A feed-forward encoder efficiently computes the feature representation of full images.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/31f04f8f83365fabf7ba9c9be1179c0da6815128",
          "authors": [
            "Karol Gregor",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Unsupervised Image Matching and Object Discovery as Optimization",
          "year": 2019,
          "citations": 66,
          "abstract": "Learning with complete or partial supervision is power- ful but relies on ever-growing human annotation efforts. As a way to mitigate this serious problem, as well as to serve specific applications, unsupervised learning has emerged as an important field of research. In computer vision, unsu- pervised learning comes in various guises. We focus here on the unsupervised discovery and matching of object cate- gories among images in a collection, following the work of Cho et al. [12]. We show that the original approach can be reformulated and solved as a proper optimization problem. Experiments on several benchmarks establish the merit of our approach.",
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2019.00848",
          "url": "https://www.semanticscholar.org/paper/96f68a831d723dc0d841daa8e5c1f457603140d3",
          "authors": [
            "Huy V. Vo",
            "F. Bach",
            "Minsu Cho",
            "Kai Han",
            "Yann LeCun",
            "P. Pérez",
            "J. Ponce"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Explorations on high dimensional landscapes",
          "year": 2014,
          "citations": 65,
          "abstract": "Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. We provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/540393e544757f103efd549ac5196de117950f94",
          "authors": [
            "Levent Sagun",
            "V. U. Güney",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "An FPGA-based stream processor for embedded real-time vision with Convolutional Networks",
          "year": 2009,
          "citations": 63,
          "abstract": null,
          "venue": "2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops",
          "doi": "10.1109/ICCVW.2009.5457611",
          "url": "https://www.semanticscholar.org/paper/777de11e21d10cb627d109d0f64630115ff7823f",
          "authors": [
            "C. Farabet",
            "Cyril Poulet",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Energy-Based Models in Document Recognition and Computer Vision",
          "year": 2007,
          "citations": 60,
          "abstract": null,
          "venue": "IEEE International Conference on Document Analysis and Recognition",
          "doi": "10.1109/ICDAR.2007.107",
          "url": "https://www.semanticscholar.org/paper/c09c49d92d10a84e6efdbaf67d979bab2c22be3e",
          "authors": [
            "Yann LeCun",
            "S. Chopra",
            "Marc'Aurelio Ranzato",
            "Fu Jie Huang"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Singularity of the Hessian in Deep Learning",
          "year": 2016,
          "citations": 59,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5fbc36b30859f5f2f1af4f724531c94b1b1f926a",
          "authors": [
            "Levent Sagun",
            "L. Bottou",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Semantic Road Segmentation via Multi-scale Ensembles of Learned Features",
          "year": 2012,
          "citations": 58,
          "abstract": null,
          "venue": "ECCV Workshops",
          "doi": "10.1007/978-3-642-33868-7_58",
          "url": "https://www.semanticscholar.org/paper/2239f2a97d80b63466815e2e2d6ee993d156813e",
          "authors": [
            "J. Álvarez",
            "Yann LeCun",
            "T. Gevers",
            "Antonio M. López"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Structured sparse coding via lateral inhibition",
          "year": 2011,
          "citations": 58,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/044fddbc2e52add6d2b2c9d79544446854ebeb39",
          "authors": [
            "Arthur Szlam",
            "Karol Gregor",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "On the applications of multimedia processing to communications",
          "year": 1998,
          "citations": 58,
          "abstract": null,
          "venue": "Proceedings of the IEEE",
          "doi": "10.1109/5.664272",
          "url": "https://www.semanticscholar.org/paper/82cf2c37ed3eca25660ead9c034ecadcfb76fb4a",
          "authors": [
            "R. Cox",
            "B. Haskell",
            "Yann LeCun",
            "B. Shahraray",
            "L. Rabiner"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Reading checks with multilayer graph transformer networks",
          "year": 1997,
          "citations": 57,
          "abstract": null,
          "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
          "doi": "10.1109/ICASSP.1997.599580",
          "url": "https://www.semanticscholar.org/paper/25de919ccc51093737e2d61cea3037ee6bd3775f",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Yoshua Bengio"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Dynamic Factor Graphs for Time Series Modeling",
          "year": 2009,
          "citations": 54,
          "abstract": null,
          "venue": "ECML/PKDD",
          "doi": "10.1007/978-3-642-04174-7_9",
          "url": "https://www.semanticscholar.org/paper/678d67db7b65b07b6d4b941cc138a33dcdf47b81",
          "authors": [
            "Piotr Wojciech Mirowski",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Reading handwritten digits: a ZIP code recognition system",
          "year": 1992,
          "citations": 54,
          "abstract": null,
          "venue": "Computer",
          "doi": "10.1109/2.144441",
          "url": "https://www.semanticscholar.org/paper/52a3dca70cc21b540ad5d7b7b6e23744c20095f7",
          "authors": [
            "O. Matan",
            "H. Baird",
            "J. Bromley",
            "C. Burges",
            "J. Denker",
            "L. Jackel",
            "Yann LeCun",
            "E. Pednault",
            "William Satterfield",
            "C. E. Stenard",
            "Timothy J. Thompson"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Constrained neural network for unconstrained handwritten digit recognition",
          "year": 1990,
          "citations": 54,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3016c38f8f5f1ad479eb25ef199a5b357c999193",
          "authors": [
            "Yann LeCun",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "L. Jackel",
            "H. Baird"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution",
          "year": 2022,
          "citations": 53,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2210.08340",
          "url": "https://www.semanticscholar.org/paper/497e2d07c1dbf84913110c02cd2990eaea57a7cb",
          "authors": [
            "A. Zador",
            "B. Richards",
            "Bence Olveczky",
            "Sean Escola",
            "Y. Bengio",
            "K. Boahen",
            "M. Botvinick",
            "D. Chklovskii",
            "A. Churchland",
            "C. Clopath",
            "J. DiCarlo",
            "S. Ganguli",
            "J. Hawkins",
            "Konrad Koerding",
            "A. Koulakov",
            "Yann LeCun",
            "T. Lillicrap",
            "A. Marblestone",
            "B. Olshausen",
            "A. Pouget",
            "Cristina Savin",
            "T. Sejnowski",
            "Eero P. Simoncelli",
            "S. Solla",
            "David Sussillo",
            "A. Tolias",
            "Doris Y. Tsao"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "DjVu: analyzing and compressing scanned documents for Internet distribution",
          "year": 1999,
          "citations": 53,
          "abstract": null,
          "venue": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)",
          "doi": "10.1109/ICDAR.1999.791865",
          "url": "https://www.semanticscholar.org/paper/3089d4a41a3310a62bc2effd4a69577f79014f8f",
          "authors": [
            "P. Haffner",
            "L. Bottou",
            "P. Howard",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Implicit Rank-Minimizing Autoencoder",
          "year": 2020,
          "citations": 51,
          "abstract": "An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate the validity of the method on several image generation and representation learning tasks.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/10266bc9d3f6cf1966c846217b66ab5f26006446",
          "authors": [
            "Li Jing",
            "Jure Zbontar",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Handwritten character recognition using neural network architectures",
          "year": 1990,
          "citations": 50,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8f2b909fa1aad7e9f13603d721ff953325a4f97d",
          "authors": [
            "O. Matan",
            "R. Kiang",
            "C. E. Stenard",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "L. Jackel",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Model-Based Planning with Discrete and Continuous Actions",
          "year": 2017,
          "citations": 49,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c6d78e818b0585144578d80b4d42585cd616709b",
          "authors": [
            "Mikael Henaff",
            "William F. Whitney",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Convolutional Matching Pursuit and Dictionary Training",
          "year": 2010,
          "citations": 49,
          "abstract": "Here, {W,Z} are the dictionary and the coefficients, respectively, and zk is the kth column of Z. K, q, and λ are user selected parameters controlling the power of the model. More recently, many models with additional structure have been proposed. For example, in [9, 2], the dictionary elements are arranged in groups and the sparsity is on the group level. In [3, 5, 7], the dictionaries are constructed to be translation invariant. In the former work, the dictionary is constructed via a non-negative matrix factorization. In the latter two works, the construction is a convolutional analogue of 1.2 or an l variant, with 0 < p < 1. In this short note we work with greedy algorithms for solving the convolutional analogues of 1.1. Specifically, we demonstrate that sparse coding by matching pursuit and dictionary learning via K-SVD [1] can be used in the translation invariant setting.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/10c8b29d7820bab2bab0610b9211b6852f272002",
          "authors": [
            "Arthur Szlam",
            "K. Kavukcuoglu",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning and Leveraging World Models in Visual Representation Learning",
          "year": 2024,
          "citations": 48,
          "abstract": "Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2403.00504",
          "url": "https://www.semanticscholar.org/paper/4d1eadaa9a04e86aef2278ae13eb9fce644c9fc5",
          "authors": [
            "Q. Garrido",
            "Mahmoud Assran",
            "Nicolas Ballas",
            "Adrien Bardes",
            "Laurent Najman",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Neural Manifold Clustering and Embedding",
          "year": 2022,
          "citations": 48,
          "abstract": "Given a union of non-linear manifolds, non-linear subspace clustering or manifold clustering aims to cluster data points based on manifold structures and also learn to parameterize each manifold as a linear subspace in a feature space. Deep neural networks have the potential to achieve this goal under highly non-linear settings given their large capacity and flexibility. We argue that achieving manifold clustering with neural networks requires two essential ingredients: a domain-specific constraint that ensures the identification of the manifolds, and a learning algorithm for embedding each manifold to a linear subspace in the feature space. This work shows that many constraints can be implemented by data augmentation. For subspace feature learning, Maximum Coding Rate Reduction (MCR$^2$) objective can be used. Putting them together yields {\\em Neural Manifold Clustering and Embedding} (NMCE), a novel method for general purpose manifold clustering, which significantly outperforms autoencoder-based deep subspace clustering. Further, on more challenging natural image datasets, NMCE can also outperform other algorithms specifically designed for clustering. Qualitatively, we demonstrate that NMCE learns a meaningful and interpretable feature space. As the formulation of NMCE is closely related to several important Self-supervised learning (SSL) methods, we believe this work can help us build a deeper understanding on SSL representation learning.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3834edf530c639257555d41a3c67e128a22aefc0",
          "authors": [
            "Zengyi Li",
            "Yubei Chen",
            "Yann LeCun",
            "F. Sommer"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A multirange architecture for collision‐free off‐road robot navigation",
          "year": 2009,
          "citations": 48,
          "abstract": null,
          "venue": "J. Field Robotics",
          "doi": "10.1002/rob.20270",
          "url": "https://www.semanticscholar.org/paper/b48d78ed73144d69f6239696e55ba9596fe7813b",
          "authors": [
            "P. Sermanet",
            "R. Hadsell",
            "Marco Scoffier",
            "M. Grimes",
            "J. Ben",
            "A. Erkan",
            "Chris Crudele",
            "Urs Miller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "[Learning with computers].",
          "year": 1998,
          "citations": 48,
          "abstract": null,
          "venue": "Schweizer Monatsschrift fur Zahnmedizin = Revue mensuelle suisse d'odonto-stomatologie = Rivista mensile svizzera di odontologia e stomatologia",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/784cb31fc9fe06cfda34368b59258b0e5a7201e9",
          "authors": [
            "Cecilia Ka Yuk Chan",
            "Zoya A. Zorina",
            "Jesse R. Sparks",
            "S. López Ornat",
            "Christine D. Tsang",
            "Elena L. Grigorenko",
            "R. Lubow",
            "John C. Malone",
            "Michael Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "Thomas R. Minor",
            "Traci N. Plumb",
            "Hendrik Drachsler",
            "P. A. Kirschner",
            "Minoru Nakayama",
            "Rowena Santiago",
            "Ali Simsek",
            "Fang-Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "Richard P. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "Luis C. Lamb",
            "Artur d’Avila Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "Tracy Packiam Alloway",
            "Ulrike Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "Gabriele Kern-Isberner",
            "Joost Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C.-J. Olsson",
            "Lars Nyberg",
            "R. Freivalds",
            "Lynne Hall",
            "M. Hall",
            "Ulrike Hanke",
            "Lin S. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "Paul C. Burnett",
            "M. Cannon",
            "Amy C. Edmondson",
            "Pitoyo Hartono",
            "A. Callender",
            "Rachel Barr",
            "Natalie Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohd. Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "David A. Kolb",
            "R. Reilly",
            "Klaus Nielsen",
            "Patricia Couvillon",
            "Heather King",
            "Justin Dillon",
            "Delia Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "Fernand Gobet",
            "Peter C. R. Lane",
            "Som Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "Don M. Tucker",
            "Paul J. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "Daniel Ansari",
            "K. Lionello-DeNolf",
            "John Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. U. Hoppe",
            "Krisztina Molnar",
            "Erica de Vries",
            "Emmanuel G. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "Tristan Cazenave",
            "Ton Jong",
            "Jan Meij",
            "J. Gavelek",
            "Ailing Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "Michael J. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "Dong-oh Seo",
            "M. Monfils",
            "Anthony A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "Michael J. Kennedy",
            "Michael Jackson",
            "Terezinha Nunes",
            "B. Jackling",
            "R. Howard",
            "William G. Kennedy",
            "L. C. Drickamer"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors",
          "year": 2022,
          "citations": 46,
          "abstract": "Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2205.10279",
          "url": "https://www.semanticscholar.org/paper/1e9fbd0e9d047c192d7e2a75f0034400c5c403c7",
          "authors": [
            "Ravid Shwartz-Ziv",
            "Micah Goldblum",
            "Hossein Souri",
            "Sanyam Kapoor",
            "Chen Zhu",
            "Yann LeCun",
            "A. Wilson"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Saturating Auto-Encoders",
          "year": 2013,
          "citations": 44,
          "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0a9cbc7484a0da0962b39ca880ee63b398746170",
          "authors": [
            "Rostislav Goroshin",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Reverse Engineering Self-Supervised Learning",
          "year": 2023,
          "citations": 43,
          "abstract": "Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provide valuable insights into SSL's representation learning mechanisms and their impact on performance across different sets of classes.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2305.15614",
          "url": "https://www.semanticscholar.org/paper/a2b8ff257658b8291deb9e40ec1c164c8fefeb06",
          "authors": [
            "Ido Ben-Shaul",
            "Ravid Shwartz-Ziv",
            "Tomer Galanti",
            "S. Dekel",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Spectral Regularizer for Unsupervised Disentanglement",
          "year": 2018,
          "citations": 43,
          "abstract": "A generative model with a disentangled representation allows for independent control over different aspects of the output. Learning disentangled representations has been a recent topic of great interest, but it remains poorly understood. We show that even for GANs that do not possess disentangled representations, one can find curved trajectories in latent space over which local disentanglement occurs. These trajectories are found by iteratively following the leading right-singular vectors of the Jacobian of the generator with respect to its input. Based on this insight, we describe an efficient regularizer that aligns these vectors with the coordinate axes, and show that it can be used to induce disentangled representations in GANs, in a completely unsupervised manner.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0ac54252eab1ac7641e03bc9b7344e2064e06405",
          "authors": [
            "A. Ramesh",
            "Youngduck Choi",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Orthogonal RNNs and Long-Memory Tasks",
          "year": 2016,
          "citations": 43,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/4ca151307e3be93c6cd14ed403f6162892e7fbed",
          "authors": [
            "Mikael Henaff",
            "Arthur Szlam",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The SSL Interplay: Augmentations, Inductive Bias, and Generalization",
          "year": 2023,
          "citations": 41,
          "abstract": "Self-supervised learning (SSL) has emerged as a powerful framework to learn representations from raw data without supervision. Yet in practice, engineers face issues such as instability in tuning optimizers and collapse of representations during training. Such challenges motivate the need for a theory to shed light on the complex interplay between the choice of data augmentation, network architecture, and training algorithm. We study such an interplay with a precise analysis of generalization performance on both pretraining and downstream tasks in a theory friendly setup, and highlight several insights for SSL practitioners that arise from our theory.",
          "venue": "International Conference on Machine Learning",
          "doi": "10.48550/arXiv.2302.02774",
          "url": "https://www.semanticscholar.org/paper/034081ba8bd12b9466414fce3e885451a92b075a",
          "authors": [
            "Vivien A. Cabannes",
            "B. Kiani",
            "Randall Balestriero",
            "Yann LeCun",
            "A. Bietti"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Introduction to latent variable energy-based models: a path toward autonomous machine intelligence",
          "year": 2023,
          "citations": 41,
          "abstract": "Current automated systems have crucial limitations that need to be addressed before artificial intelligence can reach human-like levels and bring new technological revolutions. Among others, our societies still lack level-5 self-driving cars, domestic robots, and virtual assistants that learn reliable world models, reason, and plan complex action sequences. In these notes, we summarize the main ideas behind the architecture of autonomous intelligence of the future proposed by Yann LeCun. In particular, we introduce energy-based and latent variable models and combine their advantages in the building block of LeCun’s proposal, that is, in the hierarchical joint-embedding predictive architecture.",
          "venue": "Journal of Statistical Mechanics: Theory and Experiment",
          "doi": "10.1088/1742-5468/ad292b",
          "url": "https://www.semanticscholar.org/paper/9502c180be0ebc92fcf2085ea90c3cb45280a6bc",
          "authors": [
            "Anna Dawid",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Boosting and Other Machine Learning Algorithms",
          "year": 1994,
          "citations": 41,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": "10.1016/b978-1-55860-335-6.50015-5",
          "url": "https://www.semanticscholar.org/paper/b560bb5b24cd89a68ce20bd30d4c0f22ccae7d82",
          "authors": [
            "H. Drucker",
            "Corinna Cortes",
            "L. Jackel",
            "Yann LeCun",
            "V. Vapnik"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Hardware requirements for neural network pattern classifiers: a case study and implementation",
          "year": 1992,
          "citations": 41,
          "abstract": null,
          "venue": "IEEE Micro",
          "doi": "10.1109/40.124378",
          "url": "https://www.semanticscholar.org/paper/bba485f09c82dd2a4261ca9648f2a4cfcd0d343f",
          "authors": [
            "B. Boser",
            "Eduard Säckinger",
            "J. Bromley",
            "Yann LeCun",
            "L. Jackel"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Self-supervised learning of Split Invariant Equivariant representations",
          "year": 2023,
          "citations": 40,
          "abstract": "Recent progress has been made towards learning invariant or equivariant representations with self-supervised learning. While invariant methods are evaluated on large scale datasets, equivariant ones are evaluated in smaller, more controlled, settings. We aim at bridging the gap between the two in order to learn more diverse representations that are suitable for a wide range of tasks. We start by introducing a dataset called 3DIEBench, consisting of renderings from 3D models over 55 classes and more than 2.5 million images where we have full control on the transformations applied to the objects. We further introduce a predictor architecture based on hypernetworks to learn equivariant representations with no possible collapse to invariance. We introduce SIE (Split Invariant-Equivariant) which combines the hypernetwork-based predictor with representations split in two parts, one invariant, the other equivariant, to learn richer representations. We demonstrate significant performance gains over existing methods on equivariance related tasks from both a qualitative and quantitative point of view. We further analyze our introduced predictor and show how it steers the learned latent space. We hope that both our introduced dataset and approach will enable learning richer representations without supervision in more complex scenarios. Code and data are available at https://github.com/facebookresearch/SIE.",
          "venue": "International Conference on Machine Learning",
          "doi": "10.48550/arXiv.2302.10283",
          "url": "https://www.semanticscholar.org/paper/10923e416d15ab36161f4ab9ad40aa15bb91f541",
          "authors": [
            "Q. Garrido",
            "Laurent Najman",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning Stable Group Invariant Representations with Convolutional Networks",
          "year": 2013,
          "citations": 39,
          "abstract": "Transformation groups, such as translations or rotations, effectively express part of the variability observed in many recognition problems. The group structure enables the construction of invariant signal representations with appealing mathematical properties, where convolutions, together with pooling operators, bring stability to additive and geometric perturbations of the input. Whereas physical transformation groups are ubiquitous in image and audio applications, they do not account for all the variability of complex signal classes. \nWe show that the invariance properties built by deep convolutional networks can be cast as a form of stable group invariance. The network wiring architecture determines the invariance group, while the trainable filter coefficients characterize the group action. We give explanatory examples which illustrate how the network architecture controls the resulting invariance group. We also explore the principle by which additional convolutional layers induce a group factorization enabling more abstract, powerful invariant representations.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/a219e69af081740e0faab08028557965620d8303",
          "authors": [
            "Joan Bruna",
            "Arthur Szlam",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Fast Approximations to Structured Sparse Coding and Applications to Object Classification",
          "year": 2012,
          "citations": 39,
          "abstract": "We describe a method for fast approximation of sparse coding. A given input vector is passed through a binary tree. Each leaf of the tree contains a subset of dictionary elements. The coefficients corresponding to these dictionary elements are allowed to be nonzero and their values are calculated quickly by multiplication with a precomputed pseudoinverse. The tree parameters, the dictionary, and the subsets of the dictionary corresponding to each leaf are learned. In the process of describing this algorithm, we discuss the more general problem of learning the groups in group structured sparse modeling. We show that our method creates good sparse representations by using it in the object recognition framework of [1,2]. Implementing our own fast version of the SIFT descriptor the whole system runs at 20 frames per second on 321 ×481 sized images on a laptop with a quad-core cpu, while sacrificing very little accuracy on the Caltech 101, Caltech 256, and 15 scenes benchmarks.",
          "venue": "European Conference on Computer Vision",
          "doi": "10.1007/978-3-642-33715-4_15",
          "url": "https://www.semanticscholar.org/paper/e884feeab763a2e4a4d1279267a42d17c67c015c",
          "authors": [
            "Arthur Szlam",
            "Karol Gregor",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Mapping and planning under uncertainty in mobile robots with long-range perception",
          "year": 2008,
          "citations": 38,
          "abstract": null,
          "venue": "2008 IEEE/RSJ International Conference on Intelligent Robots and Systems",
          "doi": "10.1109/IROS.2008.4651203",
          "url": "https://www.semanticscholar.org/paper/1331f67dd0c39845a7b129817a0697d798ffc548",
          "authors": [
            "P. Sermanet",
            "R. Hadsell",
            "Marco Scoffier",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Word normalization for on-line handwritten word recognition",
          "year": 1994,
          "citations": 38,
          "abstract": "We introduce a new approach to normalizing words written with an electronic stylus that applies to all styles of handwriting (upper case, lower case, printed, cursive, or mixed). A geometrical model of the word spatial structure is fitted to the pen trajectory using the expectation-maximisation algorithm. The fitting process maximizes the likelihood of the trajectory given the model and a set a priors on its parameters. The method was evaluated and integrated to a recognition system that combines neural networks and hidden Markov models.",
          "venue": "Signal Processing",
          "doi": "10.1109/ICPR.1994.576966",
          "url": "https://www.semanticscholar.org/paper/4b336597d72bef3b5b6964a88040129edcaf8dcd",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Masked Siamese ConvNets",
          "year": 2022,
          "citations": 37,
          "abstract": "Self-supervised learning has shown superior performances over supervised methods on various vision benchmarks. The siamese network, which encourages embeddings to be invariant to distortions, is one of the most successful self-supervised visual representation learning approaches. Among all the augmentation methods, masking is the most general and straightforward method that has the potential to be applied to all kinds of input and requires the least amount of domain knowledge. However, masked siamese networks require particular inductive bias and practically only work well with Vision Transformers. This work empirically studies the problems behind masked siamese networks with ConvNets. We propose several empirical designs to overcome these problems gradually. Our method performs competitively on low-shot image classification and outperforms previous methods on object detection benchmarks. We discuss several remaining issues and hope this work can provide useful data points for future general-purpose self-supervised learning.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2206.07700",
          "url": "https://www.semanticscholar.org/paper/df1af149a7b9aaa2ef0bb05adf639e9ae6b56249",
          "authors": [
            "L. Jing",
            "Jiachen Zhu",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features",
          "year": 2023,
          "citations": 36,
          "abstract": "Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2307.12698",
          "url": "https://www.semanticscholar.org/paper/3c1e43b7d3f5fd42a06c65e3aafe6d8f4a606d5c",
          "authors": [
            "Adrien Bardes",
            "J. Ponce",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations",
          "year": 2018,
          "citations": 36,
          "abstract": "Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden unit), or embedding-free units such as image pixels.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/446efa0bcf3528b51332a12495cb56784dd8bad3",
          "authors": [
            "Zhilin Yang",
            "J. Zhao",
            "Bhuwan Dhingra",
            "Kaiming He",
            "William W. Cohen",
            "R. Salakhutdinov",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Convolutional nets and watershed cuts for real-time semantic Labeling of RGBD videos",
          "year": 2014,
          "citations": 36,
          "abstract": null,
          "venue": "Journal of machine learning research",
          "doi": "10.5555/2627435.2697077",
          "url": "https://www.semanticscholar.org/paper/8934e44350ca741f7623bf26f89d43835ece0113",
          "authors": [
            "C. Couprie",
            "C. Farabet",
            "Laurent Najman",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Globally trained handwritten word recognizer using spatial representation, space displacement neural networks and hidden Markov models",
          "year": 1993,
          "citations": 36,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5fd7cbbecb090f32c198a1b2cc4e2582e06ea431",
          "authors": [
            "Yoshua Bengio",
            "Yann LeCun",
            "D. Henderson"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Guest Editorial: Deep Learning",
          "year": 2015,
          "citations": 35,
          "abstract": null,
          "venue": "International Journal of Computer Vision",
          "doi": "10.1007/s11263-015-0813-1",
          "url": "https://www.semanticscholar.org/paper/08ee53dac878ee6ab1e5cf06824713bed614e17c",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Geoffrey E. Hinton",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Unsupervised Feature Learning from Temporal Data",
          "year": 2015,
          "citations": 35,
          "abstract": "Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/394cbf2d589eadcfbdbdaaed65c77532b9c856af",
          "authors": [
            "Ross Goroshin",
            "Joan Bruna",
            "Jonathan Tompson",
            "D. Eigen",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "projUNN: efficient method for training deep networks with unitary matrices",
          "year": 2022,
          "citations": 34,
          "abstract": "In learning with recurrent or very deep feed-forward networks, employing unitary matrices in each layer can be very effective at maintaining long-range stability. However, restricting network parameters to be unitary typically comes at the cost of expensive parameterizations or increased training runtime. We propose instead an efficient method based on rank-$k$ updates -- or their rank-$k$ approximation -- that maintains performance at a nearly optimal training runtime. We introduce two variants of this method, named Direct (projUNN-D) and Tangent (projUNN-T) projected Unitary Neural Networks, that can parameterize full $N$-dimensional unitary or orthogonal matrices with a training runtime scaling as $O(kN^2)$. Our method either projects low-rank gradients onto the closest unitary matrix (projUNN-T) or transports unitary matrices in the direction of the low-rank gradient (projUNN-D). Even in the fastest setting ($k=1$), projUNN is able to train a model's unitary parameters to reach comparable performances against baseline implementations. In recurrent neural network settings, projUNN closely matches or exceeds benchmarked results from prior unitary neural networks. Finally, we preliminarily explore projUNN in training orthogonal convolutional neural networks, which are currently unable to outperform state of the art models but can potentially enhance stability and robustness at large depth.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2203.05483",
          "url": "https://www.semanticscholar.org/paper/d43f665fad45256659dce9e9d2c2a05a6383e5b6",
          "authors": [
            "B. Kiani",
            "Randall Balestriero",
            "Yann LeCun",
            "S. Lloyd"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities",
          "year": 2013,
          "citations": 34,
          "abstract": "Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.",
          "venue": "International Conference on Learning Representations",
          "doi": "10.1007/978-3-642-42054-2_55",
          "url": "https://www.semanticscholar.org/paper/f72c079d9179cfaada1135a7e4c77d48b6309a30",
          "authors": [
            "T. Vatanen",
            "T. Raiko",
            "Harri Valpola",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Dynamic auto-encoders for semantic indexing",
          "year": 2010,
          "citations": 34,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/80922663aaa7b09e86276ab97210ab2372d3f61a",
          "authors": [
            "Piotr Wojciech Mirowski",
            "Marc'Aurelio Ranzato",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "EBLearn: Open-Source Energy-Based Learning in C++",
          "year": 2009,
          "citations": 34,
          "abstract": null,
          "venue": "IEEE International Conference on Tools with Artificial Intelligence",
          "doi": "10.1109/ICTAI.2009.28",
          "url": "https://www.semanticscholar.org/paper/78e5bca056ffc6186400ba540a0c0f43df909a12",
          "authors": [
            "P. Sermanet",
            "K. Kavukcuoglu",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Online Learning for Offroad Robots: Spatial Label Propagation to Learn Long-Range Traversability",
          "year": 2007,
          "citations": 34,
          "abstract": null,
          "venue": "Robotics: Science and Systems",
          "doi": "10.15607/RSS.2007.III.003",
          "url": "https://www.semanticscholar.org/paper/6e06fa79a4a7fbbda36ae4d12cd0d5135b67d28d",
          "authors": [
            "R. Hadsell",
            "P. Sermanet",
            "J. Ben",
            "A. Erkan",
            "Jeff Han",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Binary embeddings with structured hashed projections",
          "year": 2015,
          "citations": 33,
          "abstract": "We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudorandom projection is described by a matrix, where not all entries are independent random variables but instead a fixed \"budget of randomness\" is distributed across the matrix. Such matrices can be efficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/16a26289d7c37e6a0179fa57b14a327286696d33",
          "authors": [
            "A. Choromańska",
            "K. Choromanski",
            "Mariusz Bojarski",
            "Tony Jebara",
            "Sanjiv Kumar",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The Power and Limits of Deep Learning",
          "year": 2018,
          "citations": 32,
          "abstract": "Artificial intelligence (AI) is advancing very rapidly. I’ve had a front-row seat for a lot of the recent progress—first at Bell Labs (which was renamed AT&T Labs in 1996, while I was there) and th...",
          "venue": "Research technology management",
          "doi": "10.1080/08956308.2018.1516928",
          "url": "https://www.semanticscholar.org/paper/9e6006531597c0f8422e25de7d62c068ad9e68ee",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Off Line Recognition of Handwritten Postal Words Using Neural Networks",
          "year": 1993,
          "citations": 32,
          "abstract": null,
          "venue": "International journal of pattern recognition and artificial intelligence",
          "doi": "10.1142/S0218001493000340",
          "url": "https://www.semanticscholar.org/paper/065b0af1bc05ea4f1fbd2afc50a96b0ef1698c8d",
          "authors": [
            "C. Burges",
            "J. Ben",
            "J. Denker",
            "Yann LeCun",
            "C. Nohl"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "GEMINI: Gradient Estimation Through Matrix Inversion After Noise Injection",
          "year": 1988,
          "citations": 32,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/19ec5c7be1fe5e7088f3a042d3160ede757f5902",
          "authors": [
            "Yann LeCun",
            "C. Galland",
            "Geoffrey E. Hinton"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A hierarchical loss and its problems when classifying non-hierarchically",
          "year": 2017,
          "citations": 31,
          "abstract": "Failing to distinguish between a sheepdog and a skyscraper should be worse and penalized more than failing to distinguish between a sheepdog and a poodle; after all, sheepdogs and poodles are both breeds of dogs. However, existing metrics of failure (so-called “loss” or “win”) used in textual or visual classification/recognition via neural networks seldom leverage a-priori information, such as a sheepdog being more similar to a poodle than to a skyscraper. We define a metric that, inter alia, can penalize failure to distinguish between a sheepdog and a skyscraper more than failure to distinguish between a sheepdog and a poodle. Unlike previously employed possibilities, this metric is based on an ultrametric tree associated with any given tree organization into a semantically meaningful hierarchy of a classifier’s classes. An ultrametric tree is a tree with a so-called ultrametric distance metric such that all leaves are at the same distance from the root. Unfortunately, extensive numerical experiments indicate that the standard practice of training neural networks via stochastic gradient descent with random starting points often drives down the hierarchical loss nearly as much when minimizing the standard cross-entropy loss as when trying to minimize the hierarchical loss directly. Thus, this hierarchical loss is unreliable as an objective for plain, randomly started stochastic gradient descent to minimize; the main value of the hierarchical loss may be merely as a meaningful metric of success of a classifier.",
          "venue": "PLoS ONE",
          "doi": "10.1371/journal.pone.0226222",
          "url": "https://www.semanticscholar.org/paper/35903ec587d50db2d1ae5b26189b0f5c7771edb2",
          "authors": [
            "C. Wu",
            "M. Tygert",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Universum Prescription: Regularization Using Unlabeled Data",
          "year": 2015,
          "citations": 31,
          "abstract": "\n \n This paper shows that simply prescribing \"none of the above\" labels to unlabeled data has a beneficial regularization effect to supervised learning. We call it universum prescription by the fact that the prescribed labels cannot be one of the supervised labels. In spite of its simplicity, universum prescription obtained competitive results in training deep convolutional networks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitative justification of these approaches using Rademacher complexity is presented. The effect of a regularization parameter — probability of sampling from unlabeled data — is also studied empirically.\n \n",
          "venue": "AAAI Conference on Artificial Intelligence",
          "doi": "10.1609/aaai.v31i1.10768",
          "url": "https://www.semanticscholar.org/paper/a9b4fc31e6c0253a8924d6fcd19c70c4ac6f3db2",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Differentially- and non-differentially-private random decision trees",
          "year": 2014,
          "citations": 31,
          "abstract": "We consider supervised learning with random decision trees, where the tree construction is completely random. The method is popularly used and works well in practice despite the simplicity of the setting, but its statistical mechanism is not yet well-understood. In this paper we provide strong theoretical guarantees regarding learning with random decision trees. We analyze and compare three different variants of the algorithm that have minimal memory requirements: majority voting, threshold averaging and probabilistic averaging. The random structure of the tree enables us to adapt these methods to a differentially-private setting thus we also propose differentially-private versions of all three schemes. We give upper-bounds on the generalization error and mathematically explain how the accuracy depends on the number of random decision trees. Furthermore, we prove that only logarithmic (in the size of the dataset) number of independently selected random decision trees suffice to correctly classify most of the data, even when differential-privacy guarantees must be maintained. We empirically show that majority voting and threshold averaging give the best accuracy, also for conservative users requiring high privacy guarantees. Furthermore, we demonstrate that a simple majority voting rule is an especially good candidate for the differentially-private classifier since it is much less sensitive to the choice of forest parameters than other methods.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/a49c17b31a6d3a526b030927dace6919b6a68603",
          "authors": [
            "Mariusz Bojarski",
            "A. Choromańska",
            "K. Choromanski",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Handwritten Digit Recognition: Applications of Neural Net Chips and Automatic Learning",
          "year": 1989,
          "citations": 30,
          "abstract": null,
          "venue": "NATO Neurocomputing",
          "doi": "10.1007/978-3-642-76153-9_35",
          "url": "https://www.semanticscholar.org/paper/cdefacc5f4e4292936ea9bd542e0a46c6c49905c",
          "authors": [
            "Yann LeCun",
            "L. Jackel",
            "B. Boser",
            "J. Denker",
            "H. Graf",
            "Isabelle M Guyon",
            "D. Henderson",
            "R. E. Howard",
            "W. Hubbard"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "EMP-SSL: Towards Self-Supervised Learning in One Training Epoch",
          "year": 2023,
          "citations": 29,
          "abstract": "Recently, self-supervised learning (SSL) has achieved tremendous success in learning image representation. Despite the empirical success, most self-supervised learning methods are rather\"inefficient\"learners, typically taking hundreds of training epochs to fully converge. In this work, we show that the key towards efficient self-supervised learning is to increase the number of crops from each image instance. Leveraging one of the state-of-the-art SSL method, we introduce a simplistic form of self-supervised learning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL) that does not rely on many heuristic techniques for SSL such as weight sharing between the branches, feature-wise normalization, output quantization, and stop gradient, etc, and reduces the training epochs by two orders of magnitude. We show that the proposed method is able to converge to 85.1% on CIFAR-10, 58.5% on CIFAR-100, 38.1% on Tiny ImageNet and 58.5% on ImageNet-100 in just one epoch. Furthermore, the proposed method achieves 91.5% on CIFAR-10, 70.1% on CIFAR-100, 51.5% on Tiny ImageNet and 78.9% on ImageNet-100 with linear probing in less than ten training epochs. In addition, we show that EMP-SSL shows significantly better transferability to out-of-domain datasets compared to baseline SSL methods. We will release the code in https://github.com/tsb0601/EMP-SSL.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2304.03977",
          "url": "https://www.semanticscholar.org/paper/5367ca1c122a0806549e484fb488a977b4334777",
          "authors": [
            "Shengbang Tong",
            "Yubei Chen",
            "Y. Ma",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning about an exponential amount of conditional distributions",
          "year": 2019,
          "citations": 29,
          "abstract": "We introduce the Neural Conditioner (NC), a self-supervised machine able to learn about all the conditional distributions of a random vector $X$. The NC is a function $NC(x \\cdot a, a, r)$ that leverages adversarial training to match each conditional distribution $P(X_r|X_a=x_a)$. After training, the NC generalizes to sample from conditional distributions never seen, including the joint distribution. The NC is also able to auto-encode examples, providing data representations useful for downstream classification tasks. In sum, the NC integrates different self-supervised tasks (each being the estimation of a conditional distribution) and levels of supervision (partially observed data) seamlessly into a single learning experience.",
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/4e48a5a00a79cb586c1c912507ec6e52890cdeab",
          "authors": [
            "Mohamed Ishmael Belghazi",
            "M. Oquab",
            "Yann LeCun",
            "David Lopez-Paz"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients",
          "year": 2013,
          "citations": 29,
          "abstract": "Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.",
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b4b0a3a748dfd2618ffcf1f94411e339e1e78775",
          "authors": [
            "T. Schaul",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Machine Learning and the Spatial Structure of House Prices and Housing Returns",
          "year": 2008,
          "citations": 29,
          "abstract": null,
          "venue": "",
          "doi": "10.2139/ssrn.1316046",
          "url": "https://www.semanticscholar.org/paper/35d47b5d82e1cdb4d70de7bf35cdfe970d793f35",
          "authors": [
            "Andrew Caplin",
            "S. Chopra",
            "John Leahy",
            "Yann LeCun",
            "Trivikraman Thampy"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Self-Supervised Learning with Lie Symmetries for Partial Differential Equations",
          "year": 2023,
          "citations": 28,
          "abstract": "Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation models for PDEs. Code: https://github.com/facebookresearch/SSLForPDEs.",
          "venue": "Neural Information Processing Systems",
          "doi": "10.48550/arXiv.2307.05432",
          "url": "https://www.semanticscholar.org/paper/5dc01119b2e4911b0c1ae61233e36efe78588aed",
          "authors": [
            "G. Mialon",
            "Q. Garrido",
            "Hannah Lawrence",
            "Danyal Rehman",
            "Yann LeCun",
            "B. Kiani"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Discovering the hidden structure of house prices with a non-parametric latent manifold model",
          "year": 2007,
          "citations": 28,
          "abstract": null,
          "venue": "Knowledge Discovery and Data Mining",
          "doi": "10.1145/1281192.1281214",
          "url": "https://www.semanticscholar.org/paper/9f4028fea3fcc99776391d66d4e40edbe78388b3",
          "authors": [
            "S. Chopra",
            "Trivikraman Thampy",
            "John Leahy",
            "Andrew Caplin",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Real Time Voice Processing with Audiovisual Feedback: Toward Autonomous Agents with Perfect Pitch",
          "year": 2002,
          "citations": 28,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6986517700641f245e908e64c2acf6dd5f333b95",
          "authors": [
            "L. Saul",
            "Daniel D. Lee",
            "C. Isbell",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Memory-based character recognition using a transformation invariant metric",
          "year": 1994,
          "citations": 28,
          "abstract": null,
          "venue": "Signal Processing",
          "doi": "10.1109/ICPR.1994.576916",
          "url": "https://www.semanticscholar.org/paper/be09fa4b8fbf399d1f2ce2263556d90f0661fe1f",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun",
            "J. Denker"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Writer independent and writer adaptive neural network for on-line character recognition",
          "year": 1992,
          "citations": 28,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/de55322662f9538bfdc422498d6184bf2da4266c",
          "authors": [
            "Isabelle M Guyon",
            "D. Henderson",
            "P. Albrecht",
            "Yann LeCun",
            "J. Denker"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Sparse and Locally Shift Invariant Feature Extractor Applied to Document Images",
          "year": 2007,
          "citations": 27,
          "abstract": null,
          "venue": "IEEE International Conference on Document Analysis and Recognition",
          "doi": "10.1109/ICDAR.2007.35",
          "url": "https://www.semanticscholar.org/paper/3cfff20568fe1964b407e2a4452f6064ca794f3c",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "An Information-Theoretic Perspective on Variance-Invariance-Covariance Regularization",
          "year": 2023,
          "citations": 26,
          "abstract": "Variance-Invariance-Covariance Regularization (VICReg) is a self-supervised learning (SSL) method that has shown promising results on a variety of tasks. However, the fundamental mechanisms underlying VICReg remain unexplored. In this paper, we present an information-theoretic perspective on the VICReg objective. We begin by deriving information-theoretic quantities for deterministic networks as an alternative to unrealistic stochastic network assumptions. We then relate the optimization of the VICReg objective to mutual information optimization, highlighting underlying assumptions and facilitating a constructive comparison with other SSL algorithms and derive a generalization bound for VICReg, revealing its inherent advantages for downstream tasks. Building on these results, we introduce a family of SSL methods derived from information-theoretic principles that outperform existing SSL techniques.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2303.00633",
          "url": "https://www.semanticscholar.org/paper/1ea4f4dcedbbe6d10aad30c3cb02fe2b0572b090",
          "authors": [
            "Ravid Shwartz-Ziv",
            "Randall Balestriero",
            "Kenji Kawaguchi",
            "Tim G. J. Rudner",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment",
          "year": 2022,
          "citations": 26,
          "abstract": "Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision-Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-modal fusion deep into the uni-modal backbones during pre-training and removes fusion-specific transformer layers, further reducing memory requirements. Extensive experiments on a wide range of vision- and vision-language downstream tasks demonstrate the effectiveness of VoLTA on fine-grained applications without compromising the coarse-grained downstream performance, often outperforming methods using significantly more caption and box annotations.",
          "venue": "Trans. Mach. Learn. Res.",
          "doi": "10.48550/arXiv.2210.04135",
          "url": "https://www.semanticscholar.org/paper/0ee11b28a9ce49d3030cab11f1178fa5abae9c3b",
          "authors": [
            "Shraman Pramanick",
            "Li Jing",
            "Sayan Nag",
            "Jiachen Zhu",
            "Hardik Shah",
            "Yann LeCun",
            "Ramalingam Chellappa"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Fast Approximation of Rotations and Hessians matrices",
          "year": 2014,
          "citations": 26,
          "abstract": "A new method to represent and approximate rotation matrices is introduced. The method represents approximations of a rotation matrix $Q$ with linearithmic complexity, i.e. with $\\frac{1}{2}n\\lg(n)$ rotations over pairs of coordinates, arranged in an FFT-like fashion. The approximation is \"learned\" using gradient descent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ is a diagonal matrix. It can be used to approximate covariance matrix of Gaussian models in order to speed up inference, or to estimate and track the inverse Hessian of an objective function by relating changes in parameters to changes in gradient along the trajectory followed by the optimization procedure. Experiments were conducted to approximate synthetic matrices, covariance matrices of real data, and Hessian matrices of objective functions involved in machine learning problems.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/cd7c5cbc246e42ebb1368f81ef403c988a1d4c89",
          "authors": [
            "Michaël Mathieu",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Causal graph-based video segmentation",
          "year": 2013,
          "citations": 26,
          "abstract": "Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. The algorithm may be trivially extended to video segmentation by considering a video as a 3D volume, however, this can not be the case for causal segmentation, when subsequent frames are unknown. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real time applications.",
          "venue": "2013 IEEE International Conference on Image Processing",
          "doi": "10.1109/ICIP.2013.6738875",
          "url": "https://www.semanticscholar.org/paper/c0d373ab0fc50663a2e638f2067dedaa9e2b2c7c",
          "authors": [
            "C. Couprie",
            "C. Farabet",
            "Yann LeCun",
            "Laurent Najman"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Language, common sense, and the Winograd schema challenge",
          "year": 2023,
          "citations": 25,
          "abstract": null,
          "venue": "Artificial Intelligence",
          "doi": "10.1016/j.artint.2023.104031",
          "url": "https://www.semanticscholar.org/paper/f875c2de4a3ccee670cc76a81b1dfd111bd40f64",
          "authors": [
            "Jacob Browning",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "TiCo: Transformation Invariance and Covariance Contrast for Self-Supervised Visual Representation Learning",
          "year": 2022,
          "citations": 25,
          "abstract": "We present Transformation Invariance and Covariance Contrast (TiCo) for self-supervised visual representation learning. Similar to other recent self-supervised learning methods, our method is based on maximizing the agreement among embeddings of different distorted versions of the same image, which pushes the encoder to produce transformation invariant representations. To avoid the trivial solution where the encoder generates constant vectors, we regularize the covariance matrix of the embeddings from different images by penalizing low rank solutions. By jointly minimizing the transformation invariance loss and covariance contrast loss, we get an encoder that is able to produce useful representations for downstream tasks. We analyze our method and show that it can be viewed as a variant of MoCo with an implicit memory bank of unlimited size at no extra memory cost. This makes our method perform better than alternative methods when using small batch sizes. TiCo can also be seen as a modification of Barlow Twins. By connecting the contrastive and redundancy-reduction methods together, TiCo gives us new insights into how joint embedding methods work.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2206.10698",
          "url": "https://www.semanticscholar.org/paper/9539f3b6ccabf66cf54acdafd8b95421b9c2b683",
          "authors": [
            "Jiachen Zhu",
            "R. M. Moraes",
            "Serkan Karakulak",
            "Vlad Sobol",
            "A. Canziani",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Prediction Under Uncertainty with Error-Encoding Networks",
          "year": 2017,
          "citations": 25,
          "abstract": "In this work we introduce a new framework for performing temporal predictions in the presence of uncertainty. It is based on a simple idea of disentangling com- ponents of the future state which are predictable from those which are inherently unpredictable, and encoding the unpredictable components into a low-dimensional latent variable which is fed into the forward model. Our method uses a simple su- pervised training objective which is fast and easy to train. We evaluate it in the context of video prediction on multiple datasets and show that it is able to consi- tently generate diverse predictions without the need for alternating minimization over a latent space or adversarial training.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/bfdb693df3a04fa9645233c444ccd8ec16c6c477",
          "authors": [
            "Mikael Henaff",
            "J. Zhao",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Inspirational Adversarial Image Generation",
          "year": 2021,
          "citations": 24,
          "abstract": "The task of image generation started receiving some attention from artists and designers, providing inspiration for new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control over the output. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user’s choosing by performing several optimization steps to recover optimal parameters from the model’s latent space. We tested several exploration methods from classical gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so they can even be used without numerical criterion nor inspirational image, only with human preferences. Thus, by iterating on one’s preferences we can make robust facial composite or fashion generation algorithms. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.",
          "venue": "IEEE Transactions on Image Processing",
          "doi": "10.1109/TIP.2021.3065845",
          "url": "https://www.semanticscholar.org/paper/836e4a8b665f46729e55e7674b4d8b0df6c1d091",
          "authors": [
            "Baptiste Rozière",
            "M. Rivière",
            "O. Teytaud",
            "J. Rapin",
            "Yann LeCun",
            "C. Couprie"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Reassessing FHA Risk",
          "year": 2010,
          "citations": 24,
          "abstract": null,
          "venue": "",
          "doi": "10.3386/W15802",
          "url": "https://www.semanticscholar.org/paper/cb4ac759471a80cc5d15fd36361c6c73c407dd97",
          "authors": [
            "Diego Aragon",
            "Andrew Caplin",
            "S. Chopra",
            "John Leahy",
            "Yann LeCun",
            "Marco Scoffier",
            "Joseph S. Tracy"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Time-Delay Neural Networks and Independent Component Analysis for EEG-Based Prediction of Epileptic Seizures Propagation",
          "year": 2007,
          "citations": 24,
          "abstract": null,
          "venue": "AAAI Conference on Artificial Intelligence",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0bb4afa755f8694a9c8bdac67149907833637b42",
          "authors": [
            "Piotr Wojciech Mirowski",
            "D. Madhavan",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Reverse TDNN: An Architecture For Trajectory Generation",
          "year": 1991,
          "citations": 24,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/040800e88fbdff598fb85ea82c12f94c3939989f",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "GLoMo: Unsupervised Learning of Transferable Relational Graphs",
          "year": 2018,
          "citations": 23,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/684e712f59f11d2bdc98be4c210824ab9e6f11f4",
          "authors": [
            "Zhilin Yang",
            "J. Zhao",
            "Bhuwan Dhingra",
            "Kaiming He",
            "William W. Cohen",
            "R. Salakhutdinov",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "What Do We Maximize in Self-Supervised Learning?",
          "year": 2022,
          "citations": 22,
          "abstract": "In this paper, we examine self-supervised learning methods, particularly VICReg, to provide an information-theoretical understanding of their construction. As a first step, we demonstrate how information-theoretic quantities can be obtained for a deterministic network, offering a possible alternative to prior work that relies on stochastic models. This enables us to demonstrate how VICReg can be (re)discovered from first principles and its assumptions about data distribution. Furthermore, we empirically demonstrate the validity of our assumptions, confirming our novel understanding of VICReg. Finally, we believe that the derivation and insights we obtain can be generalized to many other SSL methods, opening new avenues for theoretical and practical understanding of SSL and transfer learning.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2207.10081",
          "url": "https://www.semanticscholar.org/paper/b83fc5d7de2cc6a5ce6e44b8a1dd9169eec62720",
          "authors": [
            "Ravid Shwartz-Ziv",
            "Randall Balestriero",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "PROC OF THE IEEE NOVEMBER Gradient Based Learning Applied to Document Recognition",
          "year": 2006,
          "citations": 22,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/afc0a5d20dd7160f42fc5c27ef9746b14ebe53f4",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Yoshua Bengio"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Memoires associatives distribuees: Une comparaison (Distributed associative memories: A comparison)",
          "year": 1987,
          "citations": 22,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b667bca895b762611fe65929421e66181c7f23bc",
          "authors": [
            "P. Gallinari",
            "Yann LeCun",
            "S. Thiria",
            "F. F. Soulié"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Blockwise Self-Supervised Learning at Scale",
          "year": 2023,
          "citations": 21,
          "abstract": "Current state-of-the-art deep networks are all powered by backpropagation. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.",
          "venue": "Trans. Mach. Learn. Res.",
          "doi": "10.48550/arXiv.2302.01647",
          "url": "https://www.semanticscholar.org/paper/a09a197325be3fb2e865692b164e8827042201d1",
          "authors": [
            "Shoaib Ahmed Siddiqui",
            "David Krueger",
            "Yann LeCun",
            "Stéphane Deny"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A general segmentation scheme for DjVu document compression",
          "year": 2002,
          "citations": 21,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c4afb604c6e00c9fcc358daae260eeedb84be863",
          "authors": [
            "P. Haffner",
            "L. Bottou",
            "Yann LeCun",
            "L. Vincent"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "An efficient algorithm for learning invariance in adaptive classifiers",
          "year": 1992,
          "citations": 21,
          "abstract": null,
          "venue": "Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems",
          "doi": "10.1109/ICPR.1992.201861",
          "url": "https://www.semanticscholar.org/paper/99c6d1a3e73e454184f81e77563a4cb5810dc430",
          "authors": [
            "Patrice Y. Simard",
            "Yann LeCun",
            "J. Denker",
            "B. Victorri"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments",
          "year": 2022,
          "citations": 20,
          "abstract": "Data-Augmentation (DA) is known to improve performance across tasks and datasets. We propose a method to theoretically analyze the effect of DA and study questions such as: how many augmented samples are needed to correctly estimate the information encoded by that DA? How does the augmentation policy impact the final parameters of a model? We derive several quantities in close-form, such as the expectation and variance of an image, loss, and model's output under a given DA distribution. Those derivations open new avenues to quantify the benefits and limitations of DA. For example, we show that common DAs require tens of thousands of samples for the loss at hand to be correctly estimated and for the model training to converge. We show that for a training loss to be stable under DA sampling, the model's saliency map (gradient of the loss with respect to the model's input) must align with the smallest eigenvector of the sample variance under the considered DA augmentation, hinting at a possible explanation on why models tend to shift their focus from edges to textures.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3f07b6b6d1fc6020cef1f92c053810f46b46ac5e",
          "authors": [
            "Randall Balestriero",
            "Ishan Misra",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Light-weight probing of unsupervised representations for Reinforcement Learning",
          "year": 2022,
          "citations": 20,
          "abstract": "Unsupervised visual representation learning offers the opportunity to leverage large corpora of unlabeled trajectories to form useful visual representations, which can benefit the training of reinforcement learning (RL) algorithms. However, evaluating the fitness of such representations requires training RL algorithms which is computationally intensive and has high variance outcomes. Inspired by the vision community, we study whether linear probing can be a proxy evaluation task for the quality of unsupervised RL representation. Specifically, we probe for the observed reward in a given state and the action of an expert in a given state, both of which are generally applicable to many RL domains. Through rigorous experimentation, we show that the probing tasks are strongly rank correlated with the downstream RL performance on the Atari100k Benchmark, while having lower variance and up to 600x lower computational cost. This provides a more efficient method for exploring the space of pretraining algorithms and identifying promising pretraining recipes without the need to run RL evaluations for every setting. Leveraging this framework, we further improve existing self-supervised learning (SSL) recipes for RL, highlighting the importance of the forward model, the size of the visual backbone, and the precise formulation of the unsupervised objective.",
          "venue": "RLJ",
          "doi": "10.48550/arXiv.2208.12345",
          "url": "https://www.semanticscholar.org/paper/b62f6f765f033c1f023c4a424a20571564e61d97",
          "authors": [
            "Wancong Zhang",
            "Anthony GX-Chen",
            "Vlad Sobal",
            "Yann LeCun",
            "Nicolas Carion"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Unsupervised Learning of Feature Hierarchies",
          "year": 2009,
          "citations": 20,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3b3c153b09495e2f79dd973253f9d2ee763940a5",
          "authors": [
            "Yann LeCun",
            "Marc'Aurelio Ranzato"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Discriminative feature and model design for automatic speech recognition",
          "year": 1997,
          "citations": 19,
          "abstract": "AUTOMATIC SPEECH RECOGNITION Mazin Rahim, Yoshua Bengio and Yann LeCun AT&T Labs Research, 600 Mountain Avenue, Murray Hill, New Jersey 07974, USA ABSTRACT A system for discriminative feature and model design is presented for automatic speech recognition. Training based on minimum classi cation error with a single objective function is applied for designing a set of parallel networks performing feature transformation and a set of hidden Markov models performing speech recognition. This paper compares the use of linear and non-linear functional transformations when applied to conventional recognition features, such as spectrum or cepstrum. It also provides a framework for integrated feature and model training when using class-speci c transformations. Experimental results on telephone-based connected digit recognition are presented.",
          "venue": "EUROSPEECH",
          "doi": "10.21437/Eurospeech.1997-46",
          "url": "https://www.semanticscholar.org/paper/2a5c494c9ac68c8915d9df880a3bf6fe48a696a8",
          "authors": [
            "M. Rahim",
            "Yoshua Bengio",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Hierarchical loss for classification",
          "year": 2017,
          "citations": 18,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/15e2bbb28ef53f4b5da6a8c3934c54c1650fb947",
          "authors": [
            "C. Wu",
            "M. Tygert",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Adaptive long range vision in unstructured terrain",
          "year": 2007,
          "citations": 18,
          "abstract": null,
          "venue": "2007 IEEE/RSJ International Conference on Intelligent Robots and Systems",
          "doi": "10.1109/IROS.2007.4399622",
          "url": "https://www.semanticscholar.org/paper/18966721a3dc227d8632afbaa75d95cbc1708119",
          "authors": [
            "A. Erkan",
            "R. Hadsell",
            "P. Sermanet",
            "J. Ben",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Browsing through high quality document images with DjVu",
          "year": 1998,
          "citations": 18,
          "abstract": null,
          "venue": "Proceedings IEEE International Forum on Research and Technology Advances in Digital Libraries -ADL'98-",
          "doi": "10.1109/ADL.1998.670431",
          "url": "https://www.semanticscholar.org/paper/714c5e9f832b12e4029bc8516b3d9fe11ae0553e",
          "authors": [
            "P. Haffner",
            "L. Bottou",
            "P. Howard",
            "Patrice Y. Simard",
            "Yoshua Bengio",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Police: Provably Optimal Linear Constraint Enforcement For Deep Neural Networks",
          "year": 2022,
          "citations": 17,
          "abstract": "Deep Neural Networks (DNNs) outshine alternative function approximators in many settings thanks to their modularity in composing any desired differentiable operator. The formed parametrized functional is then tuned to solve a task at hand from simple gradient descent. This modularity comes at the cost of making strict enforcement of constraints on DNNs, e.g. from a priori knowledge of the task, or from desired physical properties, an open challenge. In this paper we propose the first provable affine constraint enforcement method for DNNs that only requires minimal changes into a given DNN’s forward-pass, that is computationally friendly, and that leaves the optimization of the DNN’s parameter to be unconstrained, i.e. standard gradient-based method can be employed. Our method does not require any sampling and provably ensures that the DNN fulfills the affine constraint on a given input space’s region at any point during training, and testing. We coin this method POLICE, standing for Provably Optimal LInear Constraint Enforcement. Github: https://github.com/RandallBalestriero/POLICE",
          "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
          "doi": "10.1109/ICASSP49357.2023.10096520",
          "url": "https://www.semanticscholar.org/paper/6f0aca58e13339fe78ee04b948253999a7cf85a3",
          "authors": [
            "Randall Balestriero",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A Data-Augmentation Is Worth A Thousand Samples: Analytical Moments And Sampling-Free Training",
          "year": 2022,
          "citations": 17,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/825257577638307bd13487888fb91cc1ae11501b",
          "authors": [
            "Randall Balestriero",
            "Ishan Misra",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding",
          "year": 2022,
          "citations": 17,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2206.08954",
          "url": "https://www.semanticscholar.org/paper/cf209f6cfa987e7b9320c24a8e9cb70bf39f31a7",
          "authors": [
            "Yubei Chen",
            "Adrien Bardes",
            "Zengyi Li",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Model-Based Planning in Discrete Action Spaces",
          "year": 2017,
          "citations": 17,
          "abstract": "Planning actions using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over model-free RL methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete, which may have limited its adoption. In this work, we introduce two discrete planning tasks inspired by existing question-answering datasets and show that it is in fact possible to effectively perform planning via backprop in discrete action spaces using two simple yet principled modifications. Our experiments show that this approach can significantly outperform model-free RL based methods and supervised imitation learners.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d86d17f6459978084320d7d313f38f234cf8b899",
          "authors": [
            "Mikael Henaff",
            "William F. Whitney",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Toward real-time indoor semantic segmentation using depth information",
          "year": 2014,
          "citations": 17,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/84e611d7b24323fda931a0e10aee59d8824e58bf",
          "authors": [
            "C. Couprie",
            "C. Farabet",
            "Laurent Najman",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning Representations by Maximizing Compression",
          "year": 2011,
          "citations": 17,
          "abstract": "We give an algorithm that learns a representation of data through compression. The algorithm 1) predicts bits sequentially from those previously seen and 2) has a structure and a number of computations similar to an autoencoder. The likelihood under the model can be calculated exactly, and arithmetic coding can be used directly for compression. When training on digits the algorithm learns filters similar to those of restricted boltzman machines and denoising autoencoders. Independent samples can be drawn from the model by a single sweep through the pixels. The algorithm has a good compression performance when compared to other methods that work under random ordering of pixels.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/bf326ddb9b9b15f5a285600af29e43c558ac890f",
          "authors": [
            "Karol Gregor",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Sn: A simulator for connectionist models",
          "year": 1988,
          "citations": 17,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5b5461d8ba70ecd7358b6edd8f39bda711f73a69",
          "authors": [
            "L. Bottou",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Joint Embedding Self-Supervised Learning in the Kernel Regime",
          "year": 2022,
          "citations": 15,
          "abstract": "The fundamental goal of self-supervised learning (SSL) is to produce useful representations of data without access to any labels for classifying the data. Modern methods in SSL, which form representations based on known or constructed relationships between samples, have been particularly effective at this task. Here, we aim to extend this framework to incorporate algorithms based on kernel methods where embeddings are constructed by linear maps acting on the feature space of a kernel. In this kernel regime, we derive methods to find the optimal form of the output representations for contrastive and non-contrastive loss functions. This procedure produces a new representation space with an inner product denoted as the induced kernel which generally correlates points which are related by an augmentation in kernel space and de-correlates points otherwise. We analyze our kernel model on small datasets to identify common features of self-supervised learning algorithms and gain theoretical insights into their performance on downstream tasks.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2209.14884",
          "url": "https://www.semanticscholar.org/paper/ebcccb72117b4641df9b88bac5b51dc5d2cead27",
          "authors": [
            "B. Kiani",
            "Randall Balestriero",
            "Yubei Chen",
            "S. Lloyd",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "High Performance Computer Acoustic Data Accelerator: A New System for Exploring Marine Mammal Acoustics for Big Data Applications",
          "year": 2015,
          "citations": 15,
          "abstract": "This paper presents a new software model designed for distributed sonic signal detection runtime using machine learning algorithms called DeLMA. A new algorithm--Acoustic Data-mining Accelerator (ADA)--is also presented. ADA is a robust yet scalable solution for efficiently processing big sound archives using distributing computing technologies. Together, DeLMA and the ADA algorithm provide a powerful tool currently being used by the Bioacoustics Research Program (BRP) at the Cornell Lab of Ornithology, Cornell University. This paper provides a high level technical overview of the system, and discusses various aspects of the design. Basic runtime performance and project summary are presented. The DeLMA-ADA baseline performance comparing desktop serial configuration to a 64 core distributed HPC system shows as much as a 44 times faster increase in runtime execution. Performance tests using 48 cores on the HPC shows a 9x to 12x efficiency over a 4 core desktop solution. Project summary results for 19 east coast deployments show that the DeLMA-ADA solution has processed over three million channel hours of sound to date.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6df3dc585e32f3b1cb49228d94a5469c30d79d2b",
          "authors": [
            "Peter J. Dugan",
            "J. Zollweg",
            "Marian Popescu",
            "D. Risch",
            "H. Glotin",
            "Yann LeCun",
            "C. Clark"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Hybrid hessians for flexible optimization of pose graphs",
          "year": 2010,
          "citations": 15,
          "abstract": null,
          "venue": "2010 IEEE/RSJ International Conference on Intelligent Robots and Systems",
          "doi": "10.1109/IROS.2010.5650091",
          "url": "https://www.semanticscholar.org/paper/499e299ff22c9ff7b53fcd704eb38f0ce1fce1fd",
          "authors": [
            "M. Grimes",
            "Dragomir Anguelov",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning maneuver dictionaries for ground robot planning",
          "year": 2008,
          "citations": 15,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/bb0eefb5b7d1a07c9abb2efa888060cde8a20bcc",
          "authors": [
            "P. Sermanet",
            "Marco Scoffier",
            "Chris Crudele",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A multi-range vision strategy for autonomous offroad navigation",
          "year": 2007,
          "citations": 15,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5fe3deeec85b63e9f10f643389a08e8a0aa8d2a6",
          "authors": [
            "R. Hadsell",
            "A. Erkan",
            "P. Sermanet",
            "J. Ben",
            "K. Kavukcuoglu",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Applications of Artificial Neural Networks to Image Processing",
          "year": 1998,
          "citations": 15,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/9c40fe0b583565d01f5d4032d574a020866bb817",
          "authors": [
            "R. Chellappa",
            "K. Fukushima",
            "A. Katsaggelos",
            "S. Kung",
            "Yann LeCun",
            "N. Nasrabadi",
            "T. Poggio"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Automatic Learning Rate Maximization in Large Adaptive Machines",
          "year": 1992,
          "citations": 15,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/f032d211e9db21d55b92c6349f0bb398142417a2",
          "authors": [
            "Yann LeCun",
            "Patrice Y. Simard",
            "Barak A. Pearlmutter"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need",
          "year": 2023,
          "citations": 14,
          "abstract": "Self-Supervised Learning (SSL) has emerged as the solution of choice to learn transferable representations from unlabeled data. However, SSL requires to build samples that are known to be semantically akin, i.e. positive views. Requiring such knowledge is the main limitation of SSL and is often tackled by ad-hoc strategies e.g. applying known data-augmentations to the same input. In this work, we formalize and generalize this principle through Positive Active Learning (PAL) where an oracle queries semantic relationships between samples. PAL achieves three main objectives. First, it unveils a theoretically grounded learning framework beyond SSL, based on similarity graphs, that can be extended to tackle supervised and semi-supervised learning depending on the employed oracle. Second, it provides a consistent algorithm to embed a priori knowledge, e.g. some observed labels, into any SSL losses without any change in the training pipeline. Third, it provides a proper active learning framework yielding low-cost solutions to annotate datasets, arguably bringing the gap between theory and practice of active learning that is based on simple-to-answer-by-non-experts queries of semantic relationships between inputs.",
          "venue": "IEEE International Conference on Computer Vision",
          "doi": "10.1109/ICCV51070.2023.01491",
          "url": "https://www.semanticscholar.org/paper/138fa5f64fe54376022998fe553b6156a93ff19e",
          "authors": [
            "Vivien A. Cabannes",
            "L. Bottou",
            "Yann LeCun",
            "Randall Balestriero"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Source separation with scattering Non-Negative Matrix Factorization",
          "year": 2015,
          "citations": 14,
          "abstract": null,
          "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
          "doi": "10.1109/ICASSP.2015.7178296",
          "url": "https://www.semanticscholar.org/paper/810c3027863e021041074a23af5deb6307a0f1ca",
          "authors": [
            "Joan Bruna",
            "P. Sprechmann",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Graph transformer networks for image recognition",
          "year": 2005,
          "citations": 14,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/f78017ab52a9e10d206da41363ec0c11a10e4757",
          "authors": [
            "L. Bottou",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Sparse Coding with Multi-Layer Decoders using Variance Regularization",
          "year": 2021,
          "citations": 13,
          "abstract": "Sparse representations of images are useful in many computer vision applications. Sparse coding with an $l_1$ penalty and a learned linear dictionary requires regularization of the dictionary to prevent a collapse in the $l_1$ norms of the codes. Typically, this regularization entails bounding the Euclidean norms of the dictionary's elements. In this work, we propose a novel sparse coding protocol which prevents a collapse in the codes without the need to regularize the decoder. Our method regularizes the codes directly so that each latent code component has variance greater than a fixed threshold over a set of sparse representations for a given set of inputs. Furthermore, we explore ways to effectively train sparse coding systems with multi-layer decoders since they can model more complex relationships than linear dictionaries. In our experiments with MNIST and natural image patches, we show that decoders learned with our approach have interpretable features both in the linear and multi-layer case. Moreover, we show that sparse autoencoders with multi-layer decoders trained using our variance regularization method produce higher quality reconstructions with sparser representations when compared to autoencoders with linear dictionaries. Additionally, sparse representations obtained with our variance regularization approach are useful in the downstream tasks of denoising and classification in the low-data regime.",
          "venue": "Trans. Mach. Learn. Res.",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6e08acdb27e83a26ef7e67539a11e9cb3588e822",
          "authors": [
            "Katrina Evtimova",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "E cient Learning of Sparse Overcomplete Representations with an Energy-Based Model",
          "year": 2006,
          "citations": 13,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8680319b8619e71bb5bd75103d2b7f1d08f48491",
          "authors": [
            "Marc'Aurelio Ranzato",
            "Christopher S. Poultney",
            "S. Chopra",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Color documents on the Web with DjVu",
          "year": 1999,
          "citations": 13,
          "abstract": null,
          "venue": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)",
          "doi": "10.1109/ICIP.1999.821605",
          "url": "https://www.semanticscholar.org/paper/dec714928a06610355e4d44c9468d2be0af2f47c",
          "authors": [
            "P. Haffner",
            "Yann LeCun",
            "L. Bottou",
            "P. Howard",
            "Pascal Vincent",
            "B. Riemers"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "On-line recognition of limited-vocabulary Chinese character using multiple convolutional neural networks",
          "year": 1993,
          "citations": 13,
          "abstract": null,
          "venue": "1993 IEEE International Symposium on Circuits and Systems",
          "doi": "10.1109/ISCAS.1993.394256",
          "url": "https://www.semanticscholar.org/paper/cc74bcd1c131c3fd3edae602bc80b47cfebcd0fa",
          "authors": [
            "Quen-Zong Wu",
            "Yann LeCun",
            "L. Jackel",
            "Bor-Shenn Jeng"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Second Order Properties of Error Surfaces",
          "year": 1990,
          "citations": 13,
          "abstract": null,
          "venue": "Neural Information Processing Systems",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d39adfe1d561f676b2075ac36c59405146533b0b",
          "authors": [
            "Yann LeCun",
            "I. Kanter",
            "S. Solla"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Joint Embedding Predictive Architectures Focus on Slow Features",
          "year": 2022,
          "citations": 12,
          "abstract": "Many common methods for learning a world model for pixel-based environments use generative architectures trained with pixel-level reconstruction objectives. Recently proposed Joint Embedding Predictive Architectures (JEPA) offer a reconstruction-free alternative. In this work, we analyze performance of JEPA trained with VICReg and SimCLR objectives in the fully offline setting without access to rewards, and compare the results to the performance of the generative architecture. We test the methods in a simple environment with a moving dot with various background distractors, and probe learned representations for the dot's location. We find that JEPA methods perform on par or better than reconstruction when distractor noise changes every time step, but fail when the noise is fixed. Furthermore, we provide a theoretical explanation for the poor performance of JEPA-based methods with fixed noise, highlighting an important limitation.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2211.10831",
          "url": "https://www.semanticscholar.org/paper/ed009b7423dcfec47708fb5817ec4955e4265757",
          "authors": [
            "Vlad Sobal",
            "V JyothirS",
            "Siddhartha Jalagam",
            "Nicolas Carion",
            "Kyunghyun Cho",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Real-time adaptive off-road vehicle navigation and terrain classification",
          "year": 2013,
          "citations": 12,
          "abstract": null,
          "venue": "Defense, Security, and Sensing",
          "doi": "10.1117/12.2015533",
          "url": "https://www.semanticscholar.org/paper/5fd1714d7fc5b3edfcfb755e11f914c46d377cc5",
          "authors": [
            "Urs Muller",
            "L. Jackel",
            "Yann LeCun",
            "B. Flepp"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning in Informal Settings",
          "year": 2012,
          "citations": 12,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/springerreference_301826",
          "url": "https://www.semanticscholar.org/paper/27a1c8c5e228cf4d5fd020e347a58bfb00c95d46",
          "authors": [
            "C. Chan",
            "Z. Zorina",
            "Jesse R. Sparks",
            "S. L. Ornat",
            "C. Tsang",
            "E. Grigorenko",
            "R. Lubow",
            "J. Malone",
            "M. Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "T. Minor",
            "Traci N. Plumb",
            "H. Drachsler",
            "P. Kirschner",
            "M. Nakayama",
            "Rowena Santiago",
            "Ali Şimşek",
            "Fang‐Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "R. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "L. Lamb",
            "A. Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "Lynda Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "T. Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "G. Kern-Isberner",
            "J. Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C. Olsson",
            "L. Nyberg",
            "R. Freivalds",
            "L. Hall",
            "M. Hall",
            "U. Hanke",
            "L. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "P. Burnett",
            "M. Cannon",
            "A. Edmondson",
            "P. Hartono",
            "A. Callender",
            "R. Barr",
            "N. Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohamad Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "D. Kolb",
            "R. Reilly",
            "K. Nielsen",
            "P. Couvillon",
            "H. King",
            "J. Dillon",
            "D. Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "P. C. Lane",
            "S. Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "D. Tucker",
            "P. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "D. Ansari",
            "K. Lionello-denolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "Eliza Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo P. Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. Hoppe",
            "Krisztina Molnár",
            "E. Vries",
            "E. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "T. Cazenave",
            "T. Jong",
            "J. Meij",
            "J. Gavelek",
            "A. Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "M. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "D. Seo",
            "M. Monfils",
            "A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "M. Kennedy",
            "M. Jackson",
            "T. Nunes",
            "B. Jackling",
            "R. Howard",
            "W. Kennedy",
            "L. C. Drickamer"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Natural Versus \"universal\" Probability, Complexity, And Entropy",
          "year": 1992,
          "citations": 12,
          "abstract": null,
          "venue": "Workshop on Physics and Computation",
          "doi": "10.1109/PHYCMP.1992.615508",
          "url": "https://www.semanticscholar.org/paper/4096248dca0986be1a6978945a58028e549403d8",
          "authors": [
            "J. Denker",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Hardware requirements for neural-net optical character recognition",
          "year": 1990,
          "citations": 12,
          "abstract": null,
          "venue": "1990 IJCNN International Joint Conference on Neural Networks",
          "doi": "10.1109/IJCNN.1990.137801",
          "url": "https://www.semanticscholar.org/paper/dab9de4747d1286c456f36ec69415c5d668889aa",
          "authors": [
            "L. Jackel",
            "B. Boser",
            "J. Denker",
            "H. Graf",
            "Yann LeCun",
            "Isabelle M Guyon",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "S. Solla"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The NIPS workshop on Deep Learning and Unsupervised Feature Learning",
          "year": 2011,
          "citations": 11,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ae47286f40db22e64aaeec97497b9f522bd00943",
          "authors": [
            "T. Raiko",
            "Harri Valpola",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Efficient off-road localization using visually corrected odometry",
          "year": 2009,
          "citations": 11,
          "abstract": null,
          "venue": "IEEE International Conference on Robotics and Automation",
          "doi": "10.1109/ROBOT.2009.5152880",
          "url": "https://www.semanticscholar.org/paper/083d38843c14e02970cf86d9161a5a2bf40d2bab",
          "authors": [
            "M. Grimes",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Neural Network Applications in Character Recognition and Document Analysis",
          "year": 1994,
          "citations": 11,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/978-1-4615-2734-3_14",
          "url": "https://www.semanticscholar.org/paper/f633dc06f89ab060607202ad96daa85d268055c7",
          "authors": [
            "L. Jackel",
            "M. Y. Battista",
            "J. Ben",
            "J. Bromley",
            "C. Burges",
            "H. Baird",
            "E. Cosatto",
            "J. Denker",
            "H. Graf",
            "H. Katseff",
            "Yann LeCun",
            "C. Nohl",
            "E. Sackinger",
            "J. H. Shamilian",
            "T. Shoemaker",
            "C. E. Stenard",
            "B. I. Strom",
            "R. Ting",
            "T. Wood",
            "C. R. Zuraw"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A neural network approach to handprint character recognition",
          "year": 1991,
          "citations": 11,
          "abstract": null,
          "venue": "COMPCON Spring '91 Digest of Papers",
          "doi": "10.1109/CMPCON.1991.128851",
          "url": "https://www.semanticscholar.org/paper/2fdbed2b8fffa62c86de3e5500590a197ac3327a",
          "authors": [
            "L. Jackel",
            "C. E. Stenard",
            "H. Baird",
            "B. Boser",
            "J. Bromley",
            "C. Burges",
            "J. Denker",
            "H. Graf",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard",
            "Yann LeCun",
            "O. Matan",
            "E. Pednault",
            "William Satterfield",
            "Eduard Säckinger",
            "Timothy J. Thompson"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A time delay neural network character recognizer for a touch terminal",
          "year": 1990,
          "citations": 11,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2c25cb7cb7c0be41510efeaef5cb96dea339823f",
          "authors": [
            "Isabelle M Guyon",
            "P. Albrecht",
            "Yann LeCun",
            "J. Denker",
            "W. Hubbard"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations",
          "year": 2022,
          "citations": 10,
          "abstract": "Self-Supervised Learning (SSL) methods such as VICReg, Barlow Twins or W-MSE avoid collapse of their joint embedding architectures by constraining or regularizing the covariance matrix of their projector's output. This study highlights important properties of such strategy, which we coin Variance-Covariance regularization (VCReg). More precisely, we show that {\\em VCReg combined to a MLP projector enforces pairwise independence between the features of the learned representation}. This result emerges by bridging VCReg applied on the projector's output to kernel independence criteria applied on the projector's input. We empirically validate our findings where (i) we put in evidence which projector's characteristics favor pairwise independence, (ii) we demonstrate pairwise independence to be beneficial for out-of-domain generalization, (iii) we demonstrate that the scope of VCReg goes beyond SSL by using it to solve Independent Component Analysis. This provides the first theoretical motivation and explanation of MLP projectors in SSL.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2209.14905",
          "url": "https://www.semanticscholar.org/paper/95c729ce4469ba0513380759b82d3a50d648bd9b",
          "authors": [
            "G. Mialon",
            "Randall Balestriero",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Backpropagation for Implicit Spectral Densities",
          "year": 2018,
          "citations": 10,
          "abstract": "Most successful machine intelligence systems rely on gradient-based learning, which is made possible by backpropagation. Some systems are designed to aid us in interpreting data when explicit goals cannot be provided. These unsupervised systems are commonly trained by backpropagating through a likelihood function. We introduce a tool that allows us to do this even when the likelihood is not explicitly set, by instead using the implicit likelihood of the model. Explicitly defining the likelihood often entails making heavy-handed assumptions that impede our ability to solve challenging tasks. On the other hand, the implicit likelihood of the model is accessible without the need for such assumptions. Our tool, which we call spectral backpropagation, allows us to optimize it in much greater generality than what has been attempted before. GANs can also be viewed as a technique for optimizing implicit likelihoods. We study them using spectral backpropagation in order to demonstrate robustness for high-dimensional problems, and identify two novel properties of the generator G: (1) there exist aberrant, nonsensical outputs to which G assigns very high likelihood, and (2) the eigenvectors of the metric induced by G over latent space correspond to quasi-disentangled explanatory factors.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c52fd8ed00b4d3275889532f2f345130af462848",
          "authors": [
            "A. Ramesh",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "A theoretical argument for complex-valued convolutional networks",
          "year": 2015,
          "citations": 10,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2eff9844bddfcabe7b3f16c07fe5dad20ccedd53",
          "authors": [
            "Joan Bruna",
            "Soumith Chintala",
            "Yann LeCun",
            "Serkan Piantino",
            "Arthur Szlam",
            "M. Tygert"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Saturating Auto-Encoder",
          "year": 2013,
          "citations": 10,
          "abstract": null,
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/dab50f7682bf410743e2d2447eb5d2bc652f1463",
          "authors": [
            "Ross Goroshin",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "SPEED-RANGE DILEMMAS FOR VISION-BASED NAVIGATION IN UNSTRUCTURED TERRAIN",
          "year": 2007,
          "citations": 10,
          "abstract": null,
          "venue": "",
          "doi": "10.3182/20070903-3-FR-2921.00052",
          "url": "https://www.semanticscholar.org/paper/181c805eb0b2524efb16c6b14dcf2e435140e634",
          "authors": [
            "P. Sermanet",
            "R. Hadsell",
            "J. Ben",
            "A. Erkan",
            "B. Flepp",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Efficient conversion of digital documents to multilayer raster formats",
          "year": 2001,
          "citations": 10,
          "abstract": null,
          "venue": "Proceedings of Sixth International Conference on Document Analysis and Recognition",
          "doi": "10.1109/ICDAR.2001.953829",
          "url": "https://www.semanticscholar.org/paper/bc95fe16a7a9f29b13ea242cdc69ec3b977f245d",
          "authors": [
            "L. Bottou",
            "P. Haffner",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Unsupervised Learning of Structured Representations via Closed-Loop Transcription",
          "year": 2022,
          "citations": 9,
          "abstract": "This paper proposes an unsupervised method for learning a unified representation that serves both discriminative and generative purposes. While most existing unsupervised learning approaches focus on a representation for only one of these two goals, we show that a unified representation can enjoy the mutual benefits of having both. Such a representation is attainable by generalizing the recently proposed \\textit{closed-loop transcription} framework, known as CTRL, to the unsupervised setting. This entails solving a constrained maximin game over a rate reduction objective that expands features of all samples while compressing features of augmentations of each sample. Through this process, we see discriminative low-dimensional structures emerge in the resulting representations. Under comparable experimental conditions and network complexities, we demonstrate that these structured representations enable classification performance close to state-of-the-art unsupervised discriminative representations, and conditionally generated image quality significantly higher than that of state-of-the-art unsupervised generative models. Source code can be found at https://github.com/Delay-Xili/uCTRL.",
          "venue": "CPAL",
          "doi": "10.48550/arXiv.2210.16782",
          "url": "https://www.semanticscholar.org/paper/5e071a75a6906434daffe24f612fd291db4e1496",
          "authors": [
            "Shengbang Tong",
            "Xili Dai",
            "Yubei Chen",
            "Mingyang Li",
            "Zengyi Li",
            "Brent Yi",
            "Yann LeCun",
            "Y. Ma"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Simultaneous Learning of Trees and Representations for Extreme Classification, with Application to Language Modeling",
          "year": 2016,
          "citations": 9,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/a0049f4e6ae42da87a6edd276618abec5a2e44b0",
          "authors": [
            "Yacine Jernite",
            "A. Choromańska",
            "D. Sontag",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Universal halting times in optimization and machine learning",
          "year": 2015,
          "citations": 9,
          "abstract": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that, after centering and scaling, remains unchanged even when the distribution on the landscape is changed. We observe two qualitative classes: A Gumbel-like distribution that appears in Google searches, human decision times, the QR eigenvalue algorithm and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.",
          "venue": "",
          "doi": "10.1090/qam/1483",
          "url": "https://www.semanticscholar.org/paper/30dd56bcb013893330d48829f5c0d7c02ee2da64",
          "authors": [
            "Levent Sagun",
            "T. Trogdon",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "DjVu: a Compression Method for Distributing Scanned Documents in Color over the Internet",
          "year": 1998,
          "citations": 9,
          "abstract": "We present a new image compression technique called “DjVu” that is specifically geared towards the compression of scanned documents in color at high revolution. DjVu enable any screen connected to the Internet to access and display images of scanned pages while faithfully reproducing the font, color, drawings, pictures, and paper texture. With DjVu, a typical magazine page in color at 300dpi can be compressed down to between 40 to 60 KB, approximately 5 to 10 times better than JPEG for a similar level of subjective quality. A real-time, memory efficient version of the decoder is available as a plug-in for popular web browsers.",
          "venue": "International Conference on Communications in Computing",
          "doi": "10.2352/CIC.1998.6.1.art00047",
          "url": "https://www.semanticscholar.org/paper/4737e136dbf34e37b8c2ee3c615e3e808d5d63fc",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "P. Haffner",
            "P. Howard"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "On the application of multimedia processing to telecommunications",
          "year": 1997,
          "citations": 9,
          "abstract": null,
          "venue": "Proceedings of International Conference on Image Processing",
          "doi": "10.1109/ICIP.1997.647370",
          "url": "https://www.semanticscholar.org/paper/04b5268727c322dcd9d10a14decb5d8c03a4f378",
          "authors": [
            "R. Cox",
            "B. Haskell",
            "Yann LeCun",
            "B. Shahraray",
            "L. Rabiner"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Variance-Covariance Regularization Improves Representation Learning",
          "year": 2023,
          "citations": 8,
          "abstract": "Transfer learning plays a key role in advancing machine learning models, yet conventional supervised pretraining often undermines feature transferability by prioritizing features that minimize the pretraining loss. In this work, we adapt a self-supervised learning regularization technique from the VICReg method to supervised learning contexts, introducing Variance-Covariance Regularization (VCReg). This adaptation encourages the network to learn high-variance, low-covariance representations, promoting learning more diverse features. We outline best practices for an efficient implementation of our framework, including applying it to the intermediate representations. Through extensive empirical evaluation, we demonstrate that our method significantly enhances transfer learning for images and videos, achieving state-of-the-art performance across numerous tasks and datasets. VCReg also improves performance in scenarios like long-tail learning and hierarchical classification. Additionally, we show its effectiveness may stem from its success in addressing challenges like gradient starvation and neural collapse. In summary, VCReg offers a universally applicable regularization framework that significantly advances transfer learning and highlights the connection between gradient starvation, neural collapse, and feature transferability.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2306.13292",
          "url": "https://www.semanticscholar.org/paper/c6990a1e568f5458240643688ee797b6450c9f1f",
          "authors": [
            "Jiachen Zhu",
            "Ravid Shwartz-Ziv",
            "Yubei Chen",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning",
          "year": 2022,
          "citations": 8,
          "abstract": "Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches primarily learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as BagSSL. Even with 32x32 patch representation, BagSSL achieves 62% top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the SSL representation is relatively invariant at the global scale, we show that locality is preserved when we zoom into local patch-level representation. Further, we show that patch representation aggregation can improve various SOTA baseline methods by a large margin. The patch representation is considerably easier to understand, and this work makes a step to demystify self-supervised representation learning.",
          "venue": "Trans. Mach. Learn. Res.",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0bbb60fd0fe2e0ffefacb16fc2c527cb2f01a71e",
          "authors": [
            "Yubei Chen",
            "Adrien Bardes",
            "Zengyi Li",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Minimalistic Unsupervised Learning with the Sparse Manifold Transform",
          "year": 2022,
          "citations": 8,
          "abstract": "We describe a minimalistic and interpretable method for unsupervised learning, without resorting to data augmentation, hyperparameter tuning, or other engineering designs, that achieves performance close to the SOTA SSL methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic sparse manifold transform, one can achieve 99.3% KNN top-1 accuracy on MNIST, 81.1% KNN top-1 accuracy on CIFAR-10 and 53.2% on CIFAR-100. With a simple gray-scale augmentation, the model gets 83.2% KNN top-1 accuracy on CIFAR-10 and 57% on CIFAR-100. These results significantly close the gap between simplistic\"white-box\"methods and the SOTA methods. Additionally, we provide visualization to explain how an unsupervised representation transform is formed. The proposed method is closely connected to latent-embedding self-supervised methods and can be treated as the simplest form of VICReg. Though there remains a small performance gap between our simple constructive model and SOTA methods, the evidence points to this as a promising direction for achieving a principled and white-box approach to unsupervised learning.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2209.15261",
          "url": "https://www.semanticscholar.org/paper/537166437212aac4b4297121e0ba4b7e545d4e54",
          "authors": [
            "Yubei Chen",
            "Zeyu Yun",
            "Y. Ma",
            "B. Olshausen",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Neural Potts Model",
          "year": 2021,
          "citations": 8,
          "abstract": "We propose the Neural Potts Model objective as an amortized optimization problem. The objective enables training a single model with shared parameters to explicitly model energy landscapes across multiple protein families. Given a protein sequence as input, the model is trained to predict a pairwise coupling matrix for a Potts model energy function describing the local evolutionary landscape of the sequence. Couplings can be predicted for novel sequences. A controlled ablation experiment assessing unsupervised contact prediction on sets of related protein families finds a gain from amortization for low-depth multiple sequence alignments; the result is then confirmed on a database with broad coverage of protein sequences.",
          "venue": "bioRxiv",
          "doi": "10.1101/2021.04.08.439084",
          "url": "https://www.semanticscholar.org/paper/94526a3b070927dc7a0f066cf0a17b8abaf5ce1f",
          "authors": [
            "Tom Sercu",
            "Robert Verkuil",
            "Joshua Meier",
            "Brandon Amos",
            "Zeming Lin",
            "Caroline Chen",
            "Jason Liu",
            "Yann LeCun",
            "Alexander Rives"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Fast Incremental Learning for Off-Road Robot Navigation",
          "year": 2016,
          "citations": 7,
          "abstract": "A promising approach to autonomous driving is machine learning. In such systems, training datasets are created that capture the sensory input to a vehicle as well as the desired response. A disadvantage of using a learned navigation system is that the learning process itself may require a huge number of training examples and a large amount of computing. To avoid the need to collect a large training set of driving examples, we describe a system that takes advantage of the huge number of training examples provided by ImageNet, but is able to adapt quickly using a small training set for the specific driving environment.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3f1540459ef264051f15ac15cadcc4d924dc79a8",
          "authors": [
            "Artem Provodin",
            "L. Torabi",
            "B. Flepp",
            "Yann LeCun",
            "Michael Sergio",
            "L. Jackel",
            "Urs Muller",
            "Jure Zbontar"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Hardware accelerated visual attention algorithm",
          "year": 2011,
          "citations": 7,
          "abstract": null,
          "venue": "Annual Conference on Information Sciences and Systems",
          "doi": "10.1109/CISS.2011.5766191",
          "url": "https://www.semanticscholar.org/paper/4c87d431433a5e16598a110691ff77160f012da0",
          "authors": [
            "Polina Akselrod",
            "Faye Zhao",
            "Ifigeneia Derekli",
            "C. Farabet",
            "B. Martini",
            "Yann LeCun",
            "E. Culurciello"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Constrained neural networks for pattern recognition",
          "year": 1991,
          "citations": 7,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/68e9cab665e8555a5b55fb11f8bd930db32c1283",
          "authors": [
            "S. Solla",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Minimalistic Unsupervised Representation Learning with the Sparse Manifold Transform",
          "year": 2023,
          "citations": 5,
          "abstract": null,
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0380271e23c5a3348f9f8ad1906b692b22f8b75e",
          "authors": [
            "Yubei Chen",
            "Zeyu Yun",
            "Yi Ma",
            "B. Olshausen",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Fast and Exact Enumeration of Deep Networks Partitions Regions",
          "year": 2023,
          "citations": 5,
          "abstract": "One fruitful formulation of Deep Networks (DNs) enabling their theoretical study and providing practical guidelines to practitioners relies on Piecewise Affine Splines. In that realm, a DN’s input-mapping is expressed as per-region affine map-ping where those regions are implicitly determined by the model’s architecture and form a partition of their input space. That partition –which is involved in all the results spanned from this line of research– has so far only been computed on 2/3-dimensional slices of the DN’s input space or estimated by random sampling. In this paper, we provide the first parallel algorithm that does exact enumeration of the DN’s partition regions. The proposed algorithm enables one to finally assess the closeness of the commonly employed approximations methods, e.g. based on random sampling of the DN input space. One of our key finding is that if one is only interested in regions with \"large\" volume, then uniform sampling of the space is highly efficient, but that if one is also interested in discovering the \"small\" regions of the partition, then uniform sampling is exponentially costly with the DN’s input space dimension. On the other hand, our proposed method has complexity scaling linearly with input dimension and the number of regions.",
          "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
          "doi": "10.1109/icassp49357.2023.10095698",
          "url": "https://www.semanticscholar.org/paper/132606b0fc37bd2a9f50d3199f29b8b2defbcb93",
          "authors": [
            "Randall Balestriero",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Stochastic positional embeddings improve masked image modeling",
          "year": 2023,
          "citations": 5,
          "abstract": "Masked Image Modeling (MIM) is a promising self-supervised learning approach that enables learning from unlabeled images. Despite its recent success, learning good representations through MIM remains challenging because it requires predicting the right semantic content in accurate locations. For example, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose to incorporate location uncertainty into MIM by using stochastic positional embeddings (StoP). Specifically, we condition the model on stochastic masked token positions drawn from a Gaussian distribution. StoP reduces overfitting to location features and guides the model toward learning features that are more robust to location uncertainties. Quantitatively, StoP improves downstream MIM performance on a variety of downstream tasks, including $+1.7\\%$ on ImageNet linear probing using ViT-B, and $+2.5\\%$ for ViT-H using $1\\%$ of the data.",
          "venue": "International Conference on Machine Learning",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/da136b4651e035c2bafd0bfd34433faf7af2619e",
          "authors": [
            "Amir Bar",
            "Florian Bordes",
            "Assaf Shocher",
            "Mahmoud Assran",
            "Pascal Vincent",
            "Nicolas Ballas",
            "Trevor Darrell",
            "A. Globerson",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Separating the World and Ego Models for Self-Driving",
          "year": 2022,
          "citations": 5,
          "abstract": "Training self-driving systems to be robust to the long-tail of driving scenarios is a critical problem. Model-based approaches leverage simulation to emulate a wide range of scenarios without putting users at risk in the real world. One promising path to faithful simulation is to train a forward model of the world to predict the future states of both the environment and the ego-vehicle given past states and a sequence of actions. In this paper, we argue that it is beneficial to model the state of the ego-vehicle, which often has simple, predictable and deterministic behavior, separately from the rest of the environment, which is much more complex and highly multimodal. We propose to model the ego-vehicle using a simple and differentiable kinematic model, while training a stochastic convolutional forward model on raster representations of the state to predict the behavior of the rest of the environment. We explore several configurations of such decoupled models, and evaluate their performance both with Model Predictive Control (MPC) and direct policy learning. We test our methods on the task of highway driving and demonstrate lower crash rates and better stability. The code is available at https://github.com/vladisai/pytorch-PPUU/tree/ICLR2022.",
          "venue": "arXiv.org",
          "doi": "10.48550/arXiv.2204.07184",
          "url": "https://www.semanticscholar.org/paper/06103ec8b82b705d674df3432bbaa5dcfffcceb0",
          "authors": [
            "Vlad Sobal",
            "A. Canziani",
            "Nicolas Carion",
            "Kyunghyun Cho",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Compact and Optimal Deep Learning with Recurrent Parameter Generators",
          "year": 2021,
          "citations": 5,
          "abstract": "Deep learning has achieved tremendous success by training increasingly large models, which are then compressed for practical deployment. We propose a drastically different approach to compact and optimal deep learning: We decouple the Degrees of freedom (DoF) and the actual number of parameters of a model, optimize a small DoF with predefined random linear constraints for a large model of an arbitrary architecture, in one-stage end-to-end learning.Specifically, we create a recurrent parameter generator (RPG), which repeatedly fetches parameters from a ring and unpacks them onto a large model with random permutation and sign flipping to promote parameter decorrelation. We show that gradient descent can automatically find the best model under constraints with in fact faster convergence.Our extensive experimentation reveals a log-linear relationship between model DoF and accuracy. Our RPG demonstrates remarkable DoF reduction, and can be further pruned and quantized for additional run-time performance gain. For example, in terms of top-1 accuracy on ImageNet, RPG achieves 96% of ResNet18’s performance with only 18% DoF (the equivalent of one convolutional layer) and 52% of ResNet34’s performance with only 0.25% DoF! Our work shows significant potential of constrained neural opti-mization in compact and optimal deep learning.",
          "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
          "doi": "10.1109/WACV56688.2023.00389",
          "url": "https://www.semanticscholar.org/paper/15dc19495e95703f96989bd66135eb3bc4057976",
          "authors": [
            "Jiayun Wang",
            "Yubei Chen",
            "Stella X. Yu",
            "Brian Cheung",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning with Reflective Likelihoods",
          "year": 2018,
          "citations": 5,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ad81013e8afc5e2fd5d279468dd6c9bb504179e7",
          "authors": [
            "A. B. Dieng",
            "Kyunghyun Cho",
            "D. Blei",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Universality in halting time and its applications in optimization",
          "year": 2015,
          "citations": 5,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0bc66655d3c9ad7ab4f44e19b6777add3d83643c",
          "authors": [
            "Levent Sagun",
            "T. Trogdon",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "The bottlenecks in human letter recognition: a computational model",
          "year": 2014,
          "citations": 5,
          "abstract": null,
          "venue": "",
          "doi": "10.7490/F1000RESEARCH.1095738.1",
          "url": "https://www.semanticscholar.org/paper/553ad7d282e2c2bbad301c119e9aaaa9e8fa369a",
          "authors": [
            "Avi Ziskind",
            "Olivier J. Hénaff",
            "Yann LeCun",
            "D. Pelli"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Efficient Learning of Sparse Invariant Representations",
          "year": 2011,
          "citations": 5,
          "abstract": "We propose a simple and efficient algorithm for learning sparse invariant representations from unlabeled data with fast inference. When trained on short movies sequences, the learned features are selective to a range of orientations and spatial frequencies, but robust to a wide range of positions, similar to complex cells in the primary visual cortex. We give a hierarchical version of the algorithm, and give guarantees of fast convergence under certain conditions.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d427d3327cbab52468d310af6331e8cc84dea8ff",
          "authors": [
            "Karol Gregor",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Factor Graphs for Relational Regression",
          "year": 2008,
          "citations": 5,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d3c3ad20aa5580a6784e445d0296c4ef09a59684",
          "authors": [
            "Yann LeCun",
            "S. Chopra"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Adapting Grounded Visual Question Answering Models to Low Resource Languages",
          "year": 2023,
          "citations": 4,
          "abstract": "While huge progress has been made on a variety of vision and language tasks in recent years, most major advances have been restricted to the English language due to the scarcity of relevant training and evaluation datasets in other languages. A popular approach to address this gap, has been to utilize machine-translated multi-modal datasets or multi-lingual text-only datasets for pre-training. This approach not only fails to exploit existing pre-trained state-of-the-art English multi-modal models, but also is not a viable solution for low-resource languages where translation quality is not as reliable. Therefore, we propose xMDETR, a multi-lingual grounded vision-language model based on the state-of-the-art model MDETR, by adapting it to new languages without machine-translated data, while also keeping most of the pre-trained weights frozen. xMDETR leverages mono-lingual pre-trained MDETR to achieve results competitive to state of the art on xGQA, a standard multilingual VQA benchmark. It is also interpretable, providing bounding boxes for key phrases in the multi-lingual questions. Our method utilizes several architectural as well as data-driven techniques such as training a new embedding space with a Masked Language Modeling (MLM) objective, code-switching, and adapters for efficient and modular training. We also explore contrastive losses to enforce the bridging of multi-modal and multi-lingual representations on multi-lingual multi-modal data, when available. We evaluate xMDETR on xGQA in both zero-shot and few-shot settings, improving results on Portuguese, Indonesian and Bengali, while remaining competitive on other languages.",
          "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
          "doi": "10.1109/CVPRW59228.2023.00258",
          "url": "https://www.semanticscholar.org/paper/6be0cf37d8b05c1b8fb4235217895b3bf1b6c368",
          "authors": [
            "Y. Wang",
            "Jonas Pfeiffer",
            "Nicolas Carion",
            "Yann LeCun",
            "Aishwarya Kamath"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Audio Source Separation with Discriminative Scattering Networks",
          "year": 2014,
          "citations": 4,
          "abstract": "Many monaural signal decomposition techniques proposed in the literature operate on a feature space consisting of a time-frequency representation of the input data. A challenge faced by these approaches is to effectively exploit the temporal dependencies of the signals at scales larger than the duration of a time-frame. In this work we propose to tackle this problem by modeling the signals using a time-frequency representation with multiple temporal resolutions. For this reason we use a signal representation that consists of a pyramid of wavelet scattering operators, which generalizes Constant Q Transforms CQT with extra layers of convolution and complex modulus. We first show that learning standard models with this multi-resolution setting improves source separation results over fixed-resolution methods. As study case, we use Non-Negative Matrix Factorizations NMF that has been widely considered in many audio application. Then, we investigate the inclusion of the proposed multi-resolution setting into a discriminative training regime. We discuss several alternatives using different deep neural network architectures, and our preliminary experiments suggest that in this task, finite impulse, multi-resolution Convolutional Networks are a competitive baseline compared to recurrent alternatives.",
          "venue": "Latent Variable Analysis and Signal Separation",
          "doi": "10.1007/978-3-319-22482-4_30",
          "url": "https://www.semanticscholar.org/paper/a7cc2b78afcdbbfa2f8ba41addec2827c33f89c7",
          "authors": [
            "P. Sprechmann",
            "Joan Bruna",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "DCL System Using Deep Learning Approaches for Land-Based or Ship-Based Real-Time Recognition and Localization of Marine Mammals",
          "year": 2013,
          "citations": 4,
          "abstract": "Abstract : The overarching goals of this work are to advance the state of the art for detection, classification, and localization (DCL) in the field of bioacoustics. This goal is primarily achieved by building a generic framework for detection-classification (DC) using a fast, efficient, and scalable architecture, demonstrating the capabilities of the system using a variety of low-frequency and mid-frequency cetacean sounds. Two primary goals are to develop transferable technologies for detection and classification in the area of advanced algorithms, such as deep learning and other methods; and in advanced systems, capable of real-time and archival processing. Currently, massive amounts of acoustic data are being collected by various institutions, corporations, and national defense agencies. The long-term goal is to provide technical capability to analyze the data using automatic algorithms for DC based on machine intelligence. The goal of the automation is to provide effective and efficient mechanisms by which to process large acoustic datasets for understanding the bioacoustic behaviors of marine mammals. This capability will provide insights into the potential ecological impacts and influences of anthropogenic ocean sounds. From Oct 2012 through Sep 2013, our research focused on five major initiatives: (1) International workshops, conferences, and data challenges; (2) Enhancements of the Acoustic Segmentation Recognition (ASR) algorithm for frequency-modulated sounds: Right Whale Study; (3) Enhancements of the ASR algorithm for pulse trains: Minke Whale Study; (4) Mining Big Data Sound Archives using High Performance Computing software and hardware; and (5) Large Pulse Train Study: Minke Vocal Activity East Coast United States.",
          "venue": "",
          "doi": "10.21236/ada573473",
          "url": "https://www.semanticscholar.org/paper/e52804a57e0f32be1dc015800a9b58d35639a17d",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "1 Efficient BackProp",
          "year": 2012,
          "citations": 4,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c7f032fb72e4666f6b1763f2eec5906c86459b5b",
          "authors": [
            "Yann LeCun",
            "L. Bottou",
            "Genevieve B. Orr",
            "K. Müller"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Time Series Modeling with Hidden Variables and Gradient-Based Algorithms",
          "year": 2010,
          "citations": 4,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/5f21dd0b3eecf2a3d396f91f7183d59be6ef07fa",
          "authors": [
            "Yann LeCun",
            "Piotr Wojciech Mirowski"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Neural Networks and Gradient-Based Learning in OCR",
          "year": 1997,
          "citations": 4,
          "abstract": null,
          "venue": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop",
          "doi": "10.1109/NNSP.1997.622405",
          "url": "https://www.semanticscholar.org/paper/6c3d93bc3e3555319443a1311b6aabbcf3ffd167",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "On-line handwriting recognition with neural networks: Spatial representation versus temporal representation",
          "year": 1993,
          "citations": 4,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c8a4f5358bde28333ab94076c935bc49c3ff26ea",
          "authors": [
            "Yann LeCun",
            "Yoshua Bengio",
            "D. Henderson",
            "A. Weisbuch",
            "H. Weissman",
            "L. Jackel"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Recurrent Parameter Generators",
          "year": 2021,
          "citations": 3,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/310215110bb72f8dfce8be73a904ea1c4968739f",
          "authors": [
            "Jiayun Wang",
            "Yubei Chen",
            "Stella X. Yu",
            "Brian Cheung",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Byte-Level Recursive Convolutional Auto-Encoder for Text",
          "year": 2018,
          "citations": 3,
          "abstract": "This article proposes to auto-encode text at byte-level using convolutional networks with a recursive architecture. The motivation is to explore whether it is possible to have scalable and homogeneous text generation at byte-level in a non-sequential fashion through the simple task of auto-encoding. We show that non-sequential text generation from a fixed-length representation is not only possible, but also achieved much better auto-encoding results than recurrent networks. The proposed model is a multi-stage deep convolutional encoder-decoder framework using residual connections, containing up to 160 parameterized layers. Each encoder or decoder contains a shared group of modules that consists of either pooling or upsampling layers, making the network recursive in terms of abstraction levels in representation. Results for 6 large-scale paragraph datasets are reported, in 3 languages including Arabic, Chinese and English. Analyses are conducted to study several properties of the proposed model.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/290c96ff9be2fb517ff2eed3f55fdde919e44f05",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning Through the Breach: Language Socialization",
          "year": 2012,
          "citations": 3,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/springerreference_301829",
          "url": "https://www.semanticscholar.org/paper/0cf33632c2031f475803ee9d9215578fa0af927a",
          "authors": [
            "C. Chan",
            "Z. Zorina",
            "Jesse R. Sparks",
            "S. L. Ornat",
            "C. Tsang",
            "E. Grigorenko",
            "R. Lubow",
            "J. Malone",
            "M. Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "T. Minor",
            "Traci N. Plumb",
            "H. Drachsler",
            "P. Kirschner",
            "M. Nakayama",
            "Rowena Santiago",
            "Ali Şimşek",
            "Fang‐Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "R. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "L. Lamb",
            "A. Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "T. Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "G. Kern-Isberner",
            "J. Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C. Olsson",
            "L. Nyberg",
            "R. Freivalds",
            "L. Hall",
            "M. Hall",
            "U. Hanke",
            "L. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "P. Burnett",
            "M. Cannon",
            "A. Edmondson",
            "P. Hartono",
            "A. Callender",
            "R. Barr",
            "N. Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohamad Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "D. Kolb",
            "R. Reilly",
            "K. Nielsen",
            "P. Couvillon",
            "H. King",
            "J. Dillon",
            "D. Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "P. C. Lane",
            "S. Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "D. Tucker",
            "P. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "D. Ansari",
            "K. Lionello-DeNolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo P. Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. Hoppe",
            "Krisztina Molnár",
            "E. Vries",
            "E. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "T. Cazenave",
            "T. Jong",
            "J. Meij",
            "J. Gavelek",
            "A. Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "M. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "D. Seo",
            "M. Monfils",
            "A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "M. Kennedy",
            "M. Jackson",
            "T. Nunes",
            "B. Jackling",
            "R. Howard",
            "W. Kennedy",
            "L. C. Drickamer"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Bio-Inspired Vision Processor for Ultra-Fast Object Categorization",
          "year": 2010,
          "citations": 3,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6116a4b9af3c3e8313b442f62cf1cfd918b756e9",
          "authors": [
            "C. Farabet",
            "B. Martini",
            "Polina Akselrod",
            "B. Corda",
            "S. Talay",
            "Yann LeCun",
            "E. Culurciello"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "OPTICAL CHARACTER RECOGNTION FOR AUTOMATIC TELLER MACHINES",
          "year": 1998,
          "citations": 3,
          "abstract": null,
          "venue": "",
          "doi": "10.1142/9789812816955_0044",
          "url": "https://www.semanticscholar.org/paper/0f64a60e61a466487f0db5046b2b454756ecb32f",
          "authors": [
            "L. Jackel",
            "Yann LeCun",
            "C. E. Stenard",
            "B. I. Strom",
            "D. Sharman",
            "D. Zuckert"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "AAAS AMA: Hi, we’re researchers from Google, Microsoft, and Facebook who study Artificial Intelligence. Ask us anything!",
          "year": 2018,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": "10.15200/WINN.151896.65484",
          "url": "https://www.semanticscholar.org/paper/7abb55c10bbc1cd590723edd0e1bb457c1b021ee",
          "authors": [
            "Yann LeCun",
            "AI Facebook",
            "New Research",
            "NY Eric York",
            "Horvitz",
            "Eric Horvitz"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Phase 2: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Machine Learning Detection Algorithms",
          "year": 2016,
          "citations": 2,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c05d6dc329a0c7a52794badaeb6d86df38a27b8e",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Discriminative recurrent sparse auto-encoders: 1st International Conference on Learning Representations, ICLR 2013",
          "year": 2013,
          "citations": 2,
          "abstract": null,
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6ef907db1e8c7b0cde28c960267cdb6a7b9a56d1",
          "authors": [
            "J. Rolfe",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "No More Pesky Learning Rates : Supplementary Material",
          "year": 2013,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7ac497addf456522d02d4426ea70d4fb20c86431",
          "authors": [
            "T. Schaul",
            "Sixin Zhang",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning from Text",
          "year": 2012,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/978-1-4419-1428-6_894",
          "url": "https://www.semanticscholar.org/paper/4b680f31d78db262fe82613e62730f6ec1356eb3",
          "authors": [
            "C. Chan",
            "Z. Zorina",
            "Jesse R. Sparks",
            "S. L. Ornat",
            "C. Tsang",
            "E. Grigorenko",
            "R. Lubow",
            "J. Malone",
            "M. Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "T. Minor",
            "Traci N. Plumb",
            "H. Drachsler",
            "P. Kirschner",
            "M. Nakayama",
            "Rowena Santiago",
            "Ali Şimşek",
            "Fang‐Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "R. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "L. Lamb",
            "A. Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "T. Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "G. Kern-Isberner",
            "J. Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C. Olsson",
            "L. Nyberg",
            "R. Freivalds",
            "L. Hall",
            "M. Hall",
            "U. Hanke",
            "L. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "P. Burnett",
            "M. Cannon",
            "A. Edmondson",
            "P. Hartono",
            "A. Callender",
            "R. Barr",
            "N. Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohamad Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "D. Kolb",
            "R. Reilly",
            "K. Nielsen",
            "P. Couvillon",
            "H. King",
            "J. Dillon",
            "D. Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "P. C. Lane",
            "S. Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "D. Tucker",
            "P. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "D. Ansari",
            "K. Lionello-DeNolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo P. Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. Hoppe",
            "Krisztina Molnár",
            "E. Vries",
            "E. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "T. Cazenave",
            "T. Jong",
            "J. Meij",
            "J. Gavelek",
            "A. Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "M. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "D. Seo",
            "M. Monfils",
            "A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "M. Kennedy",
            "M. Jackson",
            "T. Nunes",
            "B. Jackling",
            "R. Howard",
            "W. Kennedy",
            "L. C. Drickamer"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning and Development After School",
          "year": 2012,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/springerreference_301883",
          "url": "https://www.semanticscholar.org/paper/9f2f315d994a95e04c695c392ac3d80c0345b880",
          "authors": [
            "C. Chan",
            "Z. Zorina",
            "Jesse R. Sparks",
            "S. L. Ornat",
            "C. Tsang",
            "E. Grigorenko",
            "R. Lubow",
            "J. Malone",
            "M. Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "T. Minor",
            "Traci N. Plumb",
            "H. Drachsler",
            "P. Kirschner",
            "M. Nakayama",
            "Rowena Santiago",
            "Ali Şimşek",
            "Fang‐Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "R. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "L. Lamb",
            "A. Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "Lynda Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "T. Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "G. Kern-Isberner",
            "J. Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C. Olsson",
            "L. Nyberg",
            "R. Freivalds",
            "L. Hall",
            "M. Hall",
            "U. Hanke",
            "L. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "P. Burnett",
            "M. Cannon",
            "A. Edmondson",
            "P. Hartono",
            "A. Callender",
            "R. Barr",
            "N. Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohamad Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "D. Kolb",
            "R. Reilly",
            "K. Nielsen",
            "P. Couvillon",
            "H. King",
            "J. Dillon",
            "D. Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "P. C. Lane",
            "S. Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "D. Tucker",
            "P. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "D. Ansari",
            "K. Lionello-denolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "Eliza Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo P. Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. Hoppe",
            "Krisztina Molnár",
            "E. Vries",
            "E. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "T. Cazenave",
            "T. Jong",
            "J. Meij",
            "J. Gavelek",
            "A. Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "M. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "D. Seo",
            "M. Monfils",
            "A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "M. Kennedy",
            "M. Jackson",
            "T. Nunes",
            "B. Jackling",
            "R. Howard",
            "W. Kennedy",
            "L. C. Drickamer"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning Feature Hierarchies for Object Recognition",
          "year": 2010,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/84d2aaa96412156c6be140b0fb27731ecb822044",
          "authors": [
            "Yann LeCun",
            "K. Kavukcuoglu"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Workshop summary: Workshop on learning feature hierarchies",
          "year": 2009,
          "citations": 2,
          "abstract": null,
          "venue": "International Conference on Machine Learning",
          "doi": "10.1145/1553374.1553543",
          "url": "https://www.semanticscholar.org/paper/7c488cbc4103524b27f42254e9455429b23d92ca",
          "authors": [
            "Kai Yu",
            "R. Salakhutdinov",
            "Yann LeCun",
            "Geoffrey E. Hinton",
            "Yoshua Bengio"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning Long-Range Vision for an Offroad Robot",
          "year": 2008,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/4b8d80f91d271f61b26db5ad627e24e59955c56a",
          "authors": [
            "Yann LeCun",
            "R. Hadsell"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "On-Line Learning of Long-Range Obstacle Detection for Off-Road Robots",
          "year": 2006,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7959e1cb872a7dd284bf082f8cf16fc2c00da750",
          "authors": [
            "R. Hadsell",
            "P. Sermanet",
            "J. Ben",
            "Jeff Han",
            "S. Chopra",
            "Marc'Aurelio Ranzato",
            "Yury Sulsky",
            "B. Flepp",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Djvu: Un systeme de compression d'images pour la distribution reticulaire de documents numerises (Djvu: An image compression system for distributing scanned document on the internet)",
          "year": 2000,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6519eb56306e2c848d883a4f47919dd9fdcab755",
          "authors": [
            "L. Bottou",
            "P. Haffner",
            "Yann LeCun",
            "P. Howard",
            "Pascal Vincent",
            "B. Riemers"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Multimedia Processing for Advanced Communications Services",
          "year": 1999,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/978-1-4471-0859-7_42",
          "url": "https://www.semanticscholar.org/paper/3fb58e78700febc4a03ebd8eccea794218bd7149",
          "authors": [
            "B. Shahraray",
            "R. Cox",
            "B. Haskell",
            "Yann LeCun",
            "L. Rabiner"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Using curvature information to improve back-propagation",
          "year": 1988,
          "citations": 2,
          "abstract": null,
          "venue": "Neural Networks",
          "doi": "10.1016/0893-6080(88)90205-5",
          "url": "https://www.semanticscholar.org/paper/e149ea080a0c8ed23319a0ae33260f4821696675",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Generalization using back-propagation",
          "year": 1987,
          "citations": 2,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/745a9bf17a4db5b91e1bef2e083a9ad5c08ec282",
          "authors": [
            "F. F. Soulié",
            "P. Gallinari",
            "Yann LeCun",
            "S. Thiria"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "URLOST: Unsupervised Representation Learning without Stationarity or Topology",
          "year": 2023,
          "citations": 1,
          "abstract": "Unsupervised representation learning has seen tremendous progress. However, it is constrained by its reliance on domain specific stationarity and topology, a limitation not found in biological intelligence systems. For instance, unlike computer vision, human vision can process visual signals sampled from highly irregular and non-stationary sensors. We introduce a novel framework that learns from high-dimensional data without prior knowledge of stationarity and topology. Our model, abbreviated as URLOST, combines a learnable self-organizing layer, spectral clustering, and a masked autoencoder (MAE). We evaluate its effectiveness on three diverse data modalities including simulated biological vision data, neural recordings from the primary visual cortex, and gene expressions. Compared to state-of-the-art unsupervised learning methods like SimCLR and MAE, our model excels at learning meaningful representations across diverse modalities without knowing their stationarity or topology. It also outperforms other methods that are not dependent on these factors, setting a new benchmark in the field. We position this work as a step toward unsupervised learning methods capable of generalizing across diverse high-dimensional data modalities.",
          "venue": "International Conference on Learning Representations",
          "doi": "10.48550/arXiv.2310.04496",
          "url": "https://www.semanticscholar.org/paper/782d400ba7aac6ccb2d4b6d3cadbf4b7c2600d50",
          "authors": [
            "Zeyu Yun",
            "Juexiao Zhang",
            "B. Olshausen",
            "Yann LeCun",
            "Yubei Chen"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Phase 4: DCL System Using Deep Learning Approaches for Land-Based or Ship-Based Real-Time Recognition and Localization of Marine Mammals - Distributed Processing and Big Data Applications",
          "year": 2016,
          "citations": 1,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/12a1fbbb2a33bbf9fc4bf7a24c5934fb1b9b7c6f",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures?",
          "year": 2016,
          "citations": 1,
          "abstract": "(This paper was written in November 2011 and never published. It is posted on arXiv.org in its original form in June 2016). Many recent object recognition systems have proposed using a two phase training procedure to learn sparse convolutional feature hierarchies: unsupervised pre-training followed by supervised fine-tuning. Recent results suggest that these methods provide little improvement over purely supervised systems when the appropriate nonlinearities are included. This paper presents an empirical exploration of the space of learning procedures for sparse convolutional networks to assess which method produces the best performance. In our study, we introduce an augmentation of the Predictive Sparse Decomposition method that includes a discriminative term (DPSD). We also introduce a new single phase supervised learning procedure that places an L1 penalty on the output state of each layer of the network. This forces the network to produce sparse codes without the expensive pre-training phase. Using DPSD with a new, complex predictor that incorporates lateral inhibition, combined with multi-scale feature pooling, and supervised refinement, the system achieves a 70.6\\% recognition rate on Caltech-101. With the addition of convolutional training, a 77\\% recognition was obtained on the CIfAR-10 dataset.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/4b5a5d55c37ca2b5e0bfd9bc366a597f749dfc78",
          "authors": [
            "Kevin Jarrett",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Phase 1: DCL System Research Using Advanced Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - HPC System Implementation",
          "year": 2016,
          "citations": 1,
          "abstract": "We aim to investigate advancing the state of the art of detection, classification and localization (DCL) in the field of bioacoustics. The two primary goals are to develop transferable technologies for detection and classification in: (1) the area of advanced algorithms, such as deep learning and other methods; and (2) advanced systems, capable of real-time and archival and processing. This project will focus on long-term, continuous datasets to provide automatic recognition, minimizing human time to annotate the signals. Effort will begin by focusing on several years of multi-channel acoustic data collected in the Stellwagen Bank National Marine Sanctuary (SBNMS) between 2006 and 2010. Our efforts will incorporate existing technologies in the bioacoustics signal processing community, advanced high performance computing (HPC) systems, and new approaches aimed at automatically detecting-classifying and measuring features for species-specific marine mammal sounds within passive acoustic data.",
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ba19e986dffe20dca60b502aeb097018d0af0f53",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Saturating auto-encoders: International Conference on Learning Representations, ICLR 2013",
          "year": 2013,
          "citations": 1,
          "abstract": null,
          "venue": "International Conference on Learning Representations",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b6c3a5c2eb0cca13d4944f549a3e29c596170e00",
          "authors": [
            "Rostislav Goroshin",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Building Artificial Vision Systems with Machine Learning",
          "year": 2011,
          "citations": 1,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7ad0435f585f7bebe5c9207ee54228b2858cfc8f",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "High-Accuracy Object Recognition with a New Convolutional Net Architecture and Learning Algorithm",
          "year": 2009,
          "citations": 1,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/10d2ea8d995c04e244bea89c66315f3a41338dd0",
          "authors": [
            "Kevin Jarrett",
            "Marc'Aurelio Ranzato",
            "K. Kavukcuoglu",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Literacy and Learning",
          "year": 2009,
          "citations": 1,
          "abstract": null,
          "venue": "",
          "doi": "10.1007/978-1-4419-1428-6_553",
          "url": "https://www.semanticscholar.org/paper/7ac232e5b466002a5890d5d9fd11adcc8a948b62",
          "authors": [
            "Cecilia Ka Yuk Chan",
            "Zoya A. Zorina",
            "Jesse R. Sparks",
            "S. López Ornat",
            "Christine D. Tsang",
            "Elena L. Grigorenko",
            "R. Lubow",
            "John C. Malone",
            "Michael Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "Thomas R. Minor",
            "Traci N. Plumb",
            "Hendrik Drachsler",
            "P. A. Kirschner",
            "Minoru Nakayama",
            "Rowena Santiago",
            "Ali Simsek",
            "Fang-Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "Richard P. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "Luis C. Lamb",
            "Artur d’Avila Garcez",
            "D. Németh",
            "K. Janacsek",
            "John J. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "Tracy Packiam Alloway",
            "Ulrike Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "Gabriele Kern-Isberner",
            "Joost Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C.-J. Olsson",
            "Lars Nyberg",
            "R. Freivalds",
            "Lynne Hall",
            "M. Hall",
            "Ulrike Hanke",
            "Lin S. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "Anthony Robins",
            "L. Thorogood",
            "M. Udell",
            "C. Wynne",
            "Paul C. Burnett",
            "M. Cannon",
            "Amy C. Edmondson",
            "Pitoyo Hartono",
            "A. Callender",
            "Rachel Barr",
            "Natalie Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohd. Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "David A. Kolb",
            "R. Reilly",
            "Klaus Nielsen",
            "Patricia Couvillon",
            "Heather King",
            "Justin Dillon",
            "Delia Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "Fernand Gobet",
            "Peter C. R. Lane",
            "Som Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "Don M. Tucker",
            "Paul J. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "Daniel Ansari",
            "K. Lionello-DeNolf",
            "John Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "Allyson F. Hadwin",
            "M. Coen",
            "Carlo Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. U. Hoppe",
            "Krisztina Molnar",
            "Erica de Vries",
            "Emmanuel G. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "Tristan Cazenave",
            "Ton Jong",
            "Jan Meij",
            "J. Gavelek",
            "Ailing Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "Stephen Billett",
            "D. Rumbaugh",
            "Michael J. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "Dong-oh Seo",
            "M. Monfils",
            "Anthony A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "Michael J. Kennedy",
            "Michael Jackson",
            "Terezinha Nunes",
            "B. Jackling",
            "R. Howard",
            "William G. Kennedy",
            "L. C. Drickamer"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Computeur Vision and Pattern Recognition (CVPR). Conference",
          "year": 2008,
          "citations": 1,
          "abstract": null,
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1007/S11263-008-0162-4",
          "url": "https://www.semanticscholar.org/paper/d66f62ffdab3b24326bebd0cf81db5789e56dce0",
          "authors": [
            "A. Fitzgibbon",
            "C. J. Taylor",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning on automata networks",
          "year": 1987,
          "citations": 1,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3768e0e1c2c74b5049d79bfccad04db0e31954e8",
          "authors": [
            "F. F. Soulié",
            "P. Gallinari",
            "Yann LeCun",
            "S. Thiria"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "FAST INCREMENTAL LEARNING FOR AUTONOMOUS GROUND NAVIGATION",
          "year": 2024,
          "citations": 0,
          "abstract": "\n ABSTRACT \n A promising approach to autonomous driving is machine learning. In machine \n learning systems, training datasets are created that capture the sensory input \n to a vehicle as well as the desired response. One disadvantage of using a \n learned navigation system is that the learning process itself may require both a \n huge number of training examples and a large amount of computing. To avoid the \n need to collect a large training set of driving examples, we describe a system \n that takes advantage of the immense number of training examples provided by \n ImageNet, but at the same time is able to adapt quickly using a small training \n set for the driving environment. \n",
          "venue": "SAE technical paper series",
          "doi": "10.4271/2024-01-3556",
          "url": "https://www.semanticscholar.org/paper/4c4d882c3e54612946d289fd159da8de98fb3200",
          "authors": [
            "Artem Provodin",
            "L. Torabi",
            "Urs Muller",
            "B. Flepp",
            "Michael Sergio",
            "Jure Zbontar",
            "Yann LeCun",
            "L. Jackel"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Open Research Online Knowledge Graph Construction with a façade: a uniﬁed method to access heterogeneous data sources on the Web",
          "year": 2022,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/346ff941ebbccdbada17a22aa074f609bf7afd28",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Extracting structural folding pattern of chromatin using chromatin condensation data",
          "year": 2022,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/d36dcc52d9d8bc1a4b43469977653cebce579dd8",
          "authors": [
            "Samira Mali",
            "N. L. V. Berkum",
            "Louise Williams",
            "Maxim Imakaev",
            "T. Ragoczy",
            "A. Telling",
            "N. A. Kinney",
            "I. Sharakhov",
            "A. Onufriev",
            "Jia Deng",
            "Wei Dong",
            "R. Socher",
            "Lijuan Li",
            "Kaixia Li",
            "Yann LeCun",
            "B. Boser",
            "J. Denker",
            "D. Henderson",
            "R. Howard",
            "W. Hubbard"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Handwritten Digit Recognition Using TensorFlow Lite Micro on i.MX RT devices",
          "year": 2021,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b9ec816bc53bd86dc3849545bf8516bc0cb8cb9f",
          "authors": [
            "Yann LeCun",
            "Corinna Cortes"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Optimization of Artificial Neural Network Hyperparameters For Processing Retrospective Information",
          "year": 2021,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/be59058dd2320648826021b8c04d4cfcf0c18431",
          "authors": [
            "A. Rogachev",
            "F. Scholle",
            "Yann LeCun",
            "Yoshua Bengio",
            "I. L. Kashirin",
            "M. Demchenko"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Jean piaget theory in punjabi pdf",
          "year": 2020,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/759f608c5246b7d634dfe13a21adc4731da4bddd",
          "authors": [
            "Jean PiagetPiaget",
            "H. Bergson",
            "P. Janet",
            "A. Binet",
            "Théodore Simon",
            "S. Spielrein",
            "Shlomo Wolbe",
            "B. Inhelder",
            "J. Bruner",
            "K. Kaye",
            "Citation",
            "L. Kohlberg",
            "R. Kegan",
            "H. Gardner",
            "T. Kuhn",
            "J. Flavell",
            "Yann LeCun",
            "J. Peterson",
            "J. Piaget"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Sticky Reasoning within Learning Representations",
          "year": 2019,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/0633bf78c0b5aa041c881bd334906733302a4427",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Lesson planning",
          "year": 2019,
          "citations": 0,
          "abstract": null,
          "venue": "Healthcare Simulation at a Glance",
          "doi": "10.1007/978-1-4419-1428-6_4758",
          "url": "https://www.semanticscholar.org/paper/d6df4299193a145080896c67f5dfe34b9ad61fea",
          "authors": [
            "Cecilia Ka Yuk Chan",
            "Zoya A. Zorina",
            "Jesse R. Sparks",
            "S. López Ornat",
            "Christine D. Tsang",
            "Elena L. Grigorenko",
            "R. Lubow",
            "John C. Malone",
            "Michael Domjan",
            "Charles Yang",
            "Michelyn C. Butler",
            "M. Gettinger",
            "Thomas R. Minor",
            "Traci N. Plumb",
            "Hendrik Drachsler",
            "P. A. Kirschner",
            "Minoru Nakayama",
            "Rowena Santiago",
            "Ali Simsek",
            "Fang-Ying Yang",
            "Yi-Chun Chen",
            "G. Brooke",
            "Heidi L. Andrade",
            "Richard P. Cooper",
            "A. Podolskiy",
            "David W. Versailles",
            "Valérie Mérindol",
            "E. Guerci",
            "Nobuyuki Hanaki",
            "D. Grollman",
            "A. Billard",
            "Luis C. Lamb",
            "Artur d’Avila Garcez",
            "D. Németh",
            "K. Janacsek",
            "John A. Nunnery",
            "L. Byrd-Poller",
            "M. Haan",
            "Rodrigo Harrison",
            "Mauricio G. Villena",
            "L. Izquierdo",
            "Segismundo S. Izquierdo",
            "F. Vega-Redondo",
            "Tracy Packiam Alloway",
            "U. Halsband",
            "N. Seel",
            "G. Bedny",
            "Hansjörg von Brevern",
            "K. Synytsya",
            "Gabriele Kern-Isberner",
            "Joost Breuker",
            "S. Cerri",
            "T. Zittoun",
            "S. Brinkmann",
            "Negin Dahya",
            "S. B. Fountain",
            "Karen E. Doyle",
            "K. Sarfo",
            "Bertram C. Bruce",
            "N. Bloch",
            "C.-J. Olsson",
            "Lars Nyberg",
            "R. Freivalds",
            "Lynne Hall",
            "M. Hall",
            "Ulrike Hanke",
            "Lin S. Norton",
            "Aytac Gogus",
            "K. Illeris",
            "M. Macy",
            "A. Flache",
            "A. Robins",
            "Laura-Rose Thorogood",
            "M. Udell",
            "C. Wynne",
            "Paul C. Burnett",
            "M. Cannon",
            "Amy C. Edmondson",
            "Pitoyo Hartono",
            "A. Callender",
            "Rachel Barr",
            "Natalie Brito",
            "Noorizah Mohd. Noor",
            "Tg. Nor Rizan Tg. Mohd. Maasum",
            "K. Cennamo",
            "Marc'Aurelio Ranzato",
            "Y-Lan Boureau",
            "K. Kavukcuoglu",
            "Karol Gregor",
            "Yann LeCun",
            "M. Alias",
            "Caifeng Shan",
            "Alice Y. Kolb",
            "David A. Kolb",
            "R. Reilly",
            "Klaus Nielsen",
            "P. Couvillon",
            "Heather King",
            "Justin Dillon",
            "Delia Neuman",
            "J. E. Purdy",
            "Linda Kragelund",
            "F. Gobet",
            "Peter C. R. Lane",
            "Som Naidu",
            "Danny R. Bedgood",
            "Dirk Ifenthaler",
            "Ida Moadab",
            "Don M. Tucker",
            "Paul J. Hager",
            "J. Fejes",
            "Danielle C. Colas-Zelin",
            "L. Matzel",
            "P. Perruchet",
            "B. Poulin-Charronnat",
            "S. Pacton",
            "C. Linnman",
            "Mohamed A Zeidan",
            "M. Milad",
            "I. Holloway",
            "Daniel Ansari",
            "K. Lionello-DeNolf",
            "J. Burgoyne",
            "Judy Huang",
            "Roger K. Thomas",
            "S. Pietropaolo",
            "Wim E. Crusio",
            "Sabine Richter",
            "J. Elen",
            "G. Clarebout",
            "E. Bliss-Moreau",
            "Jonte Bernhard",
            "Marcia L. Conner",
            "Lanita Jacobs",
            "Mariel Miller",
            "A. Hadwin",
            "M. Coen",
            "Carlo Magno",
            "Eli Hinkel",
            "S. Szedmák",
            "F. Balagué",
            "M. Milrad",
            "H. U. Hoppe",
            "Krisztina Molnar",
            "Erica de Vries",
            "Emmanuel G. Blanchard",
            "C. Frasson",
            "Susanne P. Lajoie",
            "Tristan Cazenave",
            "Ton Jong",
            "Jan Meij",
            "J. Gavelek",
            "Ailing Kong",
            "A. Daffertshofer",
            "J. Alonso-Tapia",
            "S. Billett",
            "D. Rumbaugh",
            "Michael J. Beran",
            "Imran Ho-Abdullah",
            "Norsimah Mat Awal",
            "Dong-oh Seo",
            "M. Monfils",
            "Anthony A. Wright",
            "D. Deshler",
            "Frances M. Ihle",
            "Carrie Mark",
            "Daniel T. Pollitt",
            "Michael J. Kennedy",
            "Michael Jackson",
            "Terezinha Nunes",
            "B. Jackling",
            "R. Howard",
            "William G. Kennedy",
            "L. C. Drickamer"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Adversarially-Trained Normalized Noisy-Feature Auto-Encoder for Text Generation",
          "year": 2018,
          "citations": 0,
          "abstract": "This article proposes Adversarially-Trained Normalized Noisy-Feature Auto-Encoder (ATNNFAE) for byte-level text generation. An ATNNFAE consists of an auto-encoder where the internal code is normalized on the unit sphere and corrupted by additive noise. Simultaneously, a replica of the decoder (sharing the same parameters as the AE decoder) is used as the generator and fed with random latent vectors. An adversarial discriminator is trained to distinguish training samples reconstructed from the AE from samples produced through the random-input generator, making the entire generator-discriminator path differentiable for discrete data like text. The combined effect of noise injection in the code and shared weights between the decoder and the generator can prevent the mode collapsing phenomenon commonly observed in GANs. Since perplexity cannot be applied to non-sequential text generation, we propose a new evaluation method using the total variance distance between frequencies of hash-coded byte-level n-grams (NGTVD). NGTVD is a single benchmark that can characterize both the quality and the diversity of the generated texts. Experiments are offered in 6 large-scale datasets in Arabic, Chinese and English, with comparisons against n-gram baselines and recurrent neural networks (RNNs). Ablation study on both the noise level and the discriminator is performed. We find that RNNs have trouble competing with the n-gram baselines, and the ATNNFAE results are generally competitive.",
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/c03607de715a6c578ac720a074d7fb494e37cc92",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Universality in halting time",
          "year": 2017,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/00c44c3355b094dd93cb279349485356092cd07d",
          "authors": [
            "Levent Sagun",
            "T. Trogdon",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Proceedings of the International Computer Music Conference 2016 A Fluid Chord Voicing Generator",
          "year": 2017,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8895fae88182fc5a5623a11c7916b026244f7093",
          "authors": [
            "D. Marcarian",
            "P. Savvidou",
            "B. Willis",
            "M. Skubic",
            "Yann LeCun",
            "K. Perlin",
            "J. Tilmanne",
            "M. Hashimoto",
            "Q. Sun",
            "J. Luo",
            "Y. He"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Predicting Deeper into the Future of Semantic Segmentation — Supplementary Material —",
          "year": 2017,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/de2c8419d8e5e1501f8daa17e10ee46c27adc5e4",
          "authors": [
            "Pauline Luc",
            "N. Neverova",
            "C. Couprie",
            "Jakob Verbeek",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "+ YY ’ Z X-Residual Prediction Error TargetPredictionObservation Latent 0 State φ f 1 f 2",
          "year": 2017,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/f85ea632dc79e779a6a1b3170013288f55af0d52",
          "authors": [
            "Mikael Henaff",
            "J. Zhao",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Phase 3: DCL System Using Deep Learning Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals - Bioacoustic Applicaitons",
          "year": 2016,
          "citations": 0,
          "abstract": null,
          "venue": "arXiv.org",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/e1240e95706d52160a7c51c4f3b1fed4e42658a2",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Complex-valued convolutional networks yield data-driven multiscale windowed spectra",
          "year": 2015,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/13e03191c8d83903661de9fba9b2309d990e2c84",
          "authors": [
            "Joan Bruna",
            "Soumith Chintala",
            "Yann LeCun",
            "Serkan Piantino",
            "Arthur Szlam",
            "M. Tygert"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Assignment 2 - Deep Learning with Sparse Coding",
          "year": 2013,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/016c85fb230e3be91ebc7e0d75eb12c9ddc7661c",
          "authors": [
            "Xiang Zhang",
            "Yann LeCun",
            "J. Langford"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Neural Information Processing (ICONIP 2013)",
          "year": 2013,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/297bcea9cca46cbe3181508972c724dc1c5d3f75",
          "authors": [
            "T. Vatanen",
            "T. Raiko",
            "Harri Valpola",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "AAAI Workshop - Technical Report: Preface",
          "year": 2013,
          "citations": 0,
          "abstract": null,
          "venue": "AAAI Conference on Artificial Intelligence",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/31dbf596f6f6b472aba9ea3434e2463b20c98511",
          "authors": [
            "Marc Pickett",
            "B. Kuipers",
            "Yann LeCun",
            "Clayton T. Morrison"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Flexible-Cost SLAM",
          "year": 2012,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8a96d99e4fe1bfe471889ccc2e816522b967d88f",
          "authors": [
            "Yann LeCun",
            "M. Grimes"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "AISTAT AISTATS Fifteenth International Conference on Artificial Intelligence and Statistics (AISTAT 2012) AISTAT AISTATS, La Palma, Canary Islands, April 21-23, 2012",
          "year": 2012,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/8f83cbb3f7594c7227d9e318437d700945473df7",
          "authors": [
            "T. Raiko",
            "Harri Valpola",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "DCL System Research Using Advanced Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals",
          "year": 2012,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": "10.21236/ada572279",
          "url": "https://www.semanticscholar.org/paper/8fea0cbdf12afec45db609219a56016ecc571147",
          "authors": [
            "Peter J. Dugan",
            "C. Clark",
            "Yann LeCun",
            "S. Parijs"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Concerto for violin and Markov model",
          "year": 2011,
          "citations": 0,
          "abstract": null,
          "venue": "Communications of the ACM",
          "doi": "10.1145/1897852.1897874",
          "url": "https://www.semanticscholar.org/paper/85c783700095ed2e6714bf2dd39e7f3dd79b72dd",
          "authors": [
            "J. Bello",
            "Yann LeCun",
            "R. Rowe"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Final thesis for the fulfilment of the requirements for the degree of MSc. in Applied Computing Science Improving Score Matching for learning statistical models of natural images",
          "year": 2010,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/1f6062b766dfa0dabc4edb58d3694798556c62dc",
          "authors": [
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "REASSESSING FHA RISK 1",
          "year": 2010,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/474b68356813e17dc22150df627f35703d98600c",
          "authors": [
            "Diego Aragon",
            "Andrew Caplin",
            "S. Chopra",
            "John Leahy",
            "Yann LeCun",
            "Marco Scoffier",
            "Joseph S. Tracy"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Autonomous Learning for Long Range Vision in Mobile Robots",
          "year": 2008,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/39d3c37cb479cd16c027919a5c8774649daec4a9",
          "authors": [
            "R. Hadsell",
            "P. Sermanet",
            "A. Erkan",
            "K. Kavukcuoglu",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Invariant Feature Learning on a Mobile Robot",
          "year": 2008,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/f67eceab1c562ef3e265f2a64223ffead3ed3c22",
          "authors": [
            "R. Hadsell",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Energy-Based Factor Graphs for Prediction in Relational Data",
          "year": 2007,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/47e5b8dcd44d3d1f595e9e8636ebe2ecdc1de744",
          "authors": [
            "S. Chopra",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Global Map Long Range Vision ( FAROD ) Cameras Vehicle Map Global planner Route to goal Global Map Goal Local candidates",
          "year": 2007,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/7c305a78154c714372a10ce615accc408c0eb221",
          "authors": [
            "R. Hadsell",
            "A. Erkan",
            "P. Sermanet",
            "J. Ben",
            "K. Kavukcuoglu",
            "Urs Muller",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Learning Sparse and Invariant Features Hierarchies",
          "year": 2007,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/dad2f8fd3e155c86b089b5b054a1e0c5b8f79915",
          "authors": [
            "Fu Jie Huang",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Message from the Program and General Chairs",
          "year": 2006,
          "citations": 0,
          "abstract": null,
          "venue": "Computer Vision and Pattern Recognition",
          "doi": "10.1109/CVPR.2006.184",
          "url": "https://www.semanticscholar.org/paper/1d0e3de0d4716f99008e6251261d1c7d8feb6f9a",
          "authors": [
            "A. Fitzgibbon",
            "C. J. Taylor",
            "Yann LeCun",
            "D. Huttenlocher",
            "David Forsyth"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Proceedings : 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition : CVPR 2006 : June 17-22, 2006, New York, NY",
          "year": 2006,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/baf9530d8ab59e42fef6b35b51b27261e1854183",
          "authors": [
            "A. Fitzgibbon",
            "C. J. Taylor",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Building an Automatic Phenotyping System of Developing Embryos",
          "year": 2006,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/eafc996e3a178e93bd7efe926edbcb4129dbb0fe",
          "authors": [
            "Yann LeCun",
            "F. Ning"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Advances in neural information processing systems: Proceedings of the first 12 conferences (CDROM)",
          "year": 2001,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/6822d4f630b7e7de2582e77b577b6f037e32ee5c",
          "authors": [
            "M. I. Jordan",
            "Yann LeCun",
            "S. Solla"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Unsupervised Learning of Sparse and Invariant Features Hierarchies",
          "year": 1998,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/3011a4d3c3dd2d303a71949db348dd1d999c9db2",
          "authors": [
            "Y-Lan Boureau",
            "Fu Jie Huang",
            "Yann LeCun"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Predicting transportpath degradation/failure based on recent performance history",
          "year": 1994,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/96fe89e4f006c85ea3fc743dc507113715128826",
          "authors": [
            "Wan-Ping Chiang",
            "Corinna Cortes",
            "L. Jackel",
            "Yann LeCun",
            "W. Lee",
            "E. Pednault",
            "V. Vapnik"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Device for recognizing mark",
          "year": 1993,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/25115955f80ea56236cf30704f2ef279287422d3",
          "authors": [
            "Yann LeCun",
            "Quen-Zong Wu",
            "ゾン ウ クエン",
            "ヤン アンドレレカン"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Device for recognizing mark and method for the same",
          "year": 1993,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/593701c4618ee7c8b89dcfd51639bfd9891dd0a9",
          "authors": [
            "Yann LeCun",
            "Quen-Zong Wu",
            "ゾン ウ クエン",
            "ヤン アンドレレカン"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "User manual: SN: A simulator for connectionist models",
          "year": 1988,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/b2819d429bb7e76323ef3e836cbb608b7f700ab8",
          "authors": [
            "Yann LeCun",
            "L. Bottou"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "General Information Registration & Reception",
          "year": null,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/2c3e28967d25ef2811e3675cf9d3ab244901e63b",
          "authors": [
            "B. Dosher",
            "Yann LeCun",
            "W. Estes",
            "J. Kujala",
            "A. Z. Scholten"
          ],
          "sources": [
            "semantic_scholar"
          ]
        },
        {
          "title": "Energy-based Models in Document Recognition and Computer Vision. 1. Two Challenges in Machine Learning",
          "year": null,
          "citations": 0,
          "abstract": null,
          "venue": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/9adb3a467cdc39637aca2a88a988d543cf45c72d",
          "authors": [
            "Yann LeCun",
            "S. Chopra",
            "Aurelio Marc",
            "Fu-Jie Ranzato",
            "Huang"
          ],
          "sources": [
            "semantic_scholar"
          ]
        }
      ]
    }
  }
}